{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce909ed",
   "metadata": {},
   "source": [
    "# LayerNorm\n",
    "La normalisation de couche (LayerNorm) est une technique utilisée dans les réseaux neuronaux, y compris dans l'architecture Transformer couramment utilisée dans les tâches de traitement du langage naturel (NLP). Elle aide à stabiliser et à améliorer l'entraînement des réseaux neuronaux profonds en normalisant les entrées de chaque couche.\n",
    "\n",
    "Dans le contexte de l'architecture Transformer, qui se compose de plusieurs couches d'auto-attention et de réseaux neuronaux à propagation avant, la normalisation de couche est appliquée à la fois avant et après les sous-couches de chaque couche (c'est-à-dire avant le mécanisme d'auto-attention et avant le réseau neuronal à propagation avant).\n",
    "\n",
    "Voici une brève explication de la façon dont fonctionne la normalisation de couche :\n",
    "\n",
    "1. **Normalisation de l'Entrée** :\n",
    "   - Pour chaque position dans la séquence d'entrée, les valeurs à travers différentes dimensions (par exemple, les plongements de mots) sont normalisées indépendamment.\n",
    "   - Cela signifie que pour chaque dimension, la moyenne et l'écart-type sont calculés sur l'ensemble du lot (batch) et appliqués élément par élément pour normaliser les valeurs.\n",
    "\n",
    "2. **Transformation Affine** :\n",
    "   - Après la normalisation, une transformation affine est appliquée à chaque dimension. Cela implique de mettre à l'échelle et de décaler les valeurs en utilisant des paramètres apprenables (gamma et beta) pour chaque dimension.\n",
    "   - Cela permet au modèle d'apprendre combien mettre à l'échelle ou décaler les valeurs normalisées.\n",
    "\n",
    "Mathématiquement, pour une dimension donnée $i$ et une position $j$ dans la séquence d'entrée, l'opération de normalisation de couche peut être représentée comme suit :\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x_{ij}) = \\gamma_i \\cdot \\frac{x_{ij} - \\mu_i}{\\sigma_i} + \\beta_i\n",
    "$$\n",
    "\n",
    "Où :\n",
    "- $x_{ij}$ est la valeur d'entrée pour la dimension $i$ à la position $j$.\n",
    "- $\\mu_i$ est la moyenne des valeurs pour la dimension $i$ sur l'ensemble du lot.\n",
    "- $\\sigma_i$ est l'écart-type des valeurs pour la dimension $i$ sur l'ensemble du lot.\n",
    "- $\\gamma_i$ et $\\beta_i$ sont des paramètres apprenables pour la mise à l'échelle et le décalage.\n",
    "\n",
    "Les principaux avantages de la normalisation de couche dans le contexte de l'architecture Transformer sont :\n",
    "\n",
    "1. **Stabilité** :\n",
    "   - Elle aide à stabiliser le processus d'entraînement en réduisant le déplacement covariant interne, qui peut se produire lorsque la distribution des activations change pendant l'entraînement.\n",
    "\n",
    "2. **Réduction de la Dépendance à l'Initialisation** :\n",
    "   - La normalisation de couche réduit la sensibilité du modèle à l'initialisation des poids.\n",
    "\n",
    "3. **Amélioration de la Généralisation** :\n",
    "   - Elle peut conduire à une convergence plus rapide et à de meilleures performances de généralisation.\n",
    "\n",
    "4. **Réduction du Besoin de la Normalisation par Lots** :\n",
    "   - Dans de nombreux cas, la normalisation de couche peut remplacer ou compléter la normalisation par lots, qui est plus couramment utilisée dans d'autres types d'architectures de réseaux neuronaux.\n",
    "\n",
    "En somme, la normalisation de couche est un composant crucial pour construire des modèles d'apprentissage en profondeur efficaces et stables, en particulier dans des architectures comme le Transformer qui impliquent de multiples couches de calculs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f50341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e064297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.alpha = nn.Parameter(torch.ones(self.d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha*(x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + 1e-7) + \\\n",
    "                            self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2816f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation\n",
    "layer_norm = LayerNorm(d_model=512)  # En supposant 512 caractéristiques\n",
    "\n",
    "# Création d'un tenseur d'entrée aléatoire de forme (taille_du_lot, longueur_de_la_séquence, caractéristiques)\n",
    "input_tensor = torch.randn(32, 20, 512)\n",
    "\n",
    "# Application de la normalisation des couches\n",
    "output_tensor = layer_norm(input_tensor)\n",
    "\n",
    "print(output_tensor.shape)  # Devrait s'imprimer : torch.Size([32, 20, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2444b",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Network\n",
    "   - Après le mécanisme d'auto-attention, la sortie passe à travers un réseau de neurones à propagation avant (FFN).\n",
    "   - Le FFN est un réseau neuronal simple composé de deux transformations linéaires (couches entièrement connectées) avec une fonction d'activation non linéaire (généralement ReLU ou GELU) appliquée entre elles.\n",
    "   - Le FFN fonctionne de manière indépendante à chaque position dans la séquence.\n",
    "\n",
    "\n",
    "\n",
    "Le composant \"feed-forward\" est essentiel car il permet au modèle de capturer des relations complexes entre les mots d'une séquence. En appliquant des transformations non linéaires aux sorties de l'auto-attention, le modèle peut apprendre des motifs plus sophistiqués dans les données.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ad171b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_size=2048, droprate=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden_size)\n",
    "        self.dropout = nn.Dropout(droprate)\n",
    "        self.fc2 = nn.Linear(hidden_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0126866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "FF=FeedForward(512)\n",
    "# Application du layer\n",
    "output_tensor = FF(input_tensor)\n",
    "\n",
    "print(output_tensor.shape)  # Devrait s'imprimer : torch.Size([32, 20, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab4f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
