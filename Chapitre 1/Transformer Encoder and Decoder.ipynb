{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a723450",
   "metadata": {
    "id": "9a723450"
   },
   "source": [
    "L'architecture Transformer, introduite dans l'article \"Attention is All You Need\" de Vaswani et al., est une puissante architecture d'apprentissage profond principalement utilisée pour les tâches de traitement du langage naturel (NLP). Elle repose fortement sur le mécanisme de l'attention pour traiter les séquences d'entrée en parallèle.\n",
    "\n",
    "L'architecture Transformer se compose de deux composants principaux : l'encodeur et le décodeur.\n",
    "\n",
    "1. **Encodeur :**\n",
    "   - L'encodeur est responsable du traitement de la séquence d'entrée et de sa conversion en une série de représentations de caractéristiques qui capturent l'information dans la séquence.\n",
    "   - Il se compose d'une pile de couches identiques. Chaque couche a deux sous-couches :\n",
    "      - **Auto-attention multi-têtes :** Cette sous-couche permet à l'encodeur d'évaluer l'importance des différents mots dans la séquence d'entrée les uns par rapport aux autres. Elle calcule les scores d'attention entre chaque paire de mots dans la séquence d'entrée, aboutissant à une combinaison pondérée de tous les mots.\n",
    "      - **Réseaux d'Avancement à Travers les Positions :** Cette sous-couche applique un réseau neuronal à propagation avant à chaque position de manière séparée et identique.\n",
    "   - Les sorties de ces sous-couches passent par une connexion résiduelle suivie d'une normalisation de couche.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1284d671",
   "metadata": {
    "id": "1284d671"
   },
   "outputs": [],
   "source": [
    "def clone_layers(module, num):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e826f28",
   "metadata": {
    "id": "2e826f28"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num, n_heads):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.num = num\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = Pos_Encoder(d_model, max_len= 80)\n",
    "        self.layers = clone_layers(EncoderLayer(d_model, n_heads), num)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src);\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.num):\n",
    "            x = self.layers[i](x, mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9981cea5",
   "metadata": {
    "id": "9981cea5"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, droprate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.attn = MHAttention(d_model=d_model, n_heads=n_heads)\n",
    "        self.dropout1 = nn.Dropout(droprate)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.fc1 = FeedForward(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(droprate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x1 = self.norm1(x);\n",
    "        x = x + self.dropout1(self.attn(x1, x1, x1, mask));\n",
    "        x1 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.fc1(x1));\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38990c",
   "metadata": {
    "id": "5d38990c"
   },
   "source": [
    "2. **Décodeur :**\n",
    "   - Le décodeur est responsable de la génération de la séquence de sortie en se basant sur l'information traitée provenant de l'encodeur.\n",
    "   - Il se compose également d'une pile de couches identiques, mais avec une sous-couche supplémentaire :\n",
    "      - **Auto-attention multi-têtes :** Similaire à l'encodeur, cette sous-couche permet au décodeur de se concentrer sur différentes parties de la séquence d'entrée. Cependant, elle dispose d'un mécanisme de masquage pour empêcher le décodeur de regarder les positions futures dans la séquence de sortie.\n",
    "      - **Attention de l'encodeur au décodeur multi-têtes :** Cette sous-couche aide le décodeur à se concentrer sur différentes parties de la séquence d'entrée encodée. Elle permet au décodeur de s'attarder sur différentes positions dans la séquence d'entrée en fonction de l'importance de chaque position pour générer le prochain mot.\n",
    "      - **Réseaux d'Avancement à Travers les Positions :** Identique à celui de l'encodeur.\n",
    "   - Tout comme pour l'encodeur, les sorties de ces sous-couches passent par une connexion résiduelle suivie d'une normalisation de couche.\n",
    "\n",
    "3. **Encodage de Position :**\n",
    "   - Étant donné que l'architecture Transformer n'a pas de compréhension inhérente de l'ordre des éléments dans une séquence, des encodages de position sont ajoutés aux embeddings d'entrée pour donner au modèle des informations sur les positions relatives ou absolues des jetons dans la séquence.\n",
    "\n",
    "4. **Couches Linéaires et Softmax Finales :**\n",
    "   - La sortie de la dernière couche de décodeur passe par une couche linéaire pour obtenir les logits (scores) pour chaque jeton possible dans le vocabulaire de sortie.\n",
    "   - Une fonction softmax est ensuite appliquée à ces logits pour obtenir une distribution de probabilité sur le vocabulaire, indiquant la probabilité de chaque jeton d'être le mot suivant dans la séquence.\n",
    "\n",
    "Pendant l'entraînement, le modèle est optimisé pour minimiser la différence entre les probabilités prédites et les séquences cibles réelles. Cela se fait généralement en utilisant une fonction de perte telle que la perte de cross-entropie.\n",
    "\n",
    "Pendant l'inférence (ou la génération), le modèle est utilisé de manière auto-régressive, ce qui signifie qu'il génère un jeton à la fois en utilisant ses propres prédictions comme entrée pour l'étape suivante, jusqu'à ce qu'une condition d'arrêt soit atteinte (par exemple, une longueur maximale ou un jeton de fin de séquence). En somme, l'architecture Transformer exploite les mécanismes d'auto-attention et les réseaux neuronaux à propagation avant pour traiter les séquences d'entrée en parallèle, ce qui la rend très efficace pour un large éventail de tâches de NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d1159",
   "metadata": {
    "id": "b24d1159"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, droprate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(droprate)\n",
    "        self.dropout2 = nn.Dropout(droprate)\n",
    "        self.dropout3 = nn.Dropout(droprate)\n",
    "        self.attn1 = MHAttention(d_model=d_model, n_heads=n_heads)\n",
    "        self.attn2 = MHAttention(d_model=d_model, n_heads=n_heads)\n",
    "        self.fc1 = FeedForward(d_model=d_model)\n",
    "\n",
    "    def forward(self, x, e_out, src_mask, trg_mask):\n",
    "        x1 = self.norm1(x);\n",
    "        x = x + self.dropout1(self.attn1(x1, x1, x1, trg_mask));\n",
    "        x1 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.attn2(x1, e_out, e_out, src_mask));\n",
    "        x1 = self.norm3(x)\n",
    "        x = x + self.dropout3(self.fc1(x1));\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num, n_heads):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.num = num\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = Pos_Encoder(d_model, max_len = 80)\n",
    "        self.layers = clone_layers(DecoderLayer(d_model, n_heads), num)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, trg, e_out, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.num):\n",
    "            x = self.layers[i](x, e_out, src_mask, trg_mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10102b4f",
   "metadata": {
    "id": "10102b4f"
   },
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aed86f",
   "metadata": {
    "id": "20aed86f"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, num, n_heads):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = EncoderBlock(vocab_size=src_vocab, d_model=d_model, num=num, n_heads=n_heads)\n",
    "        self.decoder = DecoderBlock(vocab_size=trg_vocab, d_model=d_model, num=num, n_heads=n_heads)\n",
    "        self.fc1 = nn.Linear(d_model, trg_vocab)\n",
    "\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_out = self.encoder(src, src_mask);\n",
    "        d_out = self.decoder(trg, e_out, src_mask, trg_mask);\n",
    "        x = self.fc1(d_out);\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
