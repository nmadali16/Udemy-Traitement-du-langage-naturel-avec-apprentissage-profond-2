{"cells":[{"cell_type":"markdown","id":"a90bd38a","metadata":{"id":"a90bd38a"},"source":["# Multi-head Attention\n","La mécanisme d'attention multi-têtes est un composant clé de l'architecture Transformer utilisée pour les tâches de traitement du langage naturel (NLP). Il permet au modèle de se concentrer sur différentes parties de la séquence d'entrée simultanément, lui offrant ainsi la capacité de capturer divers types de relations entre les mots.\n","\n","Dans le contexte du NLP, supposons que nous ayons une séquence d'incorporations d'entrée $X = \\{x_1, x_2, \\ldots, x_n\\}$, où $x_i$ représente l'incorporation du $i$-ème mot dans la séquence. Le mécanisme d'attention multi-têtes fonctionne comme suit :\n","\n","1. **Transformations Linéaires** : Les incorporations d'entrée sont transformées linéairement en trois ensembles différents d'incorporations - Requête ($Q$), Clé ($K$), et Valeur ($V$).\n","\n","   $$\n","   Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n","   $$\n","\n","   Ici, $W_Q$, $W_K$ et $W_V$ sont des matrices de poids apprenables.\n","\n","2. **Division en Plusieurs Têtes** : Chacune des matrices $Q$, $K$ et $V$ est divisée en plusieurs matrices plus petites (têtes). Supposons que nous ayons $h$ têtes, les matrices divisées deviennent :\n","\n","   $$\n","   Q_i = QW_{Qi}, \\quad K_i = KW_{Ki}, \\quad V_i = VW_{Vi}\n","   $$\n","\n","   où $W_{Qi}$, $W_{Ki}$ et $W_{Vi}$ sont des matrices de poids spécifiques à la $i$-ème tête.\n","\n","3. **Attention à Produit Scalaire Normalisé** : Pour chaque tête, les scores d'attention sont calculés en utilisant un mécanisme d'attention à produit scalaire normalisé :\n","\n","   $$\n","   \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right) V_i\n","   $$\n","\n","   où $d_k$ est la dimension des vecteurs clés.\n","\n","4. **Concaténation et Transformation Linéaire** : Les résultats de toutes les têtes sont concaténés et transformés linéairement pour obtenir la sortie finale de l'attention multi-têtes :\n","\n","   $$\n","   \\text{MultiTête}(Q, K, V) = \\text{Concat}(\\text{Attention}_1, \\ldots, \\text{Attention}_h)W_O\n","   $$\n","\n","   où $W_O$ est une matrice de poids apprenable.\n","\n","5. **Connexion Résiduelle et Normalisation de Couche** : La sortie de l'attention multi-têtes est ajoutée à l'entrée d'origine (avec une connexion directe) puis soumise à une opération de normalisation de couche.\n","\n","   $$\n","   \\text{Sortie} = \\text{LayerNorm}(X + \\text{MultiTête}(Q, K, V))\n","   $$\n","\n","Le modèle Transformer répète généralement ces étapes plusieurs fois dans une couche, en plus de réseaux neuronaux à propagation avant et d'opérations de normalisation supplémentaires.\n","\n","Ce mécanisme d'attention multi-têtes permet au modèle de se concentrer sur différentes parties de la séquence d'entrée avec différentes projections linéaires apprises, lui permettant de capturer différents types d'informations et de relations dans les données."]},{"cell_type":"code","execution_count":null,"id":"f09da0fe","metadata":{"id":"f09da0fe"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import math"]},{"cell_type":"code","execution_count":null,"id":"8f64dbfb","metadata":{"id":"8f64dbfb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"94d720e9","metadata":{"id":"94d720e9"},"outputs":[],"source":["def Attention(q, k, v, dh, mask=None, dropout=None):\n","    scores = torch.matmul(q, k.transpose(-2,-1))/math.sqrt(dh)\n","    if mask is not None:\n","        mask = mask.unsqueeze(1);\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","\n","    scores = torch.softmax(scores, dim=-1)\n","    if dropout is not None:\n","        scores = dropout(scores)\n","    output = torch.matmul(scores, v)\n","    return output,scores"]},{"cell_type":"code","execution_count":null,"id":"b4de455d","metadata":{"id":"b4de455d"},"outputs":[],"source":["class MHAttention(nn.Module):\n","    def __init__(self, d_model, n_heads, droprate=0.1):\n","        super(MHAttention, self).__init__()\n","        self.d_model = d_model\n","        self.n_heads = n_heads\n","        self.dh = d_model//n_heads\n","        # couches d'apprentissage pour q,k,v\n","        self.q_matrix = nn.Linear(d_model, d_model)\n","        self.k_matrix = nn.Linear(d_model, d_model)\n","        self.v_matrix = nn.Linear(d_model, d_model)\n","        self.dropout = nn.Dropout(droprate)\n","        self.fc1 = nn.Linear(d_model, d_model)\n","\n","    def forward(self, q, k, v, mask=None):\n","        # input = batch_size X seq_len X d_model into batch_size X heads X seq_len X d_model/heads\n","        q = self.q_matrix(q); q = q.view(q.size(0), self.n_heads, -1, self.dh)\n","        k = self.k_matrix(k); k = k.view(k.size(0), self.n_heads, -1, self.dh)\n","        v = self.v_matrix(v); v = v.view(v.size(0), self.n_heads, -1, self.dh)\n","        scores,attention_weights = Attention(q, k, v, self.dh, mask, self.dropout)\n","        scores = scores.reshape(q.size(0), -1, self.d_model)\n","        output = self.fc1(scores)\n","        return output,attention_weights\n"]},{"cell_type":"code","execution_count":null,"id":"4d46faaa","metadata":{"id":"4d46faaa"},"outputs":[],"source":["batch_size = 1\n","seq_len = 5\n","d_model = 64\n","num_heads = 4\n","\n","embedding  = torch.randn(batch_size, seq_len, d_model)"]},{"cell_type":"code","execution_count":null,"id":"cbf5c0f8","metadata":{"id":"cbf5c0f8","outputId":"2bb6e7b8-7fd6-416b-dd03-c62f958564fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Output: torch.Size([1, 5, 64])\n","Attentions: torch.Size([1, 4, 5, 5])\n"]}],"source":["# Instanciation de la couche d'attention à plusieurs têtes\n","multi_head_attention = MHAttention(d_model, num_heads)\n","\n","# Appliquer l'attention à plusieurs têtes\n","output, attention_weights = multi_head_attention(embedding,embedding,embedding)\n","\n","print(\"Output:\", output.shape)\n","print(\"Attentions:\", attention_weights.shape)"]},{"cell_type":"markdown","id":"9972462f","metadata":{"id":"9972462f"},"source":["# Masked multi-head attention\n","Le masque utilisé dans l'attention multi-têtes masquée est une matrice binaire qui empêche certaines positions d'être attentives à d'autres positions dans la séquence. Plus précisément, le masque garantit qu'une position donnée ne peut s'intéresser qu'aux positions qui la précèdent.\n","\n","Par exemple, dans une tâche de modélisation du langage où le modèle est entraîné à prédire le mot suivant dans une phrase, il est important que le modèle n'ait pas accès aux mots futurs pendant l'entraînement. C'est là que le masquage entre en jeu.\n","\n","Le masque est généralement une matrice triangulaire inférieure, dans laquelle les éléments situés au-dessus de la diagonale principale sont fixés à une grande valeur négative (généralement `-inf`), et les éléments situés au-dessous ou sur la diagonale principale sont fixés à zéro. Cela garantit que lorsque le masque est ajouté aux scores d'attention, l'opération softmax ignorera effectivement les positions masquées, car l'exponentialisation de grandes valeurs négatives aboutira à des valeurs proches de zéro.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"af74c2d6","metadata":{"id":"af74c2d6","outputId":"99b232b4-7b34-46c4-d1b9-1cb806924b03"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mask:\n","tensor([[0., -inf, -inf, -inf, -inf],\n","        [0., 0., -inf, -inf, -inf],\n","        [0., 0., 0., -inf, -inf],\n","        [0., 0., 0., 0., -inf],\n","        [0., 0., 0., 0., 0.]])\n"]}],"source":["\n","\n","def generate_mask(seq_len):\n","    mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","seq_len = 5\n","mask = generate_mask(seq_len)\n","\n","print(\"Mask:\")\n","print(mask)\n"]},{"cell_type":"code","execution_count":null,"id":"569ea6b8","metadata":{"id":"569ea6b8"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}