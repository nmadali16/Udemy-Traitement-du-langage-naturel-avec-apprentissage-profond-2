{"cells":[{"cell_type":"markdown","metadata":{"id":"fOaCES6J_FKK"},"source":["# Transformateurs Hugging Face\n","\n","Hugging Face est une organisation qui s'efforce de résoudre et de démocratiser l'IA par le biais du langage naturel. Leur bibliothèque open-source 'transformers' est très populaire parmi la communauté NLP. Elle est très utile et puissante pour plusieurs tâches NLP et NLU. Elle comprend des milliers de modèles pré-entraînés dans plus de 100 langues. L'un des nombreux avantages de la bibliothèque transformers est qu'elle est compatible avec PyTorch et TensorFlow.\n","\n","Nous pouvons installer les transformateurs directement à l'aide de pip comme indiqué ci-dessous :"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"o7PQZSHt_FKT"},"outputs":[],"source":["%%capture\n","!pip install transformers==3.5.1"]},{"cell_type":"markdown","metadata":{"id":"5ehF5uSR_FKZ"},"source":["Comme nous pouvons le voir, dans ce livre, nous utilisons la version 3.5.1 de Transformers. Maintenant que nous avons installé les transformateurs, commençons."]},{"cell_type":"markdown","metadata":{"id":"wi1Nq51R_FKc"},"source":["# Génération de l'intégration de BERT\n","Dans cette section, nous allons apprendre à extraire des embeddings à partir du BERT pré-entraîné. Considérons la phrase \"J'aime Paris\". Voyons comment obtenir l'encastrement contextualisé de tous les mots de la phrase en utilisant le modèle BERT pré-entraîné avec la bibliothèque de transformateurs de Hugging Face.\n","\n","Tout d'abord, importons les modules nécessaires :"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"qmWTWzRX_FKe"},"outputs":[],"source":["from transformers import BertModel, BertTokenizer\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"zFPzX86E_FKh"},"source":["Ensuite, nous téléchargeons le modèle BERT pré-entraîné. Nous pouvons vérifier tous les modèles BERT pré-entraînés disponibles ici - https://huggingface.co/transformers/pre-trained_models.html.We utiliser le modèle 'bert-base-uncased'. Comme son nom l'indique, il s'agit du modèle BERT-base avec 12 encodeurs et il est entraîné avec des mots non accentués. Puisque nous utilisons la base BERT, la taille de la représentation sera de 768.\n","\n","Téléchargez et chargez le modèle pré-entraîné bert-base-uncasedmodel :"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["1758400be5b14d1383a14be51f129388","66169204df0344a78bb29b5c66235f38","96a5d14686674cebaab3fe3457579f15","71872c1d14f04434a502db8328803169","a54875acc17044a0b6683d9cc94ed95c","e900b312c23641549d729f1e9f98849a","6e66fccb4d4e4518acdefab5a51c9708","d07c2f696ad544b7833925b73219da72","e79a470be88c4c2e9c5a7cc7e3eca6b1","2320668b356245449a2ba6991e21dc71","8c195883eb9f453781baf994903a07c4","420c835be5304faeb77864c5f924b768","ab9cede857e142f69b227bd15e7ac825","08fc204525d24f0ca56c42c2890737ca","f7a38dcc24f14a298d540d8827554757","084587c872cb464f8afc0f88e6b33452"]},"id":"swqqjz08_FKj","outputId":"e1ca7551-d012-41d0-e38e-d46c3169d9b8"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1758400be5b14d1383a14be51f129388","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e79a470be88c4c2e9c5a7cc7e3eca6b1","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["model = BertModel.from_pretrained('bert-base-uncased')"]},{"cell_type":"markdown","metadata":{"id":"KODXve4P_FKl"},"source":["Ensuite, nous téléchargeons et chargeons le tokenizer qui est utilisé pour le pré-entraînement du modèle sans base de Bert :"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["a1fc82d07a6c4a46bef6b105b3f53bac","b36a5b3e28394ee39058c537572ecd72","67196c8f26d3422daf6c27589757b480","7d1ef8f4cbfe46e1a8f9d6f6c944e93a","6835f5dcea2e4832a1dc76dc28e24dd0","7de7bcd4a0d841298de7f5ac7e5cc81c","c9bc07b32ba54e3cbe26bf13aec9ece9","8c7da9ea9fec4e7fbdc6ad56ee63c33f"]},"id":"lPHa9dI5_FKn","outputId":"06f9ae89-87b2-49d4-c898-60d13a34c20d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1fc82d07a6c4a46bef6b105b3f53bac","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"markdown","metadata":{"id":"qIHYHG5A_FKr"},"source":["Voyons maintenant comment prétraiter l'entrée avant de l'envoyer au BERT."]},{"cell_type":"markdown","metadata":{"id":"j8El2dcG_FKs"},"source":["## Prétraitement de l'entrée\n","Définir la phrase :"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"8Hkc5o3B_FKu"},"outputs":[],"source":["sentence = 'I love Paris'"]},{"cell_type":"markdown","metadata":{"id":"FVn3-2XM_FKv"},"source":["Tokeniser la phrase et obtenir les tokens :"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"4Ui-3oWr_FKw"},"outputs":[],"source":["tokens = tokenizer.tokenize(sentence)"]},{"cell_type":"markdown","metadata":{"id":"RSYRLcvV_FKy"},"source":["Imprimons les jetons :"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGHtPj5H_FKz","outputId":"96afb8ca-8565-4b69-dad2-fc9471dd7227"},"outputs":[{"name":"stdout","output_type":"stream","text":["['i', 'love', 'paris']\n"]}],"source":["print(tokens)"]},{"cell_type":"markdown","metadata":{"id":"rL8VgpcM_FK0"},"source":["Nous allons maintenant ajouter le jeton [CLS] au début et le jeton [SEP] à la fin de la liste des jetons :"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3CpARvDc_FK0"},"outputs":[],"source":["tokens = ['[CLS]'] + tokens + ['[SEP]']"]},{"cell_type":"markdown","metadata":{"id":"jbFEyrk7_FK2"},"source":["Voyons la liste des jetons mise à jour :"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uaf_JScw_FK3","outputId":"82f09500-1324-4022-9b5a-e165716c2443"},"outputs":[{"name":"stdout","output_type":"stream","text":["['[CLS]', 'i', 'love', 'paris', '[SEP]']\n"]}],"source":["print(tokens)"]},{"cell_type":"markdown","metadata":{"id":"wVD29L7A_FK3"},"source":["Comme nous pouvons le constater, nous avons le jeton [CLS] au début et le jeton sep à la fin de notre liste de jetons. Nous pouvons également observer que la longueur de nos jetons est de 5.\n","\n","Supposons que nous devions maintenir la longueur de notre liste de jetons à 7. Dans ce cas, nous ajouterons deux jetons [PAD] à la fin de la liste, comme indiqué ci-dessous :"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"cqMI--rA_FK4"},"outputs":[],"source":["tokens = tokens + ['[PAD]'] + ['[PAD]']"]},{"cell_type":"markdown","metadata":{"id":"XEEHs7CV_FK6"},"source":["Imprimons notre liste de jetons mise à jour :"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2tKmh_hk_FK6","outputId":"31926f5d-4329-4c21-fc57-b3318b8f0c30"},"outputs":[{"name":"stdout","output_type":"stream","text":["['[CLS]', 'i', 'love', 'paris', '[SEP]', '[PAD]', '[PAD]']\n"]}],"source":["print(tokens)"]},{"cell_type":"markdown","metadata":{"id":"aPd9TPWE_FK7"},"source":["Comme nous pouvons le constater, la liste de jetons est maintenant composée de jetons [PAD] et la longueur de notre liste de jetons est de 7.\n","\n","Ensuite, nous créons le masque d'attention. Nous fixons la valeur du masque d'attention à 1 si le jeton n'est pas un jeton [PAD], sinon nous fixons la valeur du masque d'attention à 0, comme indiqué ci-dessous :"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3SbiaBws_FK8"},"outputs":[],"source":["attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]"]},{"cell_type":"markdown","metadata":{"id":"SELKgxsG_FK9"},"source":["Imprimons le masque d'attention :"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmtZKFFq_FK-","outputId":"83e13594-2f24-427d-cd99-bfba8930adaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 1, 1, 1, 1, 0, 0]\n"]}],"source":["print(attention_mask)"]},{"cell_type":"markdown","metadata":{"id":"qvtvC8HW_FK-"},"source":["Comme nous pouvons le constater, les valeurs du masque d'attention sont 0 à la position où se trouve le jeton [PAD] et 1 aux autres positions.\n","\n","Ensuite, nous convertissons tous les tokens en leurs token_ids, comme indiqué ci-dessous :"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"KGbsHfxL_FK_"},"outputs":[],"source":["token_ids = tokenizer.convert_tokens_to_ids(tokens)"]},{"cell_type":"markdown","metadata":{"id":"zaQWwz_y_FK_"},"source":["Jetons un coup d'œil aux token_ids :"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iih_wWeT_FLA","outputId":"f83bd6c5-300a-4c1a-fac9-dc54db1e157b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[101, 1045, 2293, 3000, 102, 0, 0]\n"]}],"source":["print(token_ids)"]},{"cell_type":"markdown","metadata":{"id":"S9RL1cBs_FLA"},"source":["D'après la sortie ci-dessus, nous pouvons observer que chaque jeton est associé à un identifiant de jeton unique.\n","\n","Nous convertissons maintenant les token_ids et les attention_mask en tenseurs, comme indiqué ci-dessous :"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"KQzsDUez_FLB"},"outputs":[],"source":["token_ids = torch.tensor(token_ids).unsqueeze(0)\n","attention_mask = torch.tensor(attention_mask).unsqueeze(0)"]},{"cell_type":"markdown","metadata":{"id":"pQn5de_a_FLC"},"source":["C'est tout. Ensuite, nous transmettons les token_ids et attention_mask au modèle BERT pré-entraîné et obtenons l'intégration."]},{"cell_type":"markdown","metadata":{"id":"qYIDWAO-_FLD"},"source":["## Obtenir l'intégration\n","\n","Comme le montre le code suivant, nous transmettons les token_ids et les attention_mask au modèle et obtenons les embeddings. Notez que le modèle renvoie la sortie sous la forme d'un tuple avec deux valeurs. La première valeur indique la représentation de l'état caché, hidden_rep, et consiste en la représentation de tous les tokens obtenus à partir de l'encodeur final (encodeur 12), et la seconde valeur, cls_head, consiste en la représentation du token [CLS] :"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Rh-Ohh71_FLE"},"outputs":[],"source":["hidden_rep, cls_head = model(token_ids, attention_mask = attention_mask)"]},{"cell_type":"markdown","metadata":{"id":"F3Kfvd3F_FLF"},"source":["Dans le code précédent, hidden_rep contient l'intégration (représentation) de tous les tokens de notre entrée. Imprimons la forme du tenseur hidden_rep :"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gx1RfsaU_FLF","outputId":"8c50bb57-c886-4d3a-d029-e2c87e43adda"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 7, 768])\n"]}],"source":["print(hidden_rep.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"zWxFowIS_FLG"},"source":["La taille [1,7,768] indique le [batch_size, sequence_length, hidden_size].\n","\n","Notre taille de lot est 1, la longueur de séquence est la longueur du jeton, puisque nous avons 7 jetons, la longueur de séquence est 7, et la taille cachée est la taille de représentation (embedding) et elle est 768 pour le modèle BERT-base.\n","\n","Nous pouvons obtenir la représentation de chaque jeton comme suit :\n","\n","- hidden_rep[0][0] donne la représentation du premier token qui est [CLS]\n","- hidden_rep[0][1] donne la représentation du deuxième jeton, qui est \"I\".\n","- hidden_repo[0][2] donne la représentation du troisième jeton qui est 'love'\n","\n","De cette manière, nous pouvons obtenir la représentation contextuelle de tous les jetons. Il s'agit en fait de l'enchâssement contextualisé de tous les mots de la phrase donnée.\n","\n","Jetons maintenant un coup d'œil sur le cls_head. Il contient la représentation du token [CLS]. Imprimons la forme de cls_head :"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zAL6KEXw_FLI","outputId":"fb3a0ca9-9a1a-44e2-f292-d0b2c3f9bd68"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 768])\n"]}],"source":["print(cls_head.shape)"]},{"cell_type":"markdown","metadata":{"id":"UL8eMwZY_FLJ"},"source":["Le code ci-dessus s'imprimera :\n","\n","torch.Size([1, 768])\n","\n","La taille [1,768] indique le [batch_size, hidden_size].\n","\n","Nous avons appris que cls_head contient la représentation agrégée de la phrase, nous pouvons donc utiliser cls_head comme représentation de la phrase donnée \"J'aime Paris\".\n","\n","Nous avons appris à extraire des embeddings du BERT pré-entraîné. Mais il s'agit des encastrements obtenus uniquement à partir de la couche d'encodage la plus élevée du BERT, à savoir l'encodeur 12. Pouvons-nous également extraire les enchâssements de toutes les couches d'encodage du BERT ? Oui ! Nous verrons comment procéder dans la section suivante."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"084587c872cb464f8afc0f88e6b33452":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"08fc204525d24f0ca56c42c2890737ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"1758400be5b14d1383a14be51f129388":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96a5d14686674cebaab3fe3457579f15","IPY_MODEL_71872c1d14f04434a502db8328803169"],"layout":"IPY_MODEL_66169204df0344a78bb29b5c66235f38"},"model_module_version":"1.5.0"},"2320668b356245449a2ba6991e21dc71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"420c835be5304faeb77864c5f924b768":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_084587c872cb464f8afc0f88e6b33452","placeholder":"​","style":"IPY_MODEL_f7a38dcc24f14a298d540d8827554757","value":" 440M/440M [00:07&lt;00:00, 61.5MB/s]"},"model_module_version":"1.5.0"},"66169204df0344a78bb29b5c66235f38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"67196c8f26d3422daf6c27589757b480":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_7de7bcd4a0d841298de7f5ac7e5cc81c","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6835f5dcea2e4832a1dc76dc28e24dd0","value":231508},"model_module_version":"1.5.0"},"6835f5dcea2e4832a1dc76dc28e24dd0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"},"model_module_version":"1.5.0"},"6e66fccb4d4e4518acdefab5a51c9708":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""},"model_module_version":"1.5.0"},"71872c1d14f04434a502db8328803169":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d07c2f696ad544b7833925b73219da72","placeholder":"​","style":"IPY_MODEL_6e66fccb4d4e4518acdefab5a51c9708","value":" 433/433 [00:00&lt;00:00, 7.42kB/s]"},"model_module_version":"1.5.0"},"7d1ef8f4cbfe46e1a8f9d6f6c944e93a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c7da9ea9fec4e7fbdc6ad56ee63c33f","placeholder":"​","style":"IPY_MODEL_c9bc07b32ba54e3cbe26bf13aec9ece9","value":" 232k/232k [00:00&lt;00:00, 313kB/s]"},"model_module_version":"1.5.0"},"7de7bcd4a0d841298de7f5ac7e5cc81c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"8c195883eb9f453781baf994903a07c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_08fc204525d24f0ca56c42c2890737ca","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab9cede857e142f69b227bd15e7ac825","value":440473133},"model_module_version":"1.5.0"},"8c7da9ea9fec4e7fbdc6ad56ee63c33f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"96a5d14686674cebaab3fe3457579f15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_e900b312c23641549d729f1e9f98849a","max":433,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a54875acc17044a0b6683d9cc94ed95c","value":433},"model_module_version":"1.5.0"},"a1fc82d07a6c4a46bef6b105b3f53bac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_67196c8f26d3422daf6c27589757b480","IPY_MODEL_7d1ef8f4cbfe46e1a8f9d6f6c944e93a"],"layout":"IPY_MODEL_b36a5b3e28394ee39058c537572ecd72"},"model_module_version":"1.5.0"},"a54875acc17044a0b6683d9cc94ed95c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"},"model_module_version":"1.5.0"},"ab9cede857e142f69b227bd15e7ac825":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"},"model_module_version":"1.5.0"},"b36a5b3e28394ee39058c537572ecd72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"c9bc07b32ba54e3cbe26bf13aec9ece9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""},"model_module_version":"1.5.0"},"d07c2f696ad544b7833925b73219da72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"e79a470be88c4c2e9c5a7cc7e3eca6b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8c195883eb9f453781baf994903a07c4","IPY_MODEL_420c835be5304faeb77864c5f924b768"],"layout":"IPY_MODEL_2320668b356245449a2ba6991e21dc71"},"model_module_version":"1.5.0"},"e900b312c23641549d729f1e9f98849a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null},"model_module_version":"1.2.0"},"f7a38dcc24f14a298d540d8827554757":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""},"model_module_version":"1.5.0"}}}},"nbformat":4,"nbformat_minor":0}