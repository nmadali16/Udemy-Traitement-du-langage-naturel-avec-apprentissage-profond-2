{"cells":[{"cell_type":"markdown","metadata":{"id":"sj3LxyO9q21U"},"source":["# Construire un système QA avec BERT sur Wikipedia"]},{"cell_type":"markdown","source":["Les systèmes de question-réponse à domaine ouvert (ODQA) se composent généralement de deux composants principaux : un récupérateur et un lecteur. Ces composants travaillent ensemble pour extraire des informations pertinentes d'un document donné et générer une réponse à la question de l'utilisateur. Voici un aperçu de leur fonctionnement :\n","\n","### 1. **Récupérateur :**\n","   - **Objectif :** Le rôle du récupérateur est d'identifier rapidement un sous-ensemble de documents susceptibles de contenir la réponse à la question de l'utilisateur. Il s'agit souvent de ce qu'on appelle la récupération de documents.\n","   - **Techniques :**\n","      - **TF-IDF (Term Frequency-Inverse Document Frequency) :** Calcule l'importance des mots dans un document par rapport à leur fréquence dans une collection de documents.\n","      - **BM25 (Best Matching 25) :** Une variante de TF-IDF qui prend en compte la longueur du document et la saturation des termes.\n","      - **Modèles de Classement Neuronaux :** Des modèles d'apprentissage automatique, tels que ceux basés sur BERT, formés pour classer les documents en fonction de leur pertinence par rapport à une requête donnée.\n","   - **Sortie :** Le récupérateur fournit une liste classée de documents en fonction de leur probabilité de contenir des informations pertinentes.\n","\n","### 2. **Lecteur :**\n","   - **Objectif :** Une fois que le récupérateur a identifié un sous-ensemble de documents, le lecteur est chargé d'extraire des passages spécifiques ou des phrases qui contiennent la réponse à la question de l'utilisateur.\n","   - **Techniques :**\n","      - **Modèles de Compréhension de Lecture Automatique (MRC) :** BERT, GPT et des modèles similaires affinés spécifiquement pour extraire des réponses de passages.\n","      - **Prédiction de Plage :** Le lecteur prédit les positions de début et de fin de la réponse à l'intérieur du passage sélectionné.\n","      - **Attention Bidirectionnelle :** Des modèles capables de prendre en compte le contexte à la fois à gauche et à droite de chaque mot, permettant une compréhension plus complète du passage.\n","   - **Sortie :** Le lecteur produit la réponse finale en sélectionnant le passage de texte le plus pertinent parmi les documents récupérés.\n","\n","### Flux de travail de la Question-Réponse à Domaine Ouvert :\n","\n","1. **Requête de l'utilisateur :** Un utilisateur saisit une question dans le système.\n","\n","2. **Récupération de Documents :**\n","   - Le récupérateur classe les documents en fonction de leur pertinence par rapport à la question.\n","   - Les documents les mieux classés sont sélectionnés pour un traitement ultérieur.\n","\n","3. **Extraction de Passages :**\n","   - Le lecteur traite les documents sélectionnés pour identifier les passages susceptibles de contenir la réponse.\n","   - Cela implique la tokenisation du texte et la détermination des parties les plus saillantes.\n","\n","4. **Extraction de la Réponse :**\n","   - Le lecteur restreint davantage la recherche pour identifier le passage exact à l'intérieur des passages qui répond à la question.\n","   - Cela peut impliquer de prédire les positions de début et de fin de la réponse.\n","\n","5. **Présentation de la Réponse :**\n","   - La réponse finale est présentée à l'utilisateur.\n","\n","### Défis et Considérations :\n","- **Évolutivité :** Gérer efficacement de grandes collections de documents.\n","- **Taille du Modèle :** Équilibrer la taille des modèles pour la précision et l'efficacité computationnelle.\n","- **Raisonnement à Plusieurs Niveaux :** Répondre aux questions nécessitant des informations provenant de plusieurs documents.\n","\n","Les systèmes de question-réponse à domaine ouvert ont connu des avancées significatives, en particulier avec l'utilisation de grands modèles de langage pré-entraînés, les rendant capables de comprendre et d'extraire des informations à partir de sources diverses."],"metadata":{"id":"aGPeHTkjEyQm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKnRD9GYq21c"},"outputs":[],"source":["!pip install transformers\n","!pip install wikipedia==1.4.0"]},{"cell_type":"markdown","metadata":{"id":"rMiiq1-Dq21c"},"source":["# Transformateurs Hugging Face\n","Le paquet [Hugging Face Transformers] (https://huggingface.co/transformers/#) fournit des architectures polyvalentes de pointe pour la compréhension et la génération de langage naturel. Il héberge des dizaines de modèles pré-entraînés fonctionnant dans plus de 100 langues que vous pouvez utiliser dès la sortie de la boîte. Tous ces modèles sont livrés avec une interopérabilité profonde entre PyTorch et Tensorflow 2.0, ce qui signifie que vous pouvez déplacer un modèle de TF2.0 à PyTorch et vice-versa avec seulement une ligne ou deux de code !\n","\n","\n","Si vous ne connaissez pas Hugging Face, nous vous recommandons fortement de lire le [Quickstart guide] de HF (https://huggingface.co/transformers/quickstart.html) ainsi que leurs excellents [Transformer Notebooks] (https://huggingface.co/transformers/notebooks.html) (nous l'avons fait !), car nous n'aborderons pas ce sujet dans ce cahier. Nous utiliserons [`AutoClasses`](https://huggingface.co/transformers/model_doc/auto.html), qui sert d'enveloppe autour de presque toutes les classes de base de Transformer."]},{"cell_type":"markdown","metadata":{"id":"8IALS9o9q21d"},"source":["## Mise au point d'un modèle de transformateur pour la réponse aux questions\n","\n","Pour entraîner un Transformer pour l'AQ avec Hugging Face, nous aurons besoin de\n","1. choisir une architecture de modèle spécifique,\n","2. un ensemble de données d'AQ, et\n","3. le script de formation.\n","\n","Avec ces trois éléments en main, nous allons ensuite parcourir le processus de réglage fin.\n","\n","### 1. Choisir un modèle\n","Toutes les architectures Transformer ne se prêtent pas naturellement à la réponse aux questions. Par exemple, GPT ne fait pas QA ; de même, BERT ne fait pas de traduction automatique.  HF identifie les types de modèles suivants pour la tâche d'AQ :\n","\n","- BERT\n","- distilBERT\n","- ALBERT\n","- RoBERTa\n","- XLNet\n","- XLM\n","- FlauBERT\n","\n","\n","Nous nous en tiendrons au modèle BERT désormais classique dans ce carnet, mais n'hésitez pas à en essayer d'autres (nous le ferons - et nous vous en informerons). Prochaine étape : un ensemble d'entraînement.\n","\n","\n","### 2. Jeu de données AQ : SQuAD\n","L'un des ensembles de données les plus classiques pour l'AQ est le Stanford Question Answering Dataset, ou SQuAD, qui existe en deux versions : SQuAD 1.1 et SQuAD 2.0. Ces ensembles de données de compréhension de la lecture consistent en des questions posées sur un ensemble d'articles de Wikipédia, où la réponse à chaque question est un segment (ou une portée) du passage correspondant. Dans SQuAD 1.1, toutes les questions ont une réponse dans le passage correspondant. SQuAD 2.0 augmente la difficulté en incluant des questions auxquelles le passage fourni ne permet pas de répondre.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ksAaX4k4q21g"},"source":["## Utilisation d'un modèle pré-affiné provenant du dépôt Hugging Face\n","Si vous n'avez pas accès aux GPU ou si vous n'avez pas le temps de bricoler et d'entraîner des modèles, vous avez de la chance ! Hugging Face est bien plus qu'une collection de classes Transformer élégantes - il héberge également [un référentiel] (https://huggingface.co/models) pour les modèles pré-entraînés et affinés provenant de la vaste communauté des praticiens du NLP. Une recherche sur \"squad\" donne au moins 55 modèles.\n","\n","![](https://github.com/fastforwardlabs/ff14_blog/blob/master/_notebooks/my_icons/HF_repo.png?raw=1)\n","\n","\n","Chacun de ces liens fournit un code explicite pour l'utilisation du modèle et, dans certains cas, des informations sur la manière dont il a été entraîné et sur les résultats obtenus. Chargeons l'un de ces modèles prédéfinis."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"AqxhfxNMq21g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699704807439,"user_tz":-60,"elapsed":5135,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"91898055-fb71-4b33-f45f-af3ad285022b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","\n","# l'exécution de ces commandes pour la première fois initie un téléchargement des\n","# poids du modèle dans ~/.cache/torch/transformers/\n","tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n","model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")"]},{"cell_type":"markdown","metadata":{"id":"jLtvCInSq21g"},"source":["## Essayons notre modèle !\n","\n","Que vous ayez peaufiné votre propre modèle ou que vous ayez utilisé un modèle déjà peaufiné, il est temps de jouer avec ! QA comporte trois étapes :\n","1. symboliser les données d'entrée\n","2. obtenir les scores du modèle\n","3. obtenir l'étendue de la réponse\n","\n","Ces étapes sont examinées en détail dans les HF [Carnets de notes des transformateurs] (https://huggingface.co/transformers/notebooks.html)."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"SfG5kTvMq21g","outputId":"f1c6eb56-e775-44a5-e595-5bb83a3aa8f2","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1699704808651,"user_tz":-60,"elapsed":1216,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'the Argead dynasty'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["question = \"Who ruled Macedonia\"\n","\n","context = \"\"\"Macedonia was an ancient kingdom on the periphery of Archaic and Classical Greece,\n","and later the dominant state of Hellenistic Greece. The kingdom was founded and initially ruled\n","by the Argead dynasty, followed by the Antipatrid and Antigonid dynasties. Home to the ancient\n","Macedonians, it originated on the northeastern part of the Greek peninsula. Before the 4th\n","century BC, it was a small kingdom outside of the area dominated by the city-states of Athens,\n","Sparta and Thebes, and briefly subordinate to Achaemenid Persia.\"\"\"\n","\n","\n","# 1. SYMBOLISER L'ENTRÉE\n","# note : si vous n'incluez pas return_tensors='pt' vous obtiendrez une liste de listes ce qui est plus facile pour\n","# l'exploration, mais vous ne pouvez pas l'introduire dans un modèle.\n","inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n","\n","# 2. OBTENIR LES SCORES DU MODÈLE\n","# La classe AutoModelForQuestionAnswering inclut un prédicteur d'empan au-dessus du modèle.\n","# Le modèle renvoie les scores de début et de fin de réponse pour chaque mot du texte.\n","outputs = model(**inputs)\n","answer_start = torch.argmax(outputs.start_logits)  # obtenir le début de réponse le plus probable avec l'argmax du score\n","answer_end = torch.argmax(outputs.end_logits) + 1  # obtenir la fin de réponse la plus probable avec l'argmax du score\n","\n","# 3. OBTENIR LA RÉPONSE SPAN\n","# une fois que nous avons les tokens de début et de fin les plus probables, nous prenons tous les tokens entre eux\n","# et convertissons les tokens en mots !\n","tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))"]},{"cell_type":"markdown","metadata":{"id":"5JKPEGjVq21h"},"source":["# AQ sur les pages Wikipédia\n","Nous avons testé notre modèle sur une question associée à un court passage, mais que se passe-t-il si nous voulons extraire une réponse d'un document plus long ? Une page Wikipédia typique est beaucoup plus longue que l'exemple ci-dessus, et nous devons faire quelques manipulations avant de pouvoir utiliser notre modèle dans des contextes plus longs.\n","\n","Commençons par afficher une page Wikipédia."]},{"cell_type":"code","execution_count":4,"metadata":{"scrolled":true,"id":"lGze0BK1q21h","outputId":"2905a3c0-6267-419d-ed49-2d2dc7ec7b9c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699704810006,"user_tz":-60,"elapsed":1361,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Résultats de la recherche Wikipedia pour notre question:\n","\n","['Wandering albatross',\n"," 'Black-browed albatross',\n"," 'List of largest birds',\n"," 'Argentavis',\n"," 'Pelagornis',\n"," 'List of birds by flight speed',\n"," 'Mollymawk',\n"," 'Largest body part',\n"," 'Largest and heaviest animals',\n"," 'Orders of magnitude (length)']\n","\n"," L'article Black-browed albatross  de Wikipedia contient 16588 caractères.\n"]}],"source":["import wikipedia as wiki\n","import pprint as pp\n","\n","question = 'What is the wingspan of an albatross?'\n","\n","results = wiki.search(question)\n","print(\"Résultats de la recherche Wikipedia pour notre question:\\n\")\n","pp.pprint(results)\n","\n","page = wiki.page(results[1])\n","text = page.content\n","print(f\"\\n L'article {results[1]}  de Wikipedia contient {len(text)} caractères.\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"8lyhBvBtq21h","outputId":"2aa06281-c522-4c17-ac59-09d693d0c850","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699704810230,"user_tz":-60,"elapsed":226,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (4914 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["Cela se traduit par 4914 tokens.\n"]}],"source":["inputs = tokenizer.encode_plus(question, text, return_tensors='pt')\n","print(f\"Cela se traduit par {len(inputs['input_ids'][0])} tokens.\")"]},{"cell_type":"markdown","metadata":{"id":"E97KxNvlq21h"},"source":["Le tokéniseur prend l'entrée sous forme de texte et renvoie des tokens. En général, les tokenizers convertissent des mots ou des morceaux de mots dans un format compatible avec le modèle. Les tokens et le format spécifiques dépendent du type de modèle. Par exemple, BERT génère les mots différemment de RoBERTa. Veillez donc à toujours utiliser le tokéniseur associé qui convient à votre modèle.\n","\n","Dans ce cas, le tokenizer convertit notre texte d'entrée en 8824 tokens, mais cela dépasse de loin le nombre maximum de tokens qui peuvent être introduits dans le modèle en une seule fois. La plupart des modèles de type BERT ne peuvent accepter que 512 tokens à la fois, d'où l'avertissement (quelque peu déroutant) ci-dessus (comment se fait-il que 10 > 512 ?). Cela signifie que nous devrons diviser notre entrée en morceaux et que chaque morceau ne doit pas dépasser 512 tokens au total.\n","\n","Lorsque l'on travaille avec la réponse aux questions, il est essentiel que chaque morceau suive le format suivant :\n","\n","[CLS] jetons de question [SEP] jetons de contexte [SEP]\n","\n","Cela signifie que, pour chaque segment d'un article de Wikipédia, nous devons faire précéder la question d'origine, suivie du \"chunk\" suivant de tokens d'article."]},{"cell_type":"code","execution_count":6,"metadata":{"scrolled":false,"id":"cpq-OfSGq21h","outputId":"51d70c1d-da63-4d53-d5cc-908f9399db2d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699704810230,"user_tz":-60,"elapsed":8,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["La question consiste en 12 jetons.\n","Chaque morceau contiendra 497 jetons de l'article de Wikipédia.\n"]}],"source":["# time to chunk !\n","from collections import OrderedDict\n","# identifier les jetons de la question (token_type_ids = 0)\n","qmask = inputs['token_type_ids'].lt(1)\n","qt = torch.masked_select(inputs['input_ids'], qmask)\n","print(f\"La question consiste en {qt.size()[0]} jetons.\")\n","\n","chunk_size = model.config.max_position_embeddings - qt.size()[0] - 1 # the \"-1\" accounts for\n","# avoir à ajouter un jeton [SEP] à la fin de chaque morceau\n","print(f\"Chaque morceau contiendra {chunk_size - 2} jetons de l'article de Wikipédia.\")\n","\n","# créer un dict de dicts ; chaque sous-dict imite la structure de l'entrée du modèle pré-chunked\n","chunked_input = OrderedDict()\n","for k,v in inputs.items():\n","    q = torch.masked_select(v, qmask)\n","    c = torch.masked_select(v, ~qmask)\n","    chunks = torch.split(c, chunk_size)\n","\n","    for i, chunk in enumerate(chunks):\n","        if i not in chunked_input:\n","            chunked_input[i] = {}\n","\n","        thing = torch.cat((q, chunk))\n","        if i != len(chunks)-1:\n","            if k == 'input_ids':\n","                thing = torch.cat((thing, torch.tensor([102])))\n","            else:\n","                thing = torch.cat((thing, torch.tensor([1])))\n","\n","        chunked_input[i][k] = torch.unsqueeze(thing, dim=0)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"SwsOvDLQq21h","outputId":"cb4f4c13-e047-4aca-ed3f-6e4520d33be9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699704810230,"user_tz":-60,"elapsed":6,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Nombre de jetons dans le bloc0: 512\n","Nombre de jetons dans le bloc1: 512\n","Nombre de jetons dans le bloc2: 512\n","Nombre de jetons dans le bloc3: 512\n","Nombre de jetons dans le bloc4: 512\n","Nombre de jetons dans le bloc5: 512\n","Nombre de jetons dans le bloc6: 512\n","Nombre de jetons dans le bloc7: 512\n","Nombre de jetons dans le bloc8: 512\n","Nombre de jetons dans le bloc9: 423\n"]}],"source":["for i in range(len(chunked_input.keys())):\n","    print(f\"Nombre de jetons dans le bloc{i}: {len(chunked_input[i]['input_ids'].tolist()[0])}\")"]},{"cell_type":"markdown","metadata":{"id":"lwYTS5IYq21i"},"source":["Each of these chunks (except for the last one) has the following structure:\n","\n","[CLS], 12 question tokens, [SEP], 497 tokens of the Wikipedia article, [SEP] token = 512 tokens\n","\n","Each of these chunks can now be fed to the model without causing indexing errors. We'll get an \"answer\" for each chunk; however, not all answers are useful, since not every segment of a Wikipedia article is informative for our question. The model will return the [CLS] token when it determines that the context does not contain an answer to the question."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"xqPjxOWeq21i","outputId":"f0f6df06-9281-4fd6-e2db-00c0b5756272","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699704837258,"user_tz":-60,"elapsed":27033,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]\n","200 to 240 cm ( 79 – 94 in )\n","[CLS]\n","[CLS]\n","[CLS]\n","[CLS]\n","[CLS]\n","[CLS]\n","[CLS]\n","[CLS]\n","200 to 240 cm ( 79 – 94 in ) / \n"]}],"source":["def convert_ids_to_string(tokenizer, input_ids):\n","    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))\n","\n","answer = ''\n","\n","# maintenant nous itérons sur nos chunks, en cherchant la meilleure réponse pour chaque chunk\n","for _, chunk in chunked_input.items():\n","    outputs = model(**chunk)\n","\n","    answer_start = torch.argmax(outputs.start_logits)  # obtenir le début de réponse le plus probable avec l'argmax du score\n","    answer_end = torch.argmax(outputs.end_logits) + 1  # obtenir la fin de réponse la plus probable avec l'argmax du score\n","\n","\n","    ans = convert_ids_to_string(tokenizer, chunk['input_ids'][0][answer_start:answer_end])\n","    print(ans)\n","    # Si la réponse == [CLS], le modèle n'a pas trouvé de réponse réelle dans ce morceau.\n","    if ans != '[CLS]':\n","        answer += ans + \" / \"\n","\n","print(answer)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}