{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"EKOTlwcmxmej"},"source":["# BERT Fine-Tuning\n","\n","By Chris McCormick and Nick Ryan"]},{"cell_type":"markdown","source":["Le jeton `[CLS]` dans BERT (Bidirectional Encoder Representations from Transformers) sert de jeton spécial utilisé pour diverses tâches, y compris le fine-tuning. Voici comment il est généralement utilisé :\n","\n","1. **Phase de pré-entraînement** :\n","   - Pendant la phase de pré-entraînement de BERT, étant donné 2 phrases, le modèle apprend à prédire si la 2ème phrase est la vraie phrase, qui suit la 1ère phrase. Pour cette tâche, nous avons besoin d'un  jeton [CLS], dont la sortie nous indiquera la probabilité que la phrase actuelle soit la phrase suivante de la 1ère phrase.\n","\n","2. **Phase de fine-tuning** :\n","   - Dans la phase de fine-tuning, le modèle BERT pré-entraîné peut être adapté à des tâches spécifiques comme la classification de texte, la reconnaissance d'entités nommées, les questions-réponses, etc. L'état caché final du jeton `[CLS]` peut être utilisé comme entrée pour un classificateur spécifique à la tâche.\n","\n","   - Pour les tâches de classification de texte, vous pouvez utiliser l'état caché final du jeton `[CLS]` et l'injecter dans un classificateur linéaire simple (par exemple, une seule couche dense) pour effectuer des prédictions pour la tâche spécifique.\n","\n","   - Pour des tâches comme la reconnaissance d'entités nommées ou les questions-réponses, vous pourriez avoir besoin d'ajouter des couches supplémentaires et de fine-tuner le modèle davantage.\n","\n","   - Essentiellement, vous exploitez la capacité du modèle BERT pré-entraîné à comprendre le contexte et l'utilisez comme extracteur de caractéristiques pour votre tâche spécifique.\n","\n","   - Le processus de fine-tuning implique généralement l'entraînement des couches supplémentaires spécifiques à la tâche tout en maintenant les couches pré-entraînées de BERT gelées (ou en les fine-tunant avec un taux d'apprentissage très faible).\n","\n","   - L'état caché final du jeton `[CLS]` capture des informations sur l'ensemble de la séquence d'entrée, ce qui est particulièrement utile pour les tâches qui nécessitent une compréhension du contexte global.\n","\n"],"metadata":{"id":"QZda6GBXiie7"}},{"cell_type":"markdown","metadata":{"id":"2ElsnSNUridI"},"source":["## Installation de la bibliothèque Hugging Face"]},{"cell_type":"markdown","metadata":{"id":"G_N2UDLevYWn"},"source":["Installons le paquet [transformers](https://github.com/huggingface/transformers) de Hugging Face qui nous donnera une interface pytorch pour travailler avec BERT (cette bibliothèque contient des interfaces pour d'autres modèles de langage pré-entraînés comme GPT et GPT-2 d'OpenAI). Nous avons choisi l'interface pytorch parce qu'elle établit un bon équilibre entre les API de haut niveau (qui sont faciles à utiliser mais ne permettent pas de comprendre comment les choses fonctionnent) et le code tensorflow (qui contient beaucoup de détails mais nous détourne souvent vers des leçons sur tensorflow, alors que l'objectif ici est BERT !)\n","\n","Pour le moment, la bibliothèque Hugging Face semble être l'interface pytorch la plus largement acceptée et la plus puissante pour travailler avec BERT. En plus de supporter une variété de différents modèles de transformateurs pré-entraînés, la bibliothèque inclut également des modifications pré-construites de ces modèles adaptées à votre tâche spécifique. Par exemple, dans ce tutoriel, nous utiliserons `BertForSequenceClassification`.\n","\n","La bibliothèque comprend également des classes spécifiques pour la classification des jetons, la réponse aux questions, la prédiction de la phrase suivante, etc. L'utilisation de ces classes préconstruites simplifie le processus de modification de BERT pour vos besoins."]},{"cell_type":"code","metadata":{"id":"0NmMdkZO8R6q","outputId":"e94a9fd3-b7d4-4998-cbb2-7e3f73532eeb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699207364292,"user_tz":-60,"elapsed":7325,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}]},{"cell_type":"markdown","metadata":{"id":"guw6ZNtaswKc"},"source":["##Loading CoLA Dataset\n"]},{"cell_type":"markdown","metadata":{"id":"_9ZKxKc04Btk"},"source":["Nous utiliserons l'ensemble de données [The Corpus of Linguistic Acceptability (CoLA)] (https://nyu-mll.github.io/CoLA/) pour la classification des phrases simples. Il s'agit d'un ensemble de phrases étiquetées comme grammaticalement correctes ou incorrectes. Il a été publié pour la première fois en mai 2018, et est l'un des tests inclus dans le \"GLUE Benchmark\" sur lequel des modèles comme BERT sont en compétition."]},{"cell_type":"markdown","metadata":{"id":"3ZNVW6xd0T0X"},"source":["Nous utiliserons le paquet `wget` pour télécharger le jeu de données sur le système de fichiers de l'instance Colab."]},{"cell_type":"code","metadata":{"id":"5m6AnuFv0QXQ","outputId":"bae61fff-e63b-410c-87d5-ac4145ba4c90","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699207368963,"user_tz":-60,"elapsed":4673,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["!pip install wget"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"08pO03Ff1BjI"},"source":["L'ensemble des données est hébergé sur GitHub dans ce dossier : https://nyu-mll.github.io/CoLA/"]},{"cell_type":"code","metadata":{"id":"pMtmPMkBzrvs","outputId":"82d6046d-cf4b-437d-9d57-aa9d3207ac31","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699207368963,"user_tz":-60,"elapsed":26,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["import wget\n","import os\n","\n","print('Telechargement de lensemble des donnees...')\n","\n","# L'URL du fichier zip du jeu de données.\n","url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n","\n","# Télécharger le fichier (si ce n'est pas déjà fait)\n","if not os.path.exists('./cola_public_1.1.zip'):\n","    wget.download(url, './cola_public_1.1.zip')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading dataset...\n"]}]},{"cell_type":"markdown","metadata":{"id":"_mKctx-ll2FB"},"source":["Décompressez l'ensemble de données sur le système de fichiers. Vous pouvez parcourir le système de fichiers de l'instance de Colab dans la barre latérale de gauche."]},{"cell_type":"code","metadata":{"id":"0Yv-tNv20dnH","executionInfo":{"status":"ok","timestamp":1699207368963,"user_tz":-60,"elapsed":23,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# Unzip the dataset (if we haven't already)\n","if not os.path.exists('./cola_public/'):\n","    !unzip cola_public_1.1.zip"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oQUy9Tat2EF_"},"source":["## Parse"]},{"cell_type":"markdown","metadata":{"id":"xeyVCXT31EZQ"},"source":["Nous pouvons voir d'après les noms de fichiers que les versions `tokenized` et `raw` des données sont toutes deux disponibles.\n","\n","Nous ne pouvons pas utiliser la version pré-renseignée car, afin d'appliquer le BERT pré-entraîné, nous *devons* utiliser le tokenizer fourni par le modèle. Ceci est dû au fait que (1) le modèle a un vocabulaire spécifique et fixe et (2) le tokenizer de l'ORET a une façon particulière de traiter les mots hors-vocabulaire."]},{"cell_type":"markdown","metadata":{"id":"MYWzeGSY2xh3"},"source":["Nous allons utiliser pandas pour analyser l'ensemble d'apprentissage \"in-domain\" et examiner quelques-unes de ses propriétés et points de données."]},{"cell_type":"code","metadata":{"id":"_UkeC7SG2krJ","outputId":"4adfdd0f-40d1-4494-b478-9df3c82ede93","colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"status":"ok","timestamp":1699207368963,"user_tz":-60,"elapsed":23,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["import pandas as pd\n","\n","# Charger le jeu de données dans un dataframe pandas.\n","df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n","\n","# Indiquer le nombre de phrases.\n","print('Nombre de phrases de formation: {:,}\\n'.format(df.shape[0]))\n","\n","# Afficher 10 lignes aléatoires des données.\n","df.sample(10)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training sentences: 8,551\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["     sentence_source  label label_notes  \\\n","6604            g_81      1         NaN   \n","7723            ad03      0           *   \n","8035            ad03      0           *   \n","4682            ks08      1         NaN   \n","8205            ad03      0           *   \n","5044            ks08      0           *   \n","5501            b_73      1         NaN   \n","4388            ks08      0           *   \n","671             bc01      1         NaN   \n","6207            c_13      1         NaN   \n","\n","                                               sentence  \n","6604         John hummed, and Mary sang, the same tune.  \n","7723                            There arrived by Medea.  \n","8035                                 Can will he do it?  \n","4682  These are the books that we have gone most tho...  \n","8205                                 I destroyed there.  \n","5044  I believe that the problem is not easy to be o...  \n","5501  Such a scholar as you were speaking of just no...  \n","4388                     Margaret has had already left.  \n","671   Smith loaned, and his widow later donated, a v...  \n","6207  What did Jean think was likely to have been st...  "],"text/html":["\n","  <div id=\"df-38d8cdf8-2b4a-4643-952f-0fc5a7e00e25\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_source</th>\n","      <th>label</th>\n","      <th>label_notes</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6604</th>\n","      <td>g_81</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>John hummed, and Mary sang, the same tune.</td>\n","    </tr>\n","    <tr>\n","      <th>7723</th>\n","      <td>ad03</td>\n","      <td>0</td>\n","      <td>*</td>\n","      <td>There arrived by Medea.</td>\n","    </tr>\n","    <tr>\n","      <th>8035</th>\n","      <td>ad03</td>\n","      <td>0</td>\n","      <td>*</td>\n","      <td>Can will he do it?</td>\n","    </tr>\n","    <tr>\n","      <th>4682</th>\n","      <td>ks08</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>These are the books that we have gone most tho...</td>\n","    </tr>\n","    <tr>\n","      <th>8205</th>\n","      <td>ad03</td>\n","      <td>0</td>\n","      <td>*</td>\n","      <td>I destroyed there.</td>\n","    </tr>\n","    <tr>\n","      <th>5044</th>\n","      <td>ks08</td>\n","      <td>0</td>\n","      <td>*</td>\n","      <td>I believe that the problem is not easy to be o...</td>\n","    </tr>\n","    <tr>\n","      <th>5501</th>\n","      <td>b_73</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>Such a scholar as you were speaking of just no...</td>\n","    </tr>\n","    <tr>\n","      <th>4388</th>\n","      <td>ks08</td>\n","      <td>0</td>\n","      <td>*</td>\n","      <td>Margaret has had already left.</td>\n","    </tr>\n","    <tr>\n","      <th>671</th>\n","      <td>bc01</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>Smith loaned, and his widow later donated, a v...</td>\n","    </tr>\n","    <tr>\n","      <th>6207</th>\n","      <td>c_13</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>What did Jean think was likely to have been st...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38d8cdf8-2b4a-4643-952f-0fc5a7e00e25')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-38d8cdf8-2b4a-4643-952f-0fc5a7e00e25 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-38d8cdf8-2b4a-4643-952f-0fc5a7e00e25');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-49cf91e4-edc4-4701-95bf-ce2007248f13\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-49cf91e4-edc4-4701-95bf-ce2007248f13')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-49cf91e4-edc4-4701-95bf-ce2007248f13 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"kfWzpPi92UAH"},"source":["Les deux propriétés qui nous intéressent sont la `sentence` et son `étiquette`, que l'on appelle le \"jugement d'acceptabilité\" (0=inacceptable, 1=acceptable)."]},{"cell_type":"markdown","metadata":{"id":"H_LpQfzCn9_o"},"source":["Voici cinq phrases étiquetées comme n'étant pas grammaticalement acceptables. Notez à quel point cette tâche est plus difficile que l'analyse des sentiments !"]},{"cell_type":"code","metadata":{"id":"blqIvQaQncdJ","outputId":"0e89a3e7-2272-496b-a45a-8b05cf2e28e9","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1699207368964,"user_tz":-60,"elapsed":21,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["df.loc[df.label == 0].sample(5)[['sentence', 'label']]"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               sentence  label\n","5839                I've never seen him eats asparagus.      0\n","1770  Dean drank more booze than Frank ate Wheaties ...      0\n","3447  The professor found some strong evidences of w...      0\n","1287    I went to the store to have bought some whisky.      0\n","858     John is being discussed and Sally is being too.      0"],"text/html":["\n","  <div id=\"df-92bc6e9f-df43-421e-8e0d-b20b3fcd8c41\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5839</th>\n","      <td>I've never seen him eats asparagus.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1770</th>\n","      <td>Dean drank more booze than Frank ate Wheaties ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3447</th>\n","      <td>The professor found some strong evidences of w...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1287</th>\n","      <td>I went to the store to have bought some whisky.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>858</th>\n","      <td>John is being discussed and Sally is being too.</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92bc6e9f-df43-421e-8e0d-b20b3fcd8c41')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-92bc6e9f-df43-421e-8e0d-b20b3fcd8c41 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-92bc6e9f-df43-421e-8e0d-b20b3fcd8c41');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5d6540c9-aeec-4c70-9a8e-5ad5bf409bc1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5d6540c9-aeec-4c70-9a8e-5ad5bf409bc1')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5d6540c9-aeec-4c70-9a8e-5ad5bf409bc1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"4SMZ5T5Imhlx"},"source":["Extrayons les phrases et les étiquettes de notre ensemble d'apprentissage sous forme de tableaux numpy ndarrays."]},{"cell_type":"code","metadata":{"id":"GuE5BqICAne2","executionInfo":{"status":"ok","timestamp":1699207368964,"user_tz":-60,"elapsed":19,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# Get the lists of sentences and their labels.\n","sentences = df.sentence.values\n","labels = df.label.values"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ex5O1eV-Pfct"},"source":["## Tokenisation et formatage des entrées\n","\n","Dans cette section, nous allons transformer notre ensemble de données dans le format sur lequel BERT peut être formé."]},{"cell_type":"markdown","metadata":{"id":"-8kEDRvShcU5"},"source":["### BERT Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"bWOPOyWghJp2"},"source":["Pour transmettre notre texte à l'ORET, il doit être divisé en tokens, puis ces tokens doivent être mis en correspondance avec leur index dans le vocabulaire du tokéniseur.\n","\n","La tokenisation doit être effectuée par le tokenizer inclus dans BERT - la cellule ci-dessous le téléchargera pour nous. Nous utiliserons ici la version \"non casée\"."]},{"cell_type":"code","metadata":{"id":"Z474sSC6oe7A","outputId":"edfa95aa-0b53-49c5-88b7-2b65ec7801ec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699207372458,"user_tz":-60,"elapsed":3513,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading BERT tokenizer...\n"]}]},{"cell_type":"markdown","metadata":{"id":"dFzmtleW6KmJ"},"source":["Appliquons le tokenizer à une phrase pour voir le résultat."]},{"cell_type":"code","metadata":{"id":"dLIbudgfh6F0","outputId":"ef09cfaf-b7f3-4921-e705-af7bb6e6fa76","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699207372458,"user_tz":-60,"elapsed":3,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# Imprimer la phrase originale.\n","print(' Original: ', sentences[0])\n","\n","# Imprimer la phrase divisée en tokens.\n","print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n","\n","# Imprimer la phrase associée aux identifiants des jetons.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":[" Original:  Our friends won't buy this analysis, let alone the next one we propose.\n","Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n","Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"]}]},{"cell_type":"markdown","metadata":{"id":"WeNIc4auFUdF"},"source":["Lorsque nous convertirons toutes nos phrases, nous utiliserons la fonction `tokenize.encode` pour gérer les deux étapes, plutôt que d'appeler `tokenize` et `convert_tokens_to_ids` séparément.\n","\n","Mais avant cela, nous devons parler des exigences de formatage de BERT."]},{"cell_type":"markdown","metadata":{"id":"viKGCCh8izww"},"source":["### Formatage requis"]},{"cell_type":"markdown","metadata":{"id":"yDcqNlvVhL5W"},"source":["Le code ci-dessus a omis quelques étapes de formatage nécessaires que nous allons examiner ici.\n","\n","\n","Nous sommes tenus de :\n","1. Ajouter des jetons spéciaux au début et à la fin de chaque phrase.\n","2. Remplir et tronquer toutes les phrases à une longueur unique et constante.\n","3. Différencier explicitement les jetons réels des jetons de remplissage à l'aide du \"masque d'attention\"."]},{"cell_type":"markdown","metadata":{"id":"V6mceWWOjZnw"},"source":["### Jetons spéciaux"]},{"cell_type":"markdown","metadata":{"id":"Ykk0P9JiKtVe"},"source":["**[SEP]`**\n","\n","À la fin de chaque phrase, nous devons ajouter le jeton spécial `[SEP]`.\n","\n","Ce jeton est un artefact des tâches à deux phrases, où BERT reçoit deux phrases distinctes et doit déterminer quelque chose (par exemple, la réponse à la question de la phrase A peut-elle être trouvée dans la phrase B ?)"]},{"cell_type":"markdown","metadata":{"id":"86C9objaKu8f"},"source":["**`[CLS]`**\n","\n","Pour les tâches de classification, nous devons ajouter le jeton spécial `[CLS]` au début de chaque phrase.\n","\n","Ce jeton a une signification particulière. BERT se compose de 12 couches de transformateurs. Chaque transformateur reçoit une liste d'encastrements de jetons et produit le même nombre d'encastrements en sortie (mais avec des valeurs de caractéristiques modifiées, bien sûr !)\n","\n","![Illustration of CLS token purpose](http://www.mccormickml.com/assets/BERT/CLS_token_500x606.png)\n","\n","À la sortie du transformateur final (12e), *seule la première intégration (correspondant au jeton [CLS]) est utilisée par le classificateur*.\n","\n","> Le premier jeton de chaque séquence est toujours un jeton de classification spécial (`[CLS]`). L'état caché final\n","L'état caché final correspondant à ce jeton est utilisé comme représentation agrégée de la séquence pour les tâches de classification.\n","pour les tâches de classification.\"(from the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"u51v0kFxeteu"},"source":["### Longueur de la phrase et masque d'attention"]},{"cell_type":"markdown","metadata":{"id":"qPNuwqZVK3T6"},"source":["Les phrases de notre ensemble de données ont évidemment des longueurs variables, alors comment BERT gère-t-il cela ?\n","\n","L'ORET a deux contraintes :\n","1. Toutes les phrases doivent être complétées ou tronquées à une longueur unique et fixe.\n","2. La longueur maximale des phrases est de 512 tokens.\n","\n","Le remplissage est effectué avec un jeton spécial `[PAD]`, qui est à l'index 0 dans le vocabulaire de l'ORET. L'illustration ci-dessous montre un remplissage jusqu'à un \"MAX_LEN\" de 8 tokens.\n","\n","<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" width=\"600\">\n","\n","Le \"Attention Mask\" est simplement un tableau de 1 et de 0 indiquant quels tokens sont padding et lesquels ne le sont pas.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l6w8elb-58GJ"},"source":["### Sentences to IDs"]},{"cell_type":"markdown","metadata":{"id":"1M296yz577fV"},"source":["La fonction `tokenizer.encode` combine plusieurs étapes pour nous :\n","1. Diviser la phrase en tokens.\n","2. Ajouter les jetons spéciaux `[CLS]` et `[SEP]`.\n","3. Associer les tokens à leurs identifiants.\n","\n","Curieusement, cette fonction peut effectuer la troncature pour nous, mais ne gère pas le remplissage."]},{"cell_type":"code","metadata":{"id":"2bBdb3pt8LuQ","outputId":"cc0dfd92-8260-4447-f4cf-a277736c9c7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699207375959,"user_tz":-60,"elapsed":3503,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# Pour chaque phrase...\n","for sent in sentences:\n","    # `encode` va :\n","    # (1) Tokeniser la phrase.\n","    # (2) Ajouter le jeton `[CLS]` au début de la phrase.\n","    # (3) Ajouter le jeton `[SEP]` à la fin.\n","    # (4) Associer les jetons à leurs identifiants.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Phrase à encoder.\n","                        add_special_tokens = True, # Ajouter \"[CLS]\" et \"[SEP]\".\n","\n","                        # Cette fonction prend également en charge la troncature et la conversion\n","                        # en tenseurs pytorch, mais nous avons besoin de faire du padding, donc nous\n","                        # ne pouvons pas utiliser ces fonctionnalités :( .\n","                        #max_length = 128, # Tronquer toutes les phrases.\n","                        #return_tensors = 'pt', # Retourne les tenseurs de pytorch.\n","                   )\n","\n","    # Ajouter la phrase encodée à la liste.\n","    input_ids.append(encoded_sent)\n","\n","# Imprimer la phrase 0, maintenant sous forme de liste d'ID.\n","print('Original: ', sentences[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  Our friends won't buy this analysis, let alone the next one we propose.\n","Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"]}]},{"cell_type":"markdown","metadata":{"id":"WhwCKszh6ych"},"source":["## Remplissage et troncature"]},{"cell_type":"markdown","metadata":{"id":"xytsw1oIfnX0"},"source":["Remplir et tronquer nos séquences pour qu'elles aient toutes la même longueur, `MAX_LEN`."]},{"cell_type":"markdown","metadata":{"id":"zqiWTDrn_nGB"},"source":["Tout d'abord, quelle est la longueur maximale des phrases dans notre ensemble de données ?"]},{"cell_type":"code","metadata":{"id":"JhUZO9vc_l6T","outputId":"4e4fcd3a-0427-411d-ae28-a242faa8e14b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699207375960,"user_tz":-60,"elapsed":6,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["print('Durée maximale de la phrase: ', max([len(sen) for sen in input_ids]))"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Durée maximale de la phrase:  47\n"]}]},{"cell_type":"markdown","metadata":{"id":"hp-54FcQ_p3h"},"source":["**Compte tenu de cela, choisissons MAX_LEN = 64 et appliquons le padding."]},{"cell_type":"code","metadata":{"id":"Cp9BPRd1tMIo","outputId":"fc2f0d16-763e-4e86-be5d-737e2d834248","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699207379107,"user_tz":-60,"elapsed":3150,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# Nous allons emprunter la fonction utilitaire `pad_sequences` pour faire cela.\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# Fixer la longueur maximale de la séquence.\n","# J'ai choisi 64 un peu arbitrairement. C'est légèrement plus grand que la\n","# longueur maximale des phrases d'entraînement de 47...\n","MAX_LEN = 64\n","\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Remplir nos jetons d'entrée avec la valeur 0.\n","# \"post\" indique que nous voulons remplir et tronquer à la fin de la séquence,\n","# plutôt qu'au début.\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n","                          value=0, truncating=\"post\", padding=\"post\")\n","\n","print('\\nDone.')"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Padding/truncating all sentences to 64 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","\n","Done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"kDs-MYtYH8sL"},"source":["##  Masques d'attention"]},{"cell_type":"markdown","metadata":{"id":"KhGulL1pExCT"},"source":["Le masque d'attention rend simplement explicite les jetons qui sont des mots réels par rapport à ceux qui sont du remplissage.\n","\n","Le vocabulaire de l'ORET n'utilise pas l'ID 0, donc si l'ID d'un jeton est 0, il s'agit d'un remplissage, sinon c'est un vrai jeton."]},{"cell_type":"code","metadata":{"id":"cDoC24LeEv3N","executionInfo":{"status":"ok","timestamp":1699207379107,"user_tz":-60,"elapsed":3,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# Créer des masques d'attention\n","attention_masks = []\n","\n","# Pour chaque phrase...\n","for sent in input_ids:\n","\n","    # Créer le masque d'attention.\n","    # - Si l'ID d'un jeton est 0, alors il s'agit d'un padding, mettez le masque à 0.\n","    # - Si l'ID d'un jeton est > 0, il s'agit d'un vrai jeton, mettre le masque à 1.\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","\n","    # Stocker le masque d'attention pour cette phrase.\n","    attention_masks.append(att_mask)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aRp4O7D295d_"},"source":["## Formation & validation Split"]},{"cell_type":"markdown","metadata":{"id":"qu0ao7p8rb06"},"source":["Divisez notre ensemble de formation de manière à utiliser 90 % pour la formation et 10 % pour la validation."]},{"cell_type":"code","metadata":{"id":"aFbE-UHvsb7-","executionInfo":{"status":"ok","timestamp":1699207379854,"user_tz":-60,"elapsed":750,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# Utiliser train_test_split pour diviser nos données en ensembles de formation et de validation pour\n","# l'entraînement\n","from sklearn.model_selection import train_test_split\n","\n","# Utilisez 90 % pour la formation et 10 % pour la validation.\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n","                                                            random_state=2018, test_size=0.1)\n","# Faire de même pour les masques.\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n","                                             random_state=2018, test_size=0.1)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7LzSbTqW9_BR"},"source":["## Conversion vers les types de données PyTorch"]},{"cell_type":"markdown","metadata":{"id":"6p1uXczp-Je4"},"source":["Notre modèle attend des tenseurs PyTorch plutôt que des numpy.ndarrays, il faut donc convertir toutes les variables de notre jeu de données."]},{"cell_type":"code","metadata":{"id":"jw5K2A5Ko1RF","executionInfo":{"status":"ok","timestamp":1699207421882,"user_tz":-60,"elapsed":1185,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["import torch\n","# Convertir toutes les entrées et les étiquettes en tenseurs torch, le type de données requis\n","# pour notre modèle.\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dD9i6Z2pG-sN"},"source":["Nous allons également créer un itérateur pour notre ensemble de données en utilisant la classe torch DataLoader. Cela permet d'économiser de la mémoire pendant l'apprentissage car, contrairement à une boucle for, avec un itérateur, l'ensemble des données n'a pas besoin d'être chargé en mémoire."]},{"cell_type":"code","metadata":{"id":"GEgLpFVlo1Z-","executionInfo":{"status":"ok","timestamp":1699207423255,"user_tz":-60,"elapsed":2,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Le DataLoader a besoin de connaître la taille de notre lot pour l'entraînement, nous la spécifions donc\n","# ici.\n","# Pour affiner le BERT sur une tâche spécifique, les auteurs recommandent une taille de lot de\n","# 16 ou 32.\n","\n","batch_size = 32\n","\n","# Créer le DataLoader pour notre ensemble d'entraînement.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Créer le DataLoader pour notre jeu de validation.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8bwa6Rts-02-"},"source":["# Entraîner notre modèle de classification"]},{"cell_type":"markdown","metadata":{"id":"3xYQ3iLO08SX"},"source":["Maintenant que nos données d'entrée sont correctement formatées, il est temps d'affiner le modèle BERT."]},{"cell_type":"markdown","metadata":{"id":"D6TKgyUzPIQc"},"source":["##  BertForSequenceClassification"]},{"cell_type":"markdown","metadata":{"id":"1sjzRT1V0zwm"},"source":["Pour cette tâche, nous voulons d'abord modifier le modèle BERT pré-entraîné pour obtenir des résultats pour la classification, puis nous voulons continuer à entraîner le modèle sur notre ensemble de données jusqu'à ce que le modèle entier, de bout en bout, soit bien adapté à notre tâche.\n","\n","Heureusement, l'implémentation de huggingface pytorch inclut un ensemble d'interfaces conçues pour une variété de tâches NLP. Bien que ces interfaces soient toutes construites sur un modèle BERT entraîné, chacune d'entre elles possède des couches supérieures et des types de sortie différents, conçus pour s'adapter à leur tâche de TAL spécifique.  \n","\n","Voici la liste actuelle des classes fournies pour un réglage fin :\n","* BertModel\n","* BertForPreTraining\n","* BertForMaskedLM\n","* BertForNextSentencePrediction (prédiction de la phrase suivante)\n","* **BertForSequenceClassification** - Celle que nous utiliserons.\n","* BertForTokenClassification\n","* BertForQuestionAnswering (pour les réponses aux questions)\n","\n","La documentation relative à ces éléments est disponible à l'adresse suivante : [here](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html)."]},{"cell_type":"markdown","metadata":{"id":"BXYitPoE-cjH"},"source":["Nous utiliserons [BertForSequenceClassification] (https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification). Il s'agit du modèle BERT normal auquel a été ajoutée une couche linéaire unique pour la classification que nous utiliserons comme classificateur de phrases. Au fur et à mesure que nous fournissons des données d'entrée, l'ensemble du modèle BERT pré-entraîné et la couche de classification supplémentaire non entraînée sont entraînés sur notre tâche spécifique."]},{"cell_type":"code","metadata":{"id":"gFsCTp_mporB","outputId":"056e11f6-3a7d-4153-e4e7-8bf140a010ed","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699207430821,"user_tz":-60,"elapsed":5123,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Charger BertForSequenceClassification, le modèle BERT pré-entraîné avec une seule couche de\n","# couche de classification linéaire.\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Utiliser le modèle BERT à 12 couches, avec un vocabulaire sans majuscules.\n","    num_labels = 2, # Le nombre d'étiquettes de sortie - 2 pour la classification binaire.\n","                    # Vous pouvez augmenter cette valeur pour les tâches multi-classes.\n","    output_attentions = False, # Si le modèle renvoie des poids d'attention.\n","    output_hidden_states = False, # Si le modèle renvoie tous les états cachés.\n",")\n","\n","# Indiquer à pytorch d'exécuter ce modèle sur le GPU.\n","model.cuda()"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"e0Jv6c7-HHDW"},"source":["Par curiosité, nous pouvons parcourir ici tous les paramètres du modèle par leur nom.\n","\n","Dans la cellule ci-dessous, j'ai imprimé les noms et les dimensions des poids pour :\n","\n","1. La couche d'intégration.\n","2. Le premier des douze transformateurs.\n","3. La couche de sortie."]},{"cell_type":"code","metadata":{"id":"8PIiVlDYCtSq","outputId":"7facb457-32c4-488f-fdb3-39cf55d378b6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699207430821,"user_tz":-60,"elapsed":4,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# Obtenir tous les paramètres du modèle sous la forme d'une liste de tuples.\n","params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["The BERT model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                  (30522, 768)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (2, 768)\n","classifier.bias                                                 (2,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"qRWT-D4U_Pvx"},"source":["## Optimiseur et planificateur de taux d'apprentissage"]},{"cell_type":"markdown","metadata":{"id":"8o-VEBobKwHk"},"source":["Maintenant que notre modèle est chargé, nous devons saisir les hyperparamètres d'entraînement à partir du modèle stocké.\n","\n","Pour les besoins d'un réglage fin, les auteurs recommandent de choisir parmi les valeurs suivantes :\n","- Taille du lot : 16, 32 (nous avons choisi 32 lors de la création de nos DataLoaders).\n","- Taux d'apprentissage (Adam) : 5e-5, 3e-5, 2e-5 (nous utiliserons 2e-5).\n","- Nombre d'époques : 2, 3, 4 (nous utiliserons 4).\n","\n"]},{"cell_type":"code","metadata":{"id":"GLs72DuMODJO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699207431709,"user_tz":-60,"elapsed":891,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"174e8c18-96fb-4450-c6fe-1d4f88b4afe3"},"source":["# Note : AdamW est une classe de la bibliothèque huggingface (par opposition à pytorch)\n","# Je crois que le 'W' signifie 'Weight Decay fix' (fixation de la décroissance du poids)\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - la valeur par défaut est 5e-5, notre notebook avait 2e-5\n","                  eps = 1e-8 # args.adam_epsilon - la valeur par défaut est 1e-8.\n","                )\n"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","metadata":{"id":"-p0upAhhRiIx","executionInfo":{"status":"ok","timestamp":1699207432709,"user_tz":-60,"elapsed":2,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["from transformers import get_linear_schedule_with_warmup\n","# Nombre d'époques d'apprentissage (les auteurs recommandent entre 2 et 4)\n","epochs = 10\n","\n","# Le nombre total d'étapes d'apprentissage est égal au nombre de lots * nombre d'époques.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Créer le planificateur de taux d'apprentissage.\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RqfmWwUR_Sox"},"source":["## Boucle de formation"]},{"cell_type":"markdown","metadata":{"id":"_QXZhFb4LnV5"},"source":["Voici notre boucle d'entraînement. Il se passe beaucoup de choses, mais fondamentalement, pour chaque passage de notre boucle, nous avons une phase de formation et une phase de validation. À chaque passage, nous devons :\n","\n","Boucle d'apprentissage :\n","- Décompresser nos données d'entrée et nos étiquettes\n","- Charger les données sur le GPU pour l'accélération\n","- Effacer les gradients calculés lors de la passe précédente.\n","    - Dans pytorch, les gradients s'accumulent par défaut (utile pour les RNN) à moins que vous ne les effaciez explicitement.\n","- Passe avant (introduire les données d'entrée dans le réseau)\n","- Passe arrière (rétropropagation)\n","- Demander au réseau de mettre à jour les paramètres avec optimizer.step()\n","- Suivi des variables pour contrôler la progression\n","\n","Boucle d'évaluation :\n","- Décompressez nos données d'entrée et nos étiquettes\n","- Chargement des données sur le GPU pour l'accélération\n","- Passage en avant (alimentation des données d'entrée à travers le réseau)\n","- Calcul de la perte sur nos données de validation et suivi des variables pour contrôler les progrès."]},{"cell_type":"markdown","metadata":{"id":"pE5B99H5H2-W"},"source":["Définir une fonction d'aide pour le calcul de la précision."]},{"cell_type":"code","metadata":{"id":"9cQNvaZ9bnyy","executionInfo":{"status":"ok","timestamp":1699207435508,"user_tz":-60,"elapsed":473,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["import numpy as np\n","\n","# Fonction permettant de calculer la précision de nos prédictions par rapport aux étiquettes\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KNhRtWPXH9C3"},"source":["Fonction d'aide pour le formatage des temps écoulés."]},{"cell_type":"code","metadata":{"id":"gpt6tR83keZD","executionInfo":{"status":"ok","timestamp":1699207437058,"user_tz":-60,"elapsed":2,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Prend un temps en secondes et retourne une chaîne hh:mm:ss\n","    '''\n","    # Arrondi à la seconde la plus proche.\n","    elapsed_rounded = int(round((elapsed)))\n","\n","    # Format hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cfNIhN19te3N"},"source":["Nous sommes prêts à donner le coup d'envoi de la formation !"]},{"cell_type":"code","metadata":{"id":"6J-FYdx6nFE_","outputId":"ad23a1e9-ba6b-43ce-e3f8-07e9ef04229d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699208304630,"user_tz":-60,"elapsed":798919,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["import random\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# Fixer la valeur de la graine un peu partout pour que ce soit reproductible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","device = torch.device(\"cuda\")\n","\n","# Stocke la perte moyenne après chaque époque afin de pouvoir la représenter graphiquement.\n","loss_values = []\n","\n","# Pour chaque époque...\n","for epoch_i in range(0, epochs):\n","\n","    # ========================================\n","    # Formation\n","    # ========================================\n","\n","    # Effectuer un passage complet sur l'ensemble d'apprentissage.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Mesure la durée de l'époque d'apprentissage.\n","    t0 = time.time()\n","\n","    # Réinitialiser la perte totale pour cette époque.\n","    total_loss = 0\n","\n","    # Mettre le modèle en mode d'entraînement. Ne vous laissez pas induire en erreur - l'appel à\n","    # `train` ne fait que changer le *mode*, il ne *permet pas* l'apprentissage.\n","    # Les couches `dropout` et `batchnorm` se comportent différemment pendant l'entraînement\n","    # vs. test (source : https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # Pour chaque lot de données d'apprentissage...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Mise à jour de l'état d'avancement tous les 40 lots.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculer le temps écoulé en minutes.\n","            elapsed = format_time(time.time() - t0)\n","\n","            # Rapport sur l'état d'avancement.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Décompressez ce lot de formation à partir de notre dataloader.\n","        #\n","        # En décompressant le lot, nous copierons également chaque tenseur sur le GPU en utilisant la méthode\n","        # méthode `to`.\n","        #\n","        # `batch` contient trois tenseurs pytorch :\n","        # [0] : ids d'entrée\n","        # [1] : masques d'attention\n","        # [2] : étiquettes\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Il faut toujours effacer les gradients calculés précédemment avant d'effectuer une\n","        # avant d'effectuer une passe arrière. PyTorch ne le fait pas automatiquement parce que\n","        # l'accumulation des gradients est \"pratique lors de l'entraînement des RNN\".\n","        # (source : https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()\n","\n","        # Effectuer une passe avant (évaluer le modèle sur ce lot d'entraînement).\n","        # Ceci retournera la perte (plutôt que la sortie du modèle) parce que nous\n","        # avons fourni les `étiquettes`.\n","        # La documentation pour cette fonction `model` est ici :\n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids,\n","                    token_type_ids=None,\n","                    attention_mask=b_input_mask,\n","                    labels=b_labels)\n","\n","        # L'appel à `model` renvoie toujours un tuple, donc nous devons extraire la\n","        # valeur de la perte du tuple.\n","        loss = outputs[0]\n","\n","        # Accumuler la perte d'entraînement sur tous les lots afin de pouvoir\n","        # calculer la perte moyenne à la fin. `loss` est un tenseur contenant une seule valeur.\n","        # une seule valeur ; la fonction `.item()` renvoie simplement la valeur Python\n","        # du tenseur.\n","        total_loss += loss.item()\n","\n","        # Effectuer une passe arrière pour calculer les gradients.\n","        loss.backward()\n","\n","        # Réduire la norme des gradients à 1,0.\n","        # Cela permet d'éviter le problème des \"gradients qui explosent\".\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Mettre à jour les paramètres et faire un pas en utilisant le gradient calculé.\n","        # L'optimiseur dicte la \"règle de mise à jour\", c'est-à-dire la manière dont les paramètres sont modifiés en fonction de leurs gradients, du taux d'apprentissage, etc.\n","        # modifiés en fonction de leurs gradients, du taux d'apprentissage, etc.\n","        optimizer.step()\n","\n","        # Mettre à jour le taux d'apprentissage.\n","        scheduler.step()\n","\n","    # Calculer la perte moyenne sur les données d'apprentissage.\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Stocker la valeur de la perte pour tracer la courbe d'apprentissage.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Perte moyenne de formation: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Temps de formation epcoh: {:}\".format(format_time(time.time() - t0)))\n","\n","    # ========================================\n","    # Validation\n","    # ========================================\n","    # Après l'achèvement de chaque période d'apprentissage, nous mesurons notre performance sur\n","    # notre ensemble de validation.\n","\n","    print(\"\")\n","    print(\"Validation en cours d'exécution...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode - exclusion layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    #Suivi des variables\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Évaluer les données pour une époque\n","    for batch in validation_dataloader:\n","\n","        # Ajouter le lot au GPU\n","        batch = tuple(t.to(device) for t in batch)\n","\n","        # Décompresse les entrées de notre dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Indiquer au modèle de ne pas calculer ou stocker les gradients, ce qui permet d'économiser de la mémoire et d'accélérer la validation.\n","        # accélérer la validation\n","        with torch.no_grad():\n","\n","            # Passe en avant, calcule les prédictions logit.\n","            # Ceci renverra les logits plutôt que les pertes car nous n'avons pas\n","            # n'avons pas fourni d'étiquettes.\n","            # token_type_ids est le même que le \"segment ids\", qui # différencie la phrase 1 et 2 dans les tâches à 2 phrases.\n","            # différencie les phrases 1 et 2 dans les tâches à 2 phrases.\n","            # La documentation pour cette fonction `modèle` est ici :\n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids,\n","                            token_type_ids=None,\n","                            attention_mask=b_input_mask)\n","\n","        # Obtenir les \"logits\" produits par le modèle. Les \"logits\" sont les valeurs de sortie\n","        # avant l'application d'une fonction d'activation comme la softmax.\n","        logits = outputs[0]\n","\n","        # Déplacer les logits et les étiquettes vers l'unité centrale\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculer la précision pour ce lot de phrases de test.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","\n","        # Accumuler la précision totale.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Suivre le nombre de lots\n","        nb_eval_steps += 1\n","\n","    # Indique la précision finale pour ce cycle de validation.\n","    print(\"  Précision: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Formation terminée!\")"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 10 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:13.\n","  Batch    80  of    241.    Elapsed: 0:00:25.\n","  Batch   120  of    241.    Elapsed: 0:00:37.\n","  Batch   160  of    241.    Elapsed: 0:00:49.\n","  Batch   200  of    241.    Elapsed: 0:01:02.\n","  Batch   240  of    241.    Elapsed: 0:01:15.\n","\n","  Perte moyenne de formation: 0.49\n","  Temps de formation epcoh: 0:01:15\n","\n","Validation en cours d'exécution...\n","  Précision: 0.81\n","  Validation took: 0:00:03\n","\n","======== Epoch 2 / 10 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:13.\n","  Batch    80  of    241.    Elapsed: 0:00:25.\n","  Batch   120  of    241.    Elapsed: 0:00:38.\n","  Batch   160  of    241.    Elapsed: 0:00:51.\n","  Batch   200  of    241.    Elapsed: 0:01:04.\n","  Batch   240  of    241.    Elapsed: 0:01:17.\n","\n","  Perte moyenne de formation: 0.30\n","  Temps de formation epcoh: 0:01:17\n","\n","Validation en cours d'exécution...\n","  Précision: 0.81\n","  Validation took: 0:00:03\n","\n","======== Epoch 3 / 10 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:13.\n","  Batch    80  of    241.    Elapsed: 0:00:26.\n","  Batch   120  of    241.    Elapsed: 0:00:39.\n","  Batch   160  of    241.    Elapsed: 0:00:51.\n","  Batch   200  of    241.    Elapsed: 0:01:04.\n","  Batch   240  of    241.    Elapsed: 0:01:17.\n","\n","  Perte moyenne de formation: 0.20\n","  Temps de formation epcoh: 0:01:17\n","\n","Validation en cours d'exécution...\n","  Précision: 0.82\n","  Validation took: 0:00:03\n","\n","======== Epoch 4 / 10 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:13.\n","  Batch    80  of    241.    Elapsed: 0:00:26.\n","  Batch   120  of    241.    Elapsed: 0:00:39.\n","  Batch   160  of    241.    Elapsed: 0:00:51.\n","  Batch   200  of    241.    Elapsed: 0:01:04.\n","  Batch   240  of    241.    Elapsed: 0:01:17.\n","\n","  Perte moyenne de formation: 0.13\n","  Temps de formation epcoh: 0:01:17\n","\n","Validation en cours d'exécution...\n","  Précision: 0.82\n","  Validation took: 0:00:03\n","\n","======== Epoch 5 / 10 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:13.\n","  Batch    80  of    241.    Elapsed: 0:00:26.\n","  Batch   120  of    241.    Elapsed: 0:00:39.\n","  Batch   160  of    241.    Elapsed: 0:00:51.\n","  Batch   200  of    241.    Elapsed: 0:01:04.\n","  Batch   240  of    241.    Elapsed: 0:01:17.\n","\n","  Perte moyenne de formation: 0.10\n","  Temps de formation epcoh: 0:01:17\n","\n","Validation en cours d'exécution...\n","  Précision: 0.82\n","  Validation took: 0:00:03\n","\n","======== Epoch 6 / 10 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:13.\n","  Batch    80  of    241.    Elapsed: 0:00:26.\n","  Batch   120  of    241.    Elapsed: 0:00:38.\n","  Batch   160  of    241.    Elapsed: 0:00:51.\n","  Batch   200  of    241.    Elapsed: 0:01:04.\n","  Batch   240  of    241.    Elapsed: 0:01:17.\n","\n","  Perte moyenne de formation: 0.07\n","  Temps de formation epcoh: 0:01:17\n","\n","Validation en cours d'exécution...\n","  Précision: 0.81\n","  Validation took: 0:00:03\n","\n","======== Epoch 7 / 10 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:13.\n","  Batch    80  of    241.    Elapsed: 0:00:26.\n","  Batch   120  of    241.    Elapsed: 0:00:38.\n","  Batch   160  of    241.    Elapsed: 0:00:51.\n","  Batch   200  of    241.    Elapsed: 0:01:04.\n","  Batch   240  of    241.    Elapsed: 0:01:17.\n","\n","  Perte moyenne de formation: 0.05\n","  Temps de formation epcoh: 0:01:17\n","\n","Validation en cours d'exécution...\n","  Précision: 0.83\n","  Validation took: 0:00:03\n","\n","======== Epoch 8 / 10 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:13.\n","  Batch    80  of    241.    Elapsed: 0:00:26.\n","  Batch   120  of    241.    Elapsed: 0:00:38.\n","  Batch   160  of    241.    Elapsed: 0:00:51.\n","  Batch   200  of    241.    Elapsed: 0:01:04.\n","  Batch   240  of    241.    Elapsed: 0:01:17.\n","\n","  Perte moyenne de formation: 0.04\n","  Temps de formation epcoh: 0:01:17\n","\n","Validation en cours d'exécution...\n","  Précision: 0.82\n","  Validation took: 0:00:03\n","\n","======== Epoch 9 / 10 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:13.\n","  Batch    80  of    241.    Elapsed: 0:00:26.\n","  Batch   120  of    241.    Elapsed: 0:00:39.\n","  Batch   160  of    241.    Elapsed: 0:00:51.\n","  Batch   200  of    241.    Elapsed: 0:01:04.\n","  Batch   240  of    241.    Elapsed: 0:01:17.\n","\n","  Perte moyenne de formation: 0.03\n","  Temps de formation epcoh: 0:01:17\n","\n","Validation en cours d'exécution...\n","  Précision: 0.82\n","  Validation took: 0:00:03\n","\n","======== Epoch 10 / 10 ========\n","Training...\n","  Batch    40  of    241.    Elapsed: 0:00:13.\n","  Batch    80  of    241.    Elapsed: 0:00:26.\n","  Batch   120  of    241.    Elapsed: 0:00:38.\n","  Batch   160  of    241.    Elapsed: 0:00:51.\n","  Batch   200  of    241.    Elapsed: 0:01:04.\n","  Batch   240  of    241.    Elapsed: 0:01:17.\n","\n","  Perte moyenne de formation: 0.02\n","  Temps de formation epcoh: 0:01:17\n","\n","Validation en cours d'exécution...\n","  Précision: 0.82\n","  Validation took: 0:00:03\n","\n","Formation terminée!\n"]}]}]}