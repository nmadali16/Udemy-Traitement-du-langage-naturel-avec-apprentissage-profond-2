{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RcdcqBkV0MTU"},"source":["# Le modèle T5\n","\n"," le modèle T5 (Text-To-Text Transfer Transformer) est un type d'architecture de réseau neuronal basé sur des transformers développé par Google AI. Il a été introduit dans un article de recherche intitulé \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" en 2019.\n","\n","\n"," <img src=\"https://drive.google.com/uc?export=view&id=1-1tPmR0H0TWNeVciSFgeb1F1V2tZIrZ2\">\n","\n","L'idée clé derrière T5 est de formuler diverses tâches de traitement du langage naturel (NLP) sous forme de problèmes texte-vers-texte. Contrairement aux modèles précédents conçus pour des tâches spécifiques (comme la traduction de langues, la résumé de texte, etc.), T5 a été créé avec un cadre unifié où toutes les tâches NLP sont présentées sous un format \"texte-vers-texte\".\n","\n","Au lieu d'avoir différentes architectures pour des tâches telles que la traduction, la résumé, la réponse à des questions, etc., T5 aborde ces tâches de manière similaire : en convertissant du texte d'entrée en texte de sortie. Cela simplifie la conception du modèle et lui permet de gérer une large gamme de tâches en affinant les données spécifiques à chaque tâche.\n","\n","T5 fonctionne à l'aide de l'architecture transformer, qui inclut des mécanismes d'auto-attention pour capturer les relations entre différents mots dans une phrase ou une séquence de texte. Il se compose d'une structure encodeur-décodeur où le texte d'entrée est transformé en une représentation vectorielle continue de longueur fixe, puis décodé dans le format de sortie souhaité.\n","\n","Un des avantages de T5 est sa capacité à atteindre des performances de pointe sur plusieurs benchmarks de NLP en affinant le modèle sur des ensembles de données spécifiques à différentes tâches. Son approche texte-vers-texte offre un cadre flexible et généralisable pour diverses tâches de compréhension du langage.\n","\n","T5 a été influent en démontrant l'efficacité du transfert d'apprentissage dans le traitement du langage naturel et a inspiré de nouvelles recherches pour créer des modèles plus polyvalents et efficaces pour la compréhension et la génération de langage humain."]},{"cell_type":"markdown","source":["## Variantes du modèle T5\n","Les  différentes variantes du modèle T5 sont:\n","\n","1. **T5-Small (Petit T5)** : La version T5-Small est une version plus légère du modèle T5, avec moins de paramètres et donc une capacité de traitement réduite par rapport aux versions plus grandes. Elle est adaptée pour des tâches de traitement du langage moins complexes ou pour des ressources informatiques limitées.\n","\n","2. **T5-Base (T5 de base)** : T5-Base est une version intermédiaire du modèle T5, offrant un équilibre entre la taille et les performances. Elle est utilisée pour des tâches de traitement du langage naturel variées et est plus puissante que T5-Small, mais moins que les versions plus grandes.\n","\n","3. **T5-Large (Grand T5)** : Le modèle T5-Large est une version plus grande et plus puissante de T5, avec un plus grand nombre de paramètres. Il est capable de traiter des tâches de traitement du langage plus complexes et de capturer des nuances plus fines dans les données textuelles.\n","\n","4. **T5-3B** : T5-3B est une version encore plus grande que T5-Large, avec un nombre encore plus élevé de paramètres. Elle est conçue pour gérer des tâches de traitement du langage naturel très exigeantes nécessitant une énorme capacité de calcul.\n","\n","5. **T5-11B** : T5-11B est la version la plus grande et la plus puissante de T5, avec un nombre massif de paramètres. Elle est destinée à des travaux de recherche avancés ou à des cas d'utilisation nécessitant une énorme capacité de traitement.\n","\n","Ces différentes variantes du modèle T5 sont conçues pour répondre à différents besoins en termes de capacité de traitement, de taille du modèle et de complexité des tâches de traitement du langage naturel. Elles offrent des options adaptées à divers scénarios d'utilisation et de ressources disponibles."],"metadata":{"id":"5e82pystYmYI"}},{"cell_type":"code","metadata":{"id":"06QFZGxsf_KJ","executionInfo":{"status":"ok","timestamp":1700915756900,"user_tz":-60,"elapsed":4,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["#!pip install transformers"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"3tYFf-cEIkKL","executionInfo":{"status":"ok","timestamp":1700915756900,"user_tz":-60,"elapsed":4,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["#!pip install sentencepiece"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8suV48O07TW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0409cac-c66f-4516-be38-686100386fcd","executionInfo":{"status":"ok","timestamp":1700915784868,"user_tz":-60,"elapsed":27971,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["import torch\n","import json\n","from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n","\n","model = T5ForConditionalGeneration.from_pretrained('t5-large')\n","tokenizer = T5Tokenizer.from_pretrained('t5-large')\n","device = torch.device('cpu')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"markdown","source":["# Traduction anglais vers français"],"metadata":{"id":"2l021rNfZd0G"}},{"cell_type":"markdown","source":["La traduction linguistique est la tâche PNL dans laquelle un modèle, à partir d'un texte dans une langue, produit une version traduite du même texte dans une autre langue.\n","\n","Le modèle T5 a été formé sur l'ensemble de données C4, qui contient les langues suivantes : anglais, allemand, français et roumain.\n","\n","Grâce à T5, nous pouvons traduire entre ces langues.\n","Ci-dessous, nous allons traduire de l’anglais vers le français. Pour la traduction, nous devons ajouter le préfixe **translate English to French:**  à la séquence d'entrée.\n"],"metadata":{"id":"8qktqiIedvl3"}},{"cell_type":"code","source":["# Example text for translation\n","english_text = \"Hello, how are you?\"\n","\n","input_text = \"translate English to French: \" + english_text\n","input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n","\n","# Generate translation\n","outputs = model.generate(input_ids=input_ids, max_length=200, num_return_sequences=1, early_stopping=True)\n","\n","# Decode the generated output\n","translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XlVdCozNaZiz","executionInfo":{"status":"ok","timestamp":1700915788543,"user_tz":-60,"elapsed":3680,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"088b2b14-af9f-44f0-bc13-c459b8cbad61"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["print(\"English:\", english_text)\n","print(\"French:\", translated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QJuatsTNak8i","executionInfo":{"status":"ok","timestamp":1700915788543,"user_tz":-60,"elapsed":23,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"7ce4246b-1ea5-462d-f4b3-81edf2a41761"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["English: Hello, how are you?\n","French: Bonjour, comment êtes-vous?\n"]}]},{"cell_type":"markdown","source":["#Résumé du texte"],"metadata":{"id":"cZSSAGYEa3U3"}},{"cell_type":"markdown","source":["\n","La synthèse de texte est la tâche PNL dans laquelle un modèle, à partir d'une longue séquence de texte, produit une version résumée de l'entrée.\n","\n","\n","Pour résumer, nous devons ajouter le préfixe **summarize : ** à la séquence d'entrée."],"metadata":{"id":"6me8M3Skdgkj"}},{"cell_type":"code","source":["text=\"\"\"\n","The United States Declaration of Independence was the first Etext\n","released by Project Gutenberg, early in 1971.  The title was stored\n","in an emailed instruction set which required a tape or diskpack be\n","hand mounted for retrieval.  The diskpack was the size of a large\n","cake in a cake carrier, cost $1500, and contained 5 megabytes, of\n","which this file took 1-2%.  Two tape backups were kept plus one on\n","paper tape.  The 10,000 files we hope to have online by the end of\n","2001 should take about 1-2% of a comparably priced drive in 2001.\n","\"\"\""],"metadata":{"id":"cyQ8ax6abrU3","executionInfo":{"status":"ok","timestamp":1700915788544,"user_tz":-60,"elapsed":21,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5KfhCrifP01","executionInfo":{"status":"ok","timestamp":1700915851764,"user_tz":-60,"elapsed":38534,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["preprocess_text = text.strip().replace(\"\\n\",\"\")\n","t5_prepared_Text = \"summarize: \"+preprocess_text\n","\n","tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n","\n","summary_ids = model.generate(tokenized_text,\n","                                      num_beams=4,\n","                                      no_repeat_ngram_size=2,\n","                                      min_length=30,\n","                                      max_length=50,\n","                                      early_stopping=True)\n","\n","output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"vqiTNoDc7pOv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c802dc69-09ac-4091-e6a5-3172c4f4c8a6","executionInfo":{"status":"ok","timestamp":1700915895048,"user_tz":-60,"elapsed":6,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["\n","print (\"\\n\\nSummarized text: \\n\",output)\n"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Summarized text: \n"," the united states declaration of independence was the first etext published by project gutenberg, early in 1971. the 10,000 files we hope to have online by the end of2001 should take about 1-2% of a comparably priced drive in 2001.\n"]}]},{"cell_type":"markdown","source":["# Implication textuelle"],"metadata":{"id":"3IbcPBXTcDmF"}},{"cell_type":"markdown","source":["L'implication textuelle est une tâche de PNL dans laquelle un modèle reçoit deux phrases, l'une étant la prémisse et l'autre l'hypothèse. Sur la base de ces deux phrases, le résultat est classé en trois classes : implication, contradiction et neutre.\n","\n","Pour l'implication textuelle, nous devons ajouter « mnli prémisse : » et « hypothèse : » aux paires de phrases.\n"],"metadata":{"id":"aHAJRpi6cCOB"}},{"cell_type":"code","source":["entailment_premise = (\"I love One Piece.\")\n","entailment_hypothesis = (\"My feelings towards One Piece is filled with love\")\n","input_ids = tokenizer(\"mnli premise: \"+entailment_premise+\" hypothesis: \"+entailment_hypothesis, return_tensors=\"pt\").input_ids\n","entailment_ids = model.generate(input_ids)\n","entailment = tokenizer.decode(entailment_ids[0],skip_special_tokens=True)\n","print(entailment)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Azw_0tDQcHFN","executionInfo":{"status":"ok","timestamp":1700915903028,"user_tz":-60,"elapsed":2115,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"6f18b584-05be-4066-ab05-18ba64a451c5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["entailment\n"]}]},{"cell_type":"markdown","source":["# Acceptabilité linguistique\n","\n","L'acceptabilité linguistique est une tâche de PNL dans laquelle un modèle, à l'aide d'une invite de texte, vérifie si la phrase est grammaticalement correcte.\n","\n","Pour l'acceptabilité linguistique, nous devons ajouter « phrase cola : » à la phrase. COLA est l'ensemble de données qui contient des phrases mappées selon leur acceptabilité.\n","\n","\""],"metadata":{"id":"DEUBnaFfdA3z"}},{"cell_type":"code","source":["sentence = (\"Luffy is a great pirate.\")\n","input_ids = tokenizer(\"cola sentence: \"+ sentence, return_tensors=\"pt\").input_ids\n","sentence_ids = model.generate(input_ids)\n","sentence = tokenizer.decode(sentence_ids[0],skip_special_tokens=True)\n","print(sentence)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iji4Aygoccgq","executionInfo":{"status":"ok","timestamp":1700916018781,"user_tz":-60,"elapsed":1226,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"a4af3723-3c0b-4ca1-9f43-cf4f972c2edd"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["acceptable\n"]}]},{"cell_type":"markdown","source":["# Similitude des phrases\n","\n","La similarité de phrases est une tâche de PNL dans laquelle un modèle, étant donné deux paires de phrases, évaluerait leur similarité sur une échelle de 1 à 5. La sortie est considérée comme une valeur de chaîne et est incrémentée de 0,2. Cela signifie que nous pouvons considérer cela comme une tâche de classification de texte avec 21 classes : 1.0, 1.2,… 5.0.\n","\n","Pour la similarité des phrases, nous devons ajouter « phrase stsb 1 : » et « phrase 2 : » aux paires de phrases."],"metadata":{"id":"qF2P1dw2dFVB"}},{"cell_type":"code","source":["stsb_sentence_1 = (\"Luffy was fighting in the war.\")\n","stsb_sentence_2 = (\"Luffy's fighting style is comical.\")\n","input_ids = tokenizer(\"stsb sentence 1: \"+stsb_sentence_1+\" sentence 2: \"+stsb_sentence_2, return_tensors=\"pt\").input_ids\n","stsb_ids = model.generate(input_ids)\n","stsb = tokenizer.decode(stsb_ids[0],skip_special_tokens=True)\n","print(stsb)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gyZI1ylvcgpg","executionInfo":{"status":"ok","timestamp":1700916021220,"user_tz":-60,"elapsed":1456,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"8bc70dbb-c799-477c-f9a6-a4115925b0a6"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["2.4\n"]}]}]}