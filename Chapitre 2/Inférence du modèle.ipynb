{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb229e26",
   "metadata": {
    "id": "bb229e26"
   },
   "source": [
    "Une fois l'entraînement terminé, le processus d'inférence peut commencer. Il débute en utilisant le jeton spécial \"SOS\" (Start of Sentence) comme premier jeton d'entrée dans le décodeur.\n",
    "\n",
    "Voici comment le processus d'inférence se déroule étape par étape :\n",
    "\n",
    "1. **Encodeur** :\n",
    "   - La phrase source est d'abord passée à travers l'encodeur pour obtenir une représentation de la séquence source.\n",
    "\n",
    "2. **Initialisation du décodeur** :\n",
    "   - Le décodeur est initialisé avec le jeton SOS. À partir de là, il génère des prédictions de mots un par un.\n",
    "\n",
    "3. **Prédiction des mots** :\n",
    "   - Le décodeur prend la représentation de l'encodeur et le dernier mot prédit comme entrée pour générer le prochain mot dans la séquence cible. Il utilise l'attention sur la sortie de l'encodeur et l'attention sur ses propres sorties précédentes pour cela.\n",
    "\n",
    "4. **Échantillonnage de mots** :\n",
    "   - Lors de l'inférence, plutôt que de choisir simplement le mot avec la probabilité la plus élevée, il est souvent préférable d'échantillonner le mot en fonction des probabilités prédites. Cela peut aider à introduire de la variabilité dans les générations.\n",
    "\n",
    "5. **Poursuite de la génération** :\n",
    "   - Le processus de prédiction et d'échantillonnage est répété jusqu'à ce qu'un jeton de fin de phrase (EOS) soit généré ou jusqu'à ce qu'une longueur maximale soit atteinte.\n",
    "\n",
    "6. **Décodage** :\n",
    "   - Les jetons prédits sont décodés en mots ou en phrases de la langue cible.\n",
    "\n",
    "7. **Fin de la séquence** :\n",
    "   - Une fois qu'un jeton EOS est généré, la génération de la séquence cible est terminée.\n",
    "\n",
    "8. **Post-traitement** :\n",
    "   - Le texte généré peut nécessiter un certain post-traitement, comme la suppression des jetons de padding ou d'autres ajustements en fonction des besoins spécifiques de l'application.\n",
    "\n",
    "Le résultat final est la séquence cible générée à partir de la phrase source donnée en entrée. Ce processus est répété pour chaque nouvelle phrase que vous souhaitez traduire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a03071e1",
   "metadata": {
    "id": "a03071e1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchtext\n",
    "from models import Transformer, create_masks, create_trg_mask\n",
    "import re\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db026b85",
   "metadata": {
    "id": "db026b85"
   },
   "outputs": [],
   "source": [
    "class tokener(object):\n",
    "    def __init__(self, lang):\n",
    "        d = {\"en\":\"en_core_web_sm\", \"fr\":\"fr_core_news_sm\"}\n",
    "        self.ob = spacy.load(d[lang])\n",
    "\n",
    "    def tokenize(self, sent):\n",
    "        sent = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sent))\n",
    "        sent = re.sub(r\"\\!+\", \"!\", sent)\n",
    "        sent = re.sub(r\"\\,+\", \",\", sent)\n",
    "        sent = re.sub(r\"\\?+\", \"?\", sent)\n",
    "        sent = re.sub(r\"[ ]+\", \" \", sent)\n",
    "        sent = sent.lower()\n",
    "        sent = [token.text for token in self.ob.tokenizer(sent) if token.text != \" \"]\n",
    "        sent = \" \".join(sent)\n",
    "        return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e08f72",
   "metadata": {
    "id": "34e08f72"
   },
   "outputs": [],
   "source": [
    "\n",
    "def dum_tokenizer(sent):\n",
    "    return sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c343481a",
   "metadata": {
    "id": "c343481a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\data\\\\df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m FR \u001b[38;5;241m=\u001b[39m torchtext\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mField(tokenize\u001b[38;5;241m=\u001b[39mdum_tokenizer, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, init_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<sos>\u001b[39m\u001b[38;5;124m\"\u001b[39m, eos_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<eos>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\\\n\u001b[0;32m      2\u001b[0m                               batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m EN \u001b[38;5;241m=\u001b[39m torchtext\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mField(tokenize\u001b[38;5;241m=\u001b[39mdum_tokenizer, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m train \u001b[38;5;241m=\u001b[39m torchtext\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTabularDataset(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, fields\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEN\u001b[39m\u001b[38;5;124m\"\u001b[39m, EN), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFR\u001b[39m\u001b[38;5;124m\"\u001b[39m, FR)])\n\u001b[0;32m      5\u001b[0m FR\u001b[38;5;241m.\u001b[39mbuild_vocab(train)\n\u001b[0;32m      6\u001b[0m EN\u001b[38;5;241m.\u001b[39mbuild_vocab(train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torchtext\\data\\dataset.py:251\u001b[0m, in \u001b[0;36mTabularDataset.__init__\u001b[1;34m(self, path, format, fields, skip_header, csv_reader_params, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m    247\u001b[0m make_example \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m: Example\u001b[38;5;241m.\u001b[39mfromJSON, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdict\u001b[39m\u001b[38;5;124m'\u001b[39m: Example\u001b[38;5;241m.\u001b[39mfromdict,\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsv\u001b[39m\u001b[38;5;124m'\u001b[39m: Example\u001b[38;5;241m.\u001b[39mfromCSV, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m: Example\u001b[38;5;241m.\u001b[39mfromCSV}[\u001b[38;5;28mformat\u001b[39m]\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(path), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    253\u001b[0m         reader \u001b[38;5;241m=\u001b[39m unicode_csv_reader(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcsv_reader_params)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\data\\\\df.csv'"
     ]
    }
   ],
   "source": [
    "FR = torchtext.data.Field(tokenize=dum_tokenizer, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\",\\\n",
    "                              batch_first=True)\n",
    "EN = torchtext.data.Field(tokenize=dum_tokenizer, lower=True, batch_first=True)\n",
    "train = torchtext.data.TabularDataset(os.path.join(\"..\",\"data\", \"df.csv\"), format=\"csv\", fields=[(\"EN\", EN), (\"FR\", FR)])\n",
    "FR.build_vocab(train)\n",
    "EN.build_vocab(train)\n",
    "src_vocab = len(EN.vocab)\n",
    "trg_vocab = len(FR.vocab)\n",
    "tokenizer_en = tokener(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc75b72c",
   "metadata": {
    "id": "cc75b72c",
    "outputId": "2564d22c-bd65-48f2-97f2-294cb73cab92"
   },
   "outputs": [],
   "source": [
    "net = Transformer(src_vocab=src_vocab, trg_vocab=trg_vocab, d_model=512, num=6, n_heads=8).cuda()\n",
    "checkpoint=torch.load(os.path.join(\"..\",\"data\" ,\"test_model_best_0.pth.tar\" ))\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19ed0a0",
   "metadata": {
    "id": "b19ed0a0"
   },
   "outputs": [],
   "source": [
    "sent = 'I stayed.'\n",
    "sent = tokenizer_en.tokenize(sent).split()\n",
    "sent = [EN.vocab.stoi[tok] for tok in sent]\n",
    "sent = Variable(torch.LongTensor(sent)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f75b8",
   "metadata": {
    "id": "6e9f75b8"
   },
   "outputs": [],
   "source": [
    "trg_init = FR.vocab.stoi[\"<sos>\"]\n",
    "trg_init = Variable(torch.LongTensor([trg_init])).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e692006",
   "metadata": {
    "id": "1e692006",
    "outputId": "36110cb0-27c3-4098-9d06-52c2762cf057"
   },
   "outputs": [],
   "source": [
    "cuda=True\n",
    "trg = trg_init\n",
    "src_mask, _ = create_masks(sent, trg_init)\n",
    "sent = sent.cuda();\n",
    "src_mask = src_mask.cuda()\n",
    "trg = trg.cuda()\n",
    "e_out = net.encoder(sent, src_mask)\n",
    "translated_word = []; translated_word_idxs = []\n",
    "for i in range(2, 80):\n",
    "            trg_mask = create_trg_mask(trg, cuda=cuda)\n",
    "\n",
    "            trg = trg.cuda(); trg_mask = trg_mask.cuda()\n",
    "\n",
    "            outputs = net.fc1(net.decoder(trg, e_out, src_mask, trg_mask))\n",
    "            out_idxs = torch.softmax(outputs, dim=2).max(2)[1]\n",
    "\n",
    "            trg = torch.cat((trg, out_idxs[:,-1:]), dim=1)\n",
    "\n",
    "            out_idxs = out_idxs.cpu().numpy()\n",
    "\n",
    "            translated_word_idxs.append(out_idxs.tolist()[0][-1])\n",
    "            if translated_word_idxs[-1] == FR.vocab.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            translated_word.append(FR.vocab.itos[translated_word_idxs[-1]])\n",
    "\n",
    "print(\" \".join(translated_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8cfb21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
