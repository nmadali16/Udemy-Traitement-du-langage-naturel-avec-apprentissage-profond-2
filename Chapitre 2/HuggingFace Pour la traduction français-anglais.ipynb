{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9w1Amq-w5sp"
   },
   "source": [
    "L'utilisation d'un modèle Hugging Face de traduction français-anglais pré-entraîné :**\n",
    "\n",
    "\n",
    "**Objectif :** L'objectif de ce notebook est de démontrer comment utiliser un modèle Hugging Face pré-entraîné pour traduire du texte du français vers l'anglais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHciFgCkwwFn"
   },
   "source": [
    "Bien sûr, voici les étapes traduites en français :\n",
    "\n",
    "**Étapes :**\n",
    "\n",
    "1. **Mise en place de l'environnement :**\n",
    "   - Créez un environnement Python et installez les bibliothèques nécessaires :\n",
    "     ```bash\n",
    "     pip install transformers\n",
    "     ```\n",
    "\n",
    "2. **Chargement du Modèle Pré-entraîné :**\n",
    "   - Importez les bibliothèques nécessaires et chargez le modèle de traduction français-anglais pré-entraîné.\n",
    "   ```python\n",
    "   from transformers import MarianMTModel, MarianTokenizer\n",
    "   \n",
    "   nom_du_modele = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "   modele = MarianMTModel.from_pretrained(nom_du_modele)\n",
    "   tokenizer = MarianTokenizer.from_pretrained(nom_du_modele)\n",
    "   ```\n",
    "\n",
    "3. **Tokenisation du Texte d'Entrée :**\n",
    "   - Tokenisez le texte d'entrée dans un format adapté pour le modèle.\n",
    "   ```python\n",
    "   texte_dentree = \"Votre texte ici.\"  # Remplacez par votre propre texte\n",
    "   entrees = tokenizer(texte_dentree, return_tensors=\"pt\")\n",
    "   ```\n",
    "\n",
    "4. **Traduction du Texte :**\n",
    "   - Utilisez le modèle pré-entraîné pour traduire le texte du français vers l'anglais.\n",
    "   ```python\n",
    "   with torch.no_grad():\n",
    "       resultats = modele(**entrees)\n",
    "   \n",
    "   texte_traduit = tokenizer.decode(resultats.logits[0], skip_special_tokens=True)\n",
    "   ```\n",
    "\n",
    "5. **Affichage des Résultats :**\n",
    "   - Affichez le texte traduit.\n",
    "   ```python\n",
    "   print(f\"Texte d'Entrée (Français) : {texte_dentree}\")\n",
    "   print(f\"Texte Traduit (Anglais) : {texte_traduit}\")\n",
    "   ```\n",
    "\n",
    "6. **Exécution du Laboratoire :**\n",
    "   - Exécutez le code pour effectuer la traduction. Assurez-vous d'avoir une connexion internet active pour télécharger le modèle et les dépendances.\n",
    "\n",
    "7. **Interprétation :**\n",
    "   - Observez et analysez la sortie traduite. Vérifiez la précision et la fluidité de la traduction.\n",
    "\n",
    "**Remarque :**\n",
    "- Vous pouvez remplacer la variable `nom_du_modele` par d'autres modèles pré-entraînés disponibles sur le hub de modèles Hugging Face pour différentes paires de langues ou variations.\n",
    "- Assurez-vous d'avoir une connexion internet lors de la première exécution du script pour télécharger le modèle et le tokeniseur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uU7gaUsMxa9J"
   },
   "outputs": [],
   "source": [
    "\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9630,
     "status": "ok",
     "timestamp": 1699427710615,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "f04vmXZtxz-p",
    "outputId": "d5f3bc8e-c61b-4cd3-c883-7104b10b8390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\nmadali\\appdata\\local\\anaconda3\\lib\\site-packages (0.1.99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\nmadali\\appdata\\local\\anaconda3\\lib\\site-packages\\pyasn1_modules-0.3.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n"
     ]
    }
   ],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5793,
     "status": "ok",
     "timestamp": 1699427823268,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "5GCc2-s7xbaG",
    "outputId": "90c6337d-7e89-4f95-f16e-77860396fa04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "nom_du_modele = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "modele = MarianMTModel.from_pretrained(nom_du_modele)\n",
    "tokenizer = MarianTokenizer.from_pretrained(nom_du_modele)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3rOMWNJw072"
   },
   "source": [
    "\n",
    "\n",
    "**Qu'est-ce que la classe MarianMTModel :**\n",
    "\n",
    "`MarianMTModel` est une classe fournie par la bibliothèque `transformers` de Hugging Face. Elle fait partie de la famille de modèles MarianMT (Multilingual Translation), conçus pour les tâches de traduction automatique multilingue, vous permettant de traduire du texte entre diverses paires de langues.\n",
    "\n",
    "Le modèle `MarianMTModel` est basé sur le système de traduction automatique neuronal Marian développé par l'Université d'Édimbourg et l'Université de Tartu. Il est particulièrement reconnu pour son efficacité et son efficacité dans le traitement des tâches de traduction multilingue.\n",
    "\n",
    "Voici un bref aperçu de `MarianMTModel` :\n",
    "\n",
    "- **Architecture :** Il est généralement basé sur l'architecture transformer, similaire aux modèles tels que BERT, GPT, etc. Plus précisément, il utilise des variantes de l'architecture transformer adaptées aux tâches de traduction automatique.\n",
    "\n",
    "- **Pré-entraînement :** Le modèle est généralement pré-entraîné sur un grand corpus de données texte multilingues, ce qui lui permet de comprendre et de générer des traductions pour diverses paires de langues.\n",
    "\n",
    "- **Ajustement fin :** Bien que vous puissiez effectuer un ajustement fin de ces modèles sur des tâches de traduction spécifiques, ils sont souvent utilisés tels quels en raison de leur efficacité dans le traitement de plusieurs langues.\n",
    "\n",
    "- **Agnostique de la langue :** Une des caractéristiques clés des modèles MarianMT est leur capacité à gérer plusieurs langues sans avoir besoin de modèles spécifiques à chaque langue. Cela les rend particulièrement utiles pour les tâches impliquant la traduction entre des langues qui n'ont peut-être pas de modèles pré-entraînés dédiés disponibles.\n",
    "\n",
    "- **Hub de Modèles Hugging Face :** Le modèle est disponible sur le Hub de Modèles Hugging Face, qui propose un référentiel de modèles pré-entraînés pouvant être facilement utilisés dans diverses tâches de traitement du langage naturel.\n",
    "\n",
    "En pratique, vous pouvez utiliser `MarianMTModel` ainsi que son tokeniseur correspondant (généralement `MarianTokenizer`) pour effectuer des tâches de traduction multilingue. C'est un outil puissant pour les tâches impliquant la traduction entre différentes langues, et il est largement utilisé dans la recherche et le développement de systèmes de traduction automatique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9i7BESuHwt1A"
   },
   "outputs": [],
   "source": [
    "texte_dentree = [\"Quelle heure est-il ?\", 'Il fait très froid' ] # Remplacez par votre propre texte\n",
    "entrees = tokenizer(texte_dentree, return_tensors=\"pt\", padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1699428157491,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "fnunt1wDycvk",
    "outputId": "4975df72-0d04-4b43-e15b-a43df7c752be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['▁Quelle', '▁heure', '▁est', '-', 'il', '▁?'],\n",
       " ['▁Il', '▁fait', '▁très', '▁froid'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(texte_dentree[0]), tokenizer.tokenize(texte_dentree[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1699428161075,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "ShsLjLTlyaTH",
    "outputId": "654c0241-afdc-4a9f-b76b-27bcf4dcb090"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 4717,  4352,    43,    21,   107,    99,     0],\n",
      "        [  104,   148,   366, 10135,     0, 59513, 59513]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "print(entrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1699428835631,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "-z48Mj140GlX",
    "outputId": "0c323571-4b01-4da7-b468-9578cf80b35b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Quelle', '▁heure', '▁est', '-', 'il', '▁?', '</s>']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([ 59513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1267,
     "status": "ok",
     "timestamp": 1699428298125,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "SqR3_pYWyl6S",
    "outputId": "1f5598da-8a9f-4a30-ae00-14e9cd5b2b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8])   tensor([[59513,   430,   195,    32,    61,    54,     0, 59513],\n",
      "        [59513,   138,     6,     9,   420,  6793,     3,     0]])\n",
      "Quelle heure est-il ? ---> What time is it?\n",
      "Il fait très froid ---> It's very cold.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    resultats = modele.generate(**entrees)\n",
    "\n",
    "print(resultats.shape,' ',resultats)\n",
    "for i in range(len(texte_dentree)):\n",
    "  texte_traduit = tokenizer.decode(resultats[i], skip_special_tokens=True)\n",
    "  print(texte_dentree[i],'--->',texte_traduit)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOsgJKJ+QVjlhnT28zwZBQV",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
