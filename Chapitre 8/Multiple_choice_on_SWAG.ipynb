{"cells":[{"cell_type":"markdown","source":["# Question √† choix multiples"],"metadata":{"id":"msDGSVwjInuR"}},{"cell_type":"markdown","metadata":{"id":"X4cRE8IbIrIV"},"source":["Une t√¢che √† choix multiples est similaire √† la r√©ponse √† une question, sauf que plusieurs r√©ponses candidates sont fournies avec un contexte et que le mod√®le est entra√Æn√© √† s√©lectionner la bonne r√©ponse.\n","\n","Ce guide vous montrera comment :\n","\n","1. Fine-tune [BERT](https://huggingface.co/bert-base-uncased) sur la configuration \"r√©guli√®re\" de l'ensemble de donn√©es [SWAG](https://huggingface.co/datasets/swag) pour s√©lectionner la meilleure r√©ponse √† partir d'options multiples et d'un certain contexte.\n","2. Utilisez votre mod√®le fine-tune pour l'inf√©rence.\n","\n","<Tip>\n","La t√¢che illustr√©e dans ce tutoriel est prise en charge par les architectures de mod√®le suivantes :\n","\n","<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n","\n","[ALBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/albert), [BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bert), [BigBird](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/big_bird), [CamemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/camembert), [CANINE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/canine), [ConvBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/convbert), [Data2VecText](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/data2vec-text), [DeBERTa-v2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta-v2), [DistilBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/distilbert), [ELECTRA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/electra), [ERNIE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie), [ErnieM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie_m), [FlauBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/flaubert), [FNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/fnet), [Funnel Transformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/funnel), [I-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ibert), [Longformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longformer), [LUKE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/luke), [MEGA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mega), [Megatron-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/megatron-bert), [MobileBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mobilebert), [MPNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mpnet), [Nezha](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nezha), [Nystr√∂mformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nystromformer), [QDQBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/qdqbert), [RemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/rembert), [RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta), [RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta-prelayernorm), [RoCBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roc_bert), [RoFormer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roformer), [SqueezeBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/squeezebert), [XLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm), [XLM-RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta), [XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta-xl), [XLNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlnet), [X-MOD](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xmod), [YOSO](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/yoso)\n","\n","<!--End of the generated tip-->\n","\n","</Tip>\n","\n","Avant de commencer, assurez-vous que toutes les biblioth√®ques n√©cessaires sont install√©es :\n","\n","```bash\n","pip install transformers datasets evaluate\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MOsHUjgdIrIW"},"outputs":[],"source":["!pip install datasets transformers"]},{"cell_type":"markdown","metadata":{"id":"wXbHRbaDjQwB"},"source":["Vous devez ensuite installer Git-LFS. D√©commentez les instructions suivantes :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gvh-3-ezjQwB","outputId":"c67de4ee-09b4-488d-dca8-86b833d1f921","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699786935895,"user_tz":-60,"elapsed":1861,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n","0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n"]}],"source":["!apt install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"dr0d8nx1jQwB"},"source":["Assurez-vous que votre version de Transformers est au moins 4.11.0 car la fonctionnalit√© a √©t√© introduite dans cette version :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yk4jnBMmjQwC","outputId":"a4521367-a6b9-44bc-a3b8-26345abf0dbc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699786940258,"user_tz":-60,"elapsed":4367,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["4.35.0\n"]}],"source":["import transformers\n","\n","print(transformers.__version__)"]},{"cell_type":"markdown","metadata":{"id":"FzQLs6eDjQwC"},"source":["Nous t√©l√©chargeons √©galement rapidement des donn√©es t√©l√©m√©triques, qui nous indiquent quels exemples et quelles versions de logiciels sont utilis√©s, afin que nous sachions o√π concentrer nos efforts de maintenance. Nous ne recueillons (ni ne nous int√©ressons √†) aucune information personnelle identifiable, mais si vous pr√©f√©rez ne pas √™tre compt√©, n'h√©sitez pas √† sauter cette √©tape ou √† supprimer enti√®rement cette cellule."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7mKKqSdjQwC"},"outputs":[],"source":["from transformers.utils import send_example_telemetry\n","\n","send_example_telemetry(\"multiple_choice_notebook\", framework=\"pytorch\")"]},{"cell_type":"markdown","metadata":{"id":"rEJBSTyZIrIb"},"source":["# Fine-tuning   d'un mod√®le sur une t√¢che √† choix multiples"]},{"cell_type":"markdown","metadata":{"id":"kTCFado4IrIc"},"source":["Dans ce carnet, nous verrons comment affiner l'un des mod√®les [ü§ó Transformers](https://github.com/huggingface/transformers) pour une t√¢che de choix multiple, qui est la t√¢che de s√©lectionner les entr√©es les plus plausibles dans une s√©lection donn√©e. L'ensemble de donn√©es utilis√© ici est [SWAG](https://www.aclweb.org/anthology/D18-1009/) mais vous pouvez adapter le pr√©traitement √† tout autre ensemble de donn√©es √† choix multiples que vous souhaitez, ou √† vos propres donn√©es. SWAG est un jeu de donn√©es sur le raisonnement de bon sens, o√π chaque exemple d√©crit une situation puis propose quatre options qui pourraient y r√©pondre.\n","\n","Ce bloc-notes est con√ßu pour fonctionner avec n'importe quel point de contr√¥le de mod√®le provenant du [Model Hub] (https://huggingface.co/models), √† condition que ce mod√®le ait une version avec une t√™te √† choix multiple. En fonction de votre mod√®le et du GPU que vous utilisez, il se peut que vous deviez ajuster la taille du lot pour √©viter les erreurs de sortie de m√©moire. R√©glez ces deux param√®tres, et le reste du carnet devrait fonctionner sans probl√®me :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zVvslsfMIrIh"},"outputs":[],"source":["model_checkpoint = \"bert-base-uncased\"\n","batch_size = 16"]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Chargement du jeu de donn√©es"]},{"cell_type":"markdown","metadata":{"id":"W7QYTpxXIrIl"},"source":["Nous utiliserons la biblioth√®que [ü§ó Datasets](https://github.com/huggingface/datasets) pour t√©l√©charger les donn√©es. Cela peut √™tre facilement fait avec les fonctions `load_dataset`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IreSlFmlIrIm"},"outputs":[],"source":["from datasets import load_dataset, load_metric"]},{"cell_type":"markdown","metadata":{"id":"CKx2zKs5IrIq"},"source":["`load_dataset` mettra en cache le jeu de donn√©es pour √©viter de le t√©l√©charger √† nouveau la prochaine fois que vous ex√©cuterez cette cellule."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_AY1ATSIrIq"},"outputs":[],"source":["datasets = load_dataset(\"swag\", \"regular\")"]},{"cell_type":"markdown","metadata":{"id":"RzfPtOMoIrIu"},"source":["L'objet `dataset` lui-m√™me est [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), qui contient une cl√© pour l'ensemble d'apprentissage, de validation et de test (avec plus de cl√©s pour l'ensemble de validation et de test non appari√© dans le cas particulier de `mnli`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWiVUF0jIrIv","outputId":"c0eed95b-dcde-4c93-f64d-adf95e11b58c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699786943460,"user_tz":-60,"elapsed":16,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n","        num_rows: 73546\n","    })\n","    validation: Dataset({\n","        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n","        num_rows: 20006\n","    })\n","    test: Dataset({\n","        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n","        num_rows: 20005\n","    })\n","})"]},"metadata":{},"execution_count":8}],"source":["datasets"]},{"cell_type":"markdown","metadata":{"id":"u3EtYfeHIrIz"},"source":["Pour acc√©der √† un √©l√©ment r√©el, vous devez d'abord s√©lectionner une division, puis donner un index :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6HrpprwIrIz","outputId":"f0a39aa7-83c4-425f-bb9c-0c60a6f9cef7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699786943461,"user_tz":-60,"elapsed":14,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'video-id': 'anetv_jkn6uvmqwh4',\n"," 'fold-ind': '3416',\n"," 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n"," 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n"," 'sent2': 'A drum line',\n"," 'gold-source': 'gold',\n"," 'ending0': 'passes by walking down the street playing their instruments.',\n"," 'ending1': 'has heard approaching them.',\n"," 'ending2': \"arrives and they're outside dancing and asleep.\",\n"," 'ending3': 'turns the lead singer watches the performance.',\n"," 'label': 0}"]},"metadata":{},"execution_count":9}],"source":["datasets[\"train\"][0]"]},{"cell_type":"markdown","metadata":{"id":"WHUmphG3IrI3"},"source":["Pour avoir une id√©e de l'aspect des donn√©es, la fonction suivante montre quelques exemples choisis au hasard dans l'ensemble de donn√©es."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3j8APAoIrI3"},"outputs":[],"source":["from datasets import ClassLabel\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples=10):\n","    assert num_examples <= len(dataset), \"Il n'est pas possible de s√©lectionner plus d'√©l√©ments qu'il n'y en a dans l'ensemble de donn√©es..\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","\n","    df = pd.DataFrame(dataset[picks])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","    display(HTML(df.to_html()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZy5tRB_IrI7","outputId":"9cc125c1-7284-4caa-fe1f-c47f0136f009","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1699786943461,"user_tz":-60,"elapsed":12,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video-id</th>\n","      <th>fold-ind</th>\n","      <th>startphrase</th>\n","      <th>sent1</th>\n","      <th>sent2</th>\n","      <th>gold-source</th>\n","      <th>ending0</th>\n","      <th>ending1</th>\n","      <th>ending2</th>\n","      <th>ending3</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>lsmdc0025_THE_LORD_OF_THE_RINGS_THE_RETURN_OF_THE_KING-61837</td>\n","      <td>4747</td>\n","      <td>A little girl toddles up to greet him. Someone cotton</td>\n","      <td>A little girl toddles up to greet him.</td>\n","      <td>Someone cotton</td>\n","      <td>gold</td>\n","      <td>throws a horse onto the cart causing a branch to hit.</td>\n","      <td>arabian is vertical and rugged.</td>\n","      <td>- spreads it back on the turntable.</td>\n","      <td>steps up and kisses someone on the cheek.</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>lsmdc3033_HUGO-14943</td>\n","      <td>19103</td>\n","      <td>Someone kicks the inspector in the shin, causing him to lose his grip. Someone</td>\n","      <td>Someone kicks the inspector in the shin, causing him to lose his grip.</td>\n","      <td>Someone</td>\n","      <td>gen</td>\n","      <td>is dragged out of the water.</td>\n","      <td>holds him international his wounded gaze.</td>\n","      <td>grins, just keeps his dildo retreating into the garage.</td>\n","      <td>turns away from the door bolt.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>anetv_6HyNydVIji4</td>\n","      <td>2255</td>\n","      <td>We see a lady performing a song on a flute. The girl</td>\n","      <td>We see a lady performing a song on a flute.</td>\n","      <td>The girl</td>\n","      <td>gold</td>\n","      <td>pauses and plays again.</td>\n","      <td>spins around wearing several lights.</td>\n","      <td>spins and smiles at the camera.</td>\n","      <td>lifts the baby up in the air.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>lsmdc3017_CHRONICLE-7167</td>\n","      <td>14248</td>\n","      <td>With his free hand, he grabs the camera. Sharing his point - of - view, we</td>\n","      <td>With his free hand, he grabs the camera.</td>\n","      <td>Sharing his point - of - view, we</td>\n","      <td>gen</td>\n","      <td>see someone lying down from his attempt.</td>\n","      <td>see someone giving his instructions.</td>\n","      <td>glimpse his show collapse on the pavement.</td>\n","      <td>see it change into rhythm.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>lsmdc3019_COLOMBIANA-8317</td>\n","      <td>11548</td>\n","      <td>He returns to his seat. Someone</td>\n","      <td>He returns to his seat.</td>\n","      <td>Someone</td>\n","      <td>gold</td>\n","      <td>beams beside someone as he rubs her chest.</td>\n","      <td>sits at his desk.</td>\n","      <td>sips his drink as he watches his friend leave.</td>\n","      <td>walks down the corridor.</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>lsmdc1008_Spider-Man2-76017</td>\n","      <td>18668</td>\n","      <td>Someone crashes into a set of shelves filled with spherical bombs. There</td>\n","      <td>Someone crashes into a set of shelves filled with spherical bombs.</td>\n","      <td>There</td>\n","      <td>gold</td>\n","      <td>takes possession of the surging metal.</td>\n","      <td>someone and his friends follow each and open his mechanics for the last remark on the chess piece.</td>\n","      <td>are row upon row of the weapons his father once used.</td>\n","      <td>someone reaches into the passenger window, moves the wall punctuated back from the cabinet side - first as kids rush out.</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>lsmdc3058_RUBY_SPARKS-28352</td>\n","      <td>3237</td>\n","      <td>The little dog runs up to her. Someone</td>\n","      <td>The little dog runs up to her.</td>\n","      <td>Someone</td>\n","      <td>gold</td>\n","      <td>appears from someone in the pantry.</td>\n","      <td>takes off her bike helmet and shakes out her hair.</td>\n","      <td>flops back, then drops the rake, clutching his chest.</td>\n","      <td>pulls her down with breaks.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>lsmdc3034_IDES_OF_MARCH-3092</td>\n","      <td>8246</td>\n","      <td>Leaning forward, she brings a hand to her mouth and shifts her hopeless, tearful gaze. Now someone</td>\n","      <td>Leaning forward, she brings a hand to her mouth and shifts her hopeless, tearful gaze.</td>\n","      <td>Now someone</td>\n","      <td>gold</td>\n","      <td>sprints down the hospital dock and sets back onto the ridge, wearing the dress and carrying a suitcase.</td>\n","      <td>wrenches the wheel to the cooling counter.</td>\n","      <td>marches purposefully down a sidewalk, his jaw set tight.</td>\n","      <td>stares at the lad in the swimming costume, the lights show as him walking through the rain.</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>anetv_IgyBIt3GTAU</td>\n","      <td>7205</td>\n","      <td>The man then takes a small can of wax and puts it on a piece of wood that he is holding in his hand. After dabbing the wax with a white rag, he</td>\n","      <td>The man then takes a small can of wax and puts it on a piece of wood that he is holding in his hand.</td>\n","      <td>After dabbing the wax with a white rag, he</td>\n","      <td>gold</td>\n","      <td>removes the garment, putting it on, and tastes it with it.</td>\n","      <td>begins running his fingers over the wood and showing other pieces of wood that the product has been used on.</td>\n","      <td>begins to make the shape closeups with a lot of wax.</td>\n","      <td>shows the wax again wrapped in a parcel.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>lsmdc1056_Rambo-97140</td>\n","      <td>6662</td>\n","      <td>They raise their rifles and someone leaps onto someone to shield her. Someone</td>\n","      <td>They raise their rifles and someone leaps onto someone to shield her.</td>\n","      <td>Someone</td>\n","      <td>gold</td>\n","      <td>gazes into her eyes.</td>\n","      <td>punches someone, knocking him to the floor.</td>\n","      <td>ducks into the light.</td>\n","      <td>shoots one of them.</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>"]},"metadata":{}}],"source":["show_random_elements(datasets[\"train\"])"]},{"cell_type":"markdown","metadata":{"id":"owkLcHf9jQwD"},"source":["Chaque exemple de l'ensemble de donn√©es a un contexte compos√© d'une premi√®re phrase (dans le champ `sent1`) et d'une introduction √† la deuxi√®me phrase (dans le champ `sent2`). Ensuite, quatre fins possibles sont donn√©es (dans les champs `ending0`, `ending1`, `ending2` et `ending3`) et le mod√®le doit choisir la bonne (indiqu√©e dans le champ `label`). La fonction suivante nous permet de mieux visualiser un exemple donn√© :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sOnYrhdxjQwE"},"outputs":[],"source":["def show_one(example):\n","    print(f\"Context: {example['sent1']}\")\n","    print(f\"  A - {example['sent2']} {example['ending0']}\")\n","    print(f\"  B - {example['sent2']} {example['ending1']}\")\n","    print(f\"  C - {example['sent2']} {example['ending2']}\")\n","    print(f\"  D - {example['sent2']} {example['ending3']}\")\n","    print(f\"\\nGround truth: option {['A', 'B', 'C', 'D'][example['label']]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qm_dl_gsjQwE","outputId":"6067fa2a-c9ca-4e2c-afdb-217a5a030263","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699786943462,"user_tz":-60,"elapsed":11,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Context: Members of the procession walk down the street holding small horn brass instruments.\n","  A - A drum line passes by walking down the street playing their instruments.\n","  B - A drum line has heard approaching them.\n","  C - A drum line arrives and they're outside dancing and asleep.\n","  D - A drum line turns the lead singer watches the performance.\n","\n","Ground truth: option A\n"]}],"source":["show_one(datasets[\"train\"][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRy3_8gMjQwE","outputId":"f92fc039-adb6-4154-f406-89519b526856","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699786943462,"user_tz":-60,"elapsed":9,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Context: Now it's someone's turn to rain blades on his opponent.\n","  A - Someone pats his shoulder and spins wildly.\n","  B - Someone lunges forward through the window.\n","  C - Someone falls to the ground.\n","  D - Someone rolls up his fast run from the water and tosses in the sky.\n","\n","Ground truth: option C\n"]}],"source":["show_one(datasets[\"train\"][15])"]},{"cell_type":"markdown","metadata":{"id":"n9qywopnIrJH"},"source":["## Pr√©traitement des donn√©es"]},{"cell_type":"markdown","metadata":{"id":"YVx71GdAIrJH"},"source":["Avant de pouvoir introduire ces textes dans notre mod√®le, nous devons les pr√©traiter. Ceci est fait par un ü§ó Transformers `Tokenizer` qui va (comme son nom l'indique) tokeniser les entr√©es (y compris convertir les tokens en leurs IDs correspondants dans le vocabulaire pr√©-entra√Æn√©) et les mettre dans un format attendu par le mod√®le, ainsi que g√©n√©rer les autres entr√©es dont le mod√®le a besoin.\n","\n","Pour faire tout cela, nous instancions notre tokenizer avec la m√©thode `AutoTokenizer.from_pretrained`, qui s'assurera que :\n","\n","- nous obtenons un tokenizer qui correspond √† l'architecture du mod√®le que nous voulons utiliser,\n","- nous t√©l√©chargeons le vocabulaire utilis√© lors du pr√©-entra√Ænement de ce point de contr√¥le sp√©cifique.\n","\n","Ce vocabulaire sera mis en cache, de sorte qu'il ne sera pas t√©l√©charg√© √† nouveau la prochaine fois que nous ex√©cuterons la cellule."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXNLu_-nIrJI"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"]},{"cell_type":"markdown","metadata":{"id":"Vl6IidfdIrJK"},"source":["Nous passons `use_fast=True` √† l'appel ci-dessus pour utiliser un des tokenizers rapides (soutenu par Rust) de la biblioth√®que ü§ó Tokenizers. Ces tokenizers rapides sont disponibles pour presque tous les mod√®les, mais si vous avez eu une erreur avec l'appel pr√©c√©dent, enlevez cet argument."]},{"cell_type":"markdown","metadata":{"id":"rowT4iCLIrJK"},"source":["Vous pouvez appeler directement ce tokenizer sur une phrase ou une paire de phrases :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5hBlsrHIrJL","outputId":"e0e3d23c-ad70-4ac7-cf9a-6119c7f07e12","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699786943845,"user_tz":-60,"elapsed":5,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":16}],"source":["tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"]},{"cell_type":"markdown","metadata":{"id":"qo_0B1M2IrJM"},"source":["Selon le mod√®le que vous avez s√©lectionn√©, vous verrez diff√©rentes cl√©s dans le dictionnaire retourn√© par la cellule ci-dessus. Elles n'ont pas beaucoup d'importance pour ce que nous faisons ici (sachez simplement qu'elles sont requises par le mod√®le que nous instancierons plus tard), vous pouvez en apprendre plus √† leur sujet dans [ce tutoriel](https://huggingface.co/transformers/preprocessing.html) si cela vous int√©resse.\n","\n","Pour pr√©traiter notre jeu de donn√©es, nous aurons donc besoin des noms des colonnes contenant la/les phrase(s). Le dictionnaire suivant garde la trace de la t√¢che de correspondance avec les noms de colonnes :"]},{"cell_type":"markdown","metadata":{"id":"2C0hcmp9IrJQ"},"source":["Nous pouvons alors √©crire la fonction qui va pr√©traiter nos √©chantillons. La partie la plus d√©licate est de mettre toutes les paires de phrases possibles dans deux grandes listes avant de les passer au tokenizer, puis de d√©compresser le r√©sultat pour que chaque exemple ait quatre identifiants d'entr√©e, masques d'attention, etc.\n","\n","Lors de l'appel au `tokenizer`, nous utilisons l'argument `truncation=True`. Cela garantit qu'une entr√©e plus longue que ce que le mod√®le s√©lectionn√© peut g√©rer sera tronqu√©e √† la longueur maximale accept√©e par le mod√®le."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vc0BSBLIIrJQ"},"outputs":[],"source":["ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n","\n","def preprocess_function(examples):\n","    # R√©p√©tez chaque premi√®re phrase quatre fois pour aller avec les quatre possibilit√©s de deuxi√®me phrase.\n","    first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n","    # Saisissez toutes les secondes phrases possibles pour chaque contexte.\n","    question_headers = examples[\"sent2\"]\n","    second_sentences = [[f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)]\n","\n","    # Aplatir tout\n","    first_sentences = sum(first_sentences, [])\n","    second_sentences = sum(second_sentences, [])\n","\n","    # Tokenize\n","    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n","    # Un-flatten\n","    return {k: [v[i:i+4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"]},{"cell_type":"markdown","metadata":{"id":"0lm8ozrJIrJR"},"source":["Cette fonction fonctionne avec un ou plusieurs exemples. Dans le cas de plusieurs exemples, le tokenizer retournera une liste de listes de listes pour chaque cl√© : une liste de tous les exemples (ici 5), puis une liste de tous les choix (4) et une liste d'ID d'entr√©e (longueur variable ici puisque nous n'avons pas appliqu√© de padding) :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5lqxATZjQwJ","outputId":"16ef4ec6-95c7-41f7-fa77-d8a2a22db3b0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699786943845,"user_tz":-60,"elapsed":3,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["5 4 [30, 25, 30, 28]\n"]}],"source":["examples = datasets[\"train\"][:5]\n","features = preprocess_function(examples)\n","print(len(features[\"input_ids\"]), len(features[\"input_ids\"][0]), [len(x) for x in features[\"input_ids\"][0]])"]},{"cell_type":"markdown","metadata":{"id":"USST7rSHjQwJ"},"source":["Pour v√©rifier que nous n'avons rien fait en regroupant toutes les possibilit√©s puis en les aplatissant, examinons les entr√©es d√©cod√©es pour un exemple donn√© :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXCIQzWCjQwJ","outputId":"3b8188d7-fd03-46b4-9be6-8298410a02ac","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699786947949,"user_tz":-60,"elapsed":4106,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession are playing ping pong and celebrating one left each in quick. [SEP]',\n"," '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession wait slowly towards the cadets. [SEP]',\n"," '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession makes a square call and ends by jumping down into snowy streets where fans begin to take their positions. [SEP]',\n"," '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession play and go back and forth hitting the drums while the audience claps for them. [SEP]']"]},"metadata":{},"execution_count":19}],"source":["idx = 3\n","[tokenizer.decode(features[\"input_ids\"][idx][i]) for i in range(4)]"]},{"cell_type":"markdown","metadata":{"id":"epEYAUrijQwJ"},"source":["Nous pouvons la comparer √† la v√©rit√© terrain :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gim6FMnDjQwJ","outputId":"396d6f38-27c3-4ec7-b6a1-5032b23596ba","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699786947949,"user_tz":-60,"elapsed":24,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Context: A drum line passes by walking down the street playing their instruments.\n","  A - Members of the procession are playing ping pong and celebrating one left each in quick.\n","  B - Members of the procession wait slowly towards the cadets.\n","  C - Members of the procession makes a square call and ends by jumping down into snowy streets where fans begin to take their positions.\n","  D - Members of the procession play and go back and forth hitting the drums while the audience claps for them.\n","\n","Ground truth: option D\n"]}],"source":["show_one(datasets[\"train\"][3])"]},{"cell_type":"markdown","metadata":{"id":"zS-6iXTkIrJT"},"source":["Cela semble correct, donc nous pouvons appliquer cette fonction sur tous les exemples de notre jeu de donn√©es, nous utilisons simplement la m√©thode `map` de notre objet `dataset` que nous avons cr√©√© plus t√¥t. Cela appliquera la fonction sur tous les √©l√©ments de tous les splits de `dataset`, ainsi nos donn√©es d'entra√Ænement, de validation et de test seront pr√©trait√©es en une seule commande."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDtsaJeVIrJT","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["65fea6d5b9bf4a1bbd8747f50b7759c8","c78e9aa3efc348799a05fe1dac45579a","04d0e83fbc0144ecb5fd09864ac15e71","87cbc931cbfa4c3cbc9147cda31fac05","d265b25405a34396aa0519718d4b325b","d76a50d2678046e5b04764072350c7b7","a5c8d1263ad8482c9c6b2f4dfa73805e","3bfe3a464dde4e899ec8010f68f7d80b","8a21942d0ad54c92a87da9eff840b457","884bd103ab204270857996748a09e1bd","cbe5a55ff24745158568ded0b8328415","e43b7994a0bc41de8fc8b82c137f6d63","b9625dc7ed7343fa898826e3c6c1c418","d6f0b14b02fa42fca8bd344ca1e9e797","955de9e6d0e14eed812eb49c482e0e38","4ac88a162fb945bdac64a15c073d8bc3","bad6aa7e03954b5bbdf3b725cff57df3","2ed121c3508546fcb96ce2ce5baa4d56","aa652478db22468cabe5bd562aa12fc8","a95dbb2095d0494f89b7d71c1a59491c","9329f021a3ea4119a2d8ab64026cc5da","f6f0c22eb4644078adde62b408e55c4b","f6590f98e84240aaa2b8593e6f0e96c5","8690b4411bf747f594c0a91f80d03892","7c4911f92e774fc6aaf9c11dc6f95060","0ba222e001344b5db008edcc91cb2677","21559630fdce4818899537346886bda2","41e52310a5f64090883a11d2f91339c9","10df9b37f27d421bb30ea37c222605ed","c047725101124c34a42af061a8e02da8","9a15aa7be1974f74a9580396974fbd9d","680b829729da418b95637a3ea40357ca","a4ecb4d71ce84635a440a943186637bb"]},"executionInfo":{"status":"ok","timestamp":1699787028680,"user_tz":-60,"elapsed":80752,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"82d3d4fb-7704-43ca-e1b6-dfc093cb835e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/73546 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65fea6d5b9bf4a1bbd8747f50b7759c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/20006 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e43b7994a0bc41de8fc8b82c137f6d63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/20005 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6590f98e84240aaa2b8593e6f0e96c5"}},"metadata":{}}],"source":["encoded_datasets = datasets.map(preprocess_function, batched=True)"]},{"cell_type":"markdown","metadata":{"id":"voWiw8C7IrJV"},"source":["Mieux encore, les r√©sultats sont automatiquement mis en cache par la biblioth√®que ü§ó Datasets pour √©viter de perdre du temps √† cette √©tape la prochaine fois que vous ex√©cuterez votre notebook. La biblioth√®que ü§ó Datasets est normalement assez intelligente pour d√©tecter quand la fonction que vous passez √† map a chang√© (et n√©cessite donc de ne pas utiliser les donn√©es du cache). Par exemple, elle d√©tectera correctement si vous changez la t√¢che dans la premi√®re cellule et r√©ex√©cutez le carnet. ü§ó Datasets vous avertit lorsqu'il utilise des fichiers en cache, vous pouvez passer `load_from_cache_file=False` dans l'appel √† `map` pour ne pas utiliser les fichiers en cache et forcer le pr√©traitement √† √™tre appliqu√© √† nouveau.\n","\n","Notez que nous avons pass√© `batched=True` pour encoder les textes par lots ensemble. Cela permet de tirer le meilleur parti du tokenizer rapide que nous avons charg√© plus t√¥t, qui utilisera le multithreading pour traiter les textes d'un lot simultan√©ment."]},{"cell_type":"markdown","metadata":{"id":"545PP3o8IrJV"},"source":["## Affiner le mod√®le"]},{"cell_type":"markdown","metadata":{"id":"FBiW8UpKIrJW"},"source":["Maintenant que nos donn√©es sont pr√™tes, nous pouvons t√©l√©charger le mod√®le pr√©-entra√Æn√© et l'ajuster. Puisque notre t√¢che concerne les choix multiples, nous utilisons la classe `AutoModelForMultipleChoice`. Comme pour le tokenizer, la m√©thode `from_pretrained` t√©l√©chargera et mettra en cache le mod√®le pour nous."]},{"cell_type":"code","source":["model_checkpoint"],"metadata":{"id":"5iLQ0yIeRuLG","executionInfo":{"status":"ok","timestamp":1699788286600,"user_tz":-60,"elapsed":311,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"51d0085a-ad1e-459d-9ac7-99d97564b3c8","colab":{"base_uri":"https://localhost:8080/","height":35}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'bert-base-uncased'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":83}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TlqNaB8jIrJW","outputId":"ac716f04-0239-4136-ab64-c79e8a66c4d5","colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["88d6afc90c974ad69cae3477a52da29c","5bcfa86ceb1e4f53970b42375b4159de","ab39ab45648f4d0a9912bbf8eef00f73","458fd4ebf8d947fabfa5340e1749f991","4ef87ddfd8e74e3cab23ed9930085ba4","7dee16eca4e4461f8ba7679ea7f31d3f","49616d3e160a4da783394681797597a8","edfac5f04dd144b9b4301d6543084d4b","c5c5cbcc3efa4bacb6fbd11f47d8c2c9","543e26ba4c1a4a6e8a5e14d42dc7193c","2b429bee187d4b0c8c5389d46f019f14"]},"executionInfo":{"status":"ok","timestamp":1699787033094,"user_tz":-60,"elapsed":4441,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88d6afc90c974ad69cae3477a52da29c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n","\n","model = AutoModelForMultipleChoice.from_pretrained(model_checkpoint)"]},{"cell_type":"markdown","metadata":{"id":"CczA5lJlIrJX"},"source":["L'avertissement nous dit que nous jetons certains poids (les couches `vocab_transform` et `vocab_layer_norm`) et que nous en initialisons d'autres au hasard (les couches `pre_classifier` et `classifier`). C'est tout √† fait normal dans ce cas, car nous supprimons la t√™te utilis√©e pour pr√©-entra√Æner le mod√®le sur un objectif de mod√©lisation de langage masqu√© et nous la rempla√ßons par une nouvelle t√™te pour laquelle nous n'avons pas de poids pr√©-entra√Æn√©s, donc la biblioth√®que nous avertit que nous devrions affiner ce mod√®le avant de l'utiliser pour l'inf√©rence, ce qui est exactement ce que nous allons faire."]},{"cell_type":"markdown","metadata":{"id":"_N8urzhyIrJY"},"source":["Pour instancier un `Trainer`, nous aurons besoin de d√©finir trois autres choses. Le plus important est le [`TrainingArguments`] (https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), qui est une classe qui contient tous les attributs pour personnaliser l'entra√Ænement. Elle requiert un nom de dossier, qui sera utilis√© pour sauvegarder les points de contr√¥le du mod√®le, et tous les autres arguments sont optionnels :"]},{"cell_type":"code","source":["#pip install accelerate -U"],"metadata":{"id":"1oV-GxFjkUwp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bliy8zgjIrJY"},"outputs":[],"source":["model_name = model_checkpoint.split(\"/\")[-1]\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-swag\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    #push_to_hub=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"km3pGVdTIrJc"},"source":["Ici, nous fixons l'√©valuation √† la fin de chaque √©poch, nous ajustons le taux d'apprentissage, nous utilisons le `batch_size` d√©fini en haut du notebook et nous personnalisons le nombre d'√©pochs pour l'entra√Ænement, ainsi que la d√©croissance des poids.\n","\n","Le dernier argument permet de tout configurer pour que nous puissions pousser le mod√®le vers le [Hub](https://huggingface.co/models) r√©guli√®rement pendant l'apprentissage. Enlevez-le si vous n'avez pas suivi les √©tapes d'installation en haut du notebook. Si vous voulez sauvegarder votre mod√®le localement sous un nom diff√©rent de celui du d√©p√¥t dans lequel il sera pouss√©, ou si vous voulez pousser votre mod√®le sous une organisation et non sous votre espace de noms, utilisez l'argument `hub_model_id` pour d√©finir le nom du d√©p√¥t (il doit s'agir du nom complet, y compris votre espace de noms : par exemple `\"sgugger/bert-finetuned-swag\"` ou `\"huggingface/bert-finetuned-swag\"`).\n","\n","Ensuite, nous devons dire √† notre `Trainer` comment former des lots √† partir des entr√©es pr√©trait√©es. Nous n'avons pas encore fait de padding car nous allons padder chaque lot √† la longueur maximale √† l'int√©rieur du lot (au lieu de le faire avec la longueur maximale de l'ensemble des donn√©es). C'est le travail du *collateur de donn√©es*. Un collecteur de donn√©es prend une liste d'exemples et les convertit en un lot (en appliquant, dans notre cas, un remplissage). Comme il n'y a pas de collateur de donn√©es dans la biblioth√®que qui fonctionne pour notre probl√®me sp√©cifique, nous allons en √©crire un, adapt√© du `DataCollatorWithPadding` :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VuJK3OYkjQwK"},"outputs":[],"source":["from dataclasses import dataclass\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from typing import Optional, Union\n","import torch\n","\n","@dataclass\n","class DataCollatorForMultipleChoice:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs for multiple choice received.\n","    \"\"\"\n","\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","\n","    def __call__(self, features):\n","        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n","        labels = [feature.pop(label_name) for feature in features]\n","        batch_size = len(features)\n","        num_choices = len(features[0][\"input_ids\"])\n","        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n","        flattened_features = sum(flattened_features, [])\n","\n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","\n","        # Un-flatten\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        # Add back labels\n","        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n","        return batch"]},{"cell_type":"markdown","metadata":{"id":"EHNHslP0jQwK"},"source":["Lorsqu'il est appel√© sur une liste d'exemples, il aplatit toutes les entr√©es/masques d'attention, etc. dans de grandes listes qu'il passe √† la m√©thode `tokenizer.pad`. Cela retournera un dictionnaire avec de grands tenseurs (de la forme `(batch_size * 4) x seq_length`) que nous d√©compresserons ensuite.\n","\n","Nous pouvons v√©rifier que ce collateur de donn√©es fonctionne sur une liste de features, nous devons juste nous assurer de supprimer toutes les features qui ne sont pas des inputs accept√©s par notre mod√®le (ce que le `Trainer` fera automatiquement pour nous par la suite) :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-h4rzYgjQwK","outputId":"e276319b-3c72-4ee7-d5be-2c571863f8b3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699787033095,"user_tz":-60,"elapsed":26,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]}],"source":["accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]\n","features = [{k: v for k, v in encoded_datasets[\"train\"][i].items() if k in accepted_keys} for i in range(10)]\n","batch = DataCollatorForMultipleChoice(tokenizer)(features)"]},{"cell_type":"markdown","metadata":{"id":"dv8UNsXfjQwK"},"source":["\n","\n","```\n","# Ce texte est au format code\n","```\n","\n","Encore une fois, tous ces aplatissements/d√©platissements sont des sources d'erreurs potentielles :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OoSPnQaejQwK","outputId":"0190295d-dae5-4238-a495-b9a352a4d8f3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699787033095,"user_tz":-60,"elapsed":25,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] someone walks over to the radio. [SEP] someone hands her another phone. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n"," '[CLS] someone walks over to the radio. [SEP] someone takes the drink, then holds it. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n"," '[CLS] someone walks over to the radio. [SEP] someone looks off then looks at someone. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n"," '[CLS] someone walks over to the radio. [SEP] someone stares blearily down at the floor. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"]},"metadata":{},"execution_count":27}],"source":["[tokenizer.decode(batch[\"input_ids\"][8][i].tolist()) for i in range(4)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DV4LYj_jQwK","outputId":"0162487b-daa5-4d21-abd6-bdb6f61a905c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699787033095,"user_tz":-60,"elapsed":23,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Context: Someone walks over to the radio.\n","  A - Someone hands her another phone.\n","  B - Someone takes the drink, then holds it.\n","  C - Someone looks off then looks at someone.\n","  D - Someone stares blearily down at the floor.\n","\n","Ground truth: option D\n"]}],"source":["show_one(datasets[\"train\"][8])"]},{"cell_type":"markdown","metadata":{"id":"7sZOdRlRIrJd"},"source":["Tout va bien !\n","\n","La derni√®re chose √† d√©finir pour notre `Trainer` est comment calculer les m√©triques √† partir des pr√©dictions. Nous devons d√©finir une fonction pour cela, qui utilisera simplement la `m√©trie` que nous avons charg√©e plus t√¥t, le seul pr√©traitement que nous avons √† faire est de prendre l'argmax de nos logits pr√©dits :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UmvbnJ9JIrJd"},"outputs":[],"source":["import numpy as np\n","\n","def compute_metrics(eval_predictions):\n","    predictions, label_ids = eval_predictions\n","    preds = np.argmax(predictions, axis=1)\n","    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}"]},{"cell_type":"markdown","metadata":{"id":"rXuFTAzDIrJe"},"source":["Il ne nous reste plus qu'√† passer tout cela avec nos jeux de donn√©es au `Trainer` :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imY1oC3SIrJf"},"outputs":[],"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=encoded_datasets[\"train\"],\n","    eval_dataset=encoded_datasets[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=DataCollatorForMultipleChoice(tokenizer),\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"markdown","metadata":{"id":"CdzABDVcIrJg"},"source":["Nous pouvons maintenant affiner notre mod√®le en appelant simplement la m√©thode `train` :"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"EXKBVsrXjQwL","outputId":"87e9c35e-2a54-4a8a-fe23-9bfca00360c5","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1699787146334,"user_tz":-60,"elapsed":108019,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='194' max='13791' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  194/13791 01:43 < 2:02:26, 1.85 it/s, Epoch 0.04/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-31-3435b262f1ae>\", line 1, in <cell line: 1>\n","    trainer.train()\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1860, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2734, in training_step\n","    self.accelerator.backward(loss)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1989, in backward\n","    loss.backward(**kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 878, in getmodule\n","    os.path.realpath(f)] = module.__name__\n","  File \"/usr/lib/python3.10/posixpath.py\", line 397, in realpath\n","    return abspath(path)\n","  File \"/usr/lib/python3.10/posixpath.py\", line 386, in abspath\n","    return normpath(path)\n","  File \"/usr/lib/python3.10/posixpath.py\", line 371, in normpath\n","    path = sep.join(comps)\n","KeyboardInterrupt\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m<ipython-input-31-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1859\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}],"source":["trainer.train()"]},{"cell_type":"markdown","source":["## Inf√©rence\n","\n","Maintenant que vous avez mis au point un mod√®le, vous pouvez l'utiliser pour l'inf√©rence !\n","\n","Trouvez un texte et deux r√©ponses possibles :"],"metadata":{"id":"kyfwxOCZK1H-"}},{"cell_type":"code","source":["show_one(datasets[\"validation\"][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZIQKMVOHP7v-","executionInfo":{"status":"ok","timestamp":1699787996189,"user_tz":-60,"elapsed":249,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"fe145b25-54c5-4688-9696-7cca3f427786"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Context: Students lower their eyes nervously.\n","  A - She pats her shoulder, then saunters toward someone.\n","  B - She turns with two students.\n","  C - She walks slowly towards someone.\n","  D - She wheels around as her dog thunders out.\n","\n","Ground truth: option C\n"]}]},{"cell_type":"code","source":["prompt = \"Students lower their eyes nervously.\"\n","candidate1 = \"She pats her shoulder, then saunters toward someone.\"\n","candidate2 = \"She turns with two students.\"\n","candidate3=\"She walks slowly towards someone.\"\n","candidate4=\"She wheels around as her dog thunders out.\""],"metadata":{"id":"48lDWEfjK6gv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokeniser chaque paire d'invite et de r√©ponse de candidat et renvoyer des tenseurs PyTorch. Vous devez √©galement cr√©er des `labels` :"],"metadata":{"id":"miIzJl0_K9e3"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"ncduy/bert-base-uncased-finetuned-swag\")\n","inputs = tokenizer([[prompt, candidate1], [prompt, candidate2],[prompt, candidate3],[prompt, candidate4]], return_tensors=\"pt\", padding=True)\n","labels = torch.tensor(2).unsqueeze(0)"],"metadata":{"id":"jji5HByHK-rK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Passez vos entr√©es et vos √©tiquettes au mod√®le et renvoyez les `logits` :"],"metadata":{"id":"yU_rV1RELA_X"}},{"cell_type":"code","source":["from transformers import AutoModelForMultipleChoice\n","\n","model = AutoModelForMultipleChoice.from_pretrained(\"ncduy/bert-base-uncased-finetuned-swag\")\n","outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels=labels)\n","logits = outputs.logits"],"metadata":{"id":"bpQ7j003LC4P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Obtenez la classe avec la probabilit√© la plus √©lev√©e :"],"metadata":{"id":"oAhOhCAyLE1L"}},{"cell_type":"code","source":["predicted_class = logits.argmax().item()\n","predicted_class\n","print('Predicted: option %s'%['A','B','C','D'][predicted_class])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U2xMl3wuLGAb","executionInfo":{"status":"ok","timestamp":1699788119656,"user_tz":-60,"elapsed":251,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}},"outputId":"4424f5ab-2500-4971-dc96-29a001627b75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted: option C\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"65fea6d5b9bf4a1bbd8747f50b7759c8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c78e9aa3efc348799a05fe1dac45579a","IPY_MODEL_04d0e83fbc0144ecb5fd09864ac15e71","IPY_MODEL_87cbc931cbfa4c3cbc9147cda31fac05"],"layout":"IPY_MODEL_d265b25405a34396aa0519718d4b325b"}},"c78e9aa3efc348799a05fe1dac45579a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d76a50d2678046e5b04764072350c7b7","placeholder":"‚Äã","style":"IPY_MODEL_a5c8d1263ad8482c9c6b2f4dfa73805e","value":"Map: 100%"}},"04d0e83fbc0144ecb5fd09864ac15e71":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bfe3a464dde4e899ec8010f68f7d80b","max":73546,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a21942d0ad54c92a87da9eff840b457","value":73546}},"87cbc931cbfa4c3cbc9147cda31fac05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_884bd103ab204270857996748a09e1bd","placeholder":"‚Äã","style":"IPY_MODEL_cbe5a55ff24745158568ded0b8328415","value":" 73546/73546 [00:56&lt;00:00, 1297.16 examples/s]"}},"d265b25405a34396aa0519718d4b325b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d76a50d2678046e5b04764072350c7b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5c8d1263ad8482c9c6b2f4dfa73805e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3bfe3a464dde4e899ec8010f68f7d80b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a21942d0ad54c92a87da9eff840b457":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"884bd103ab204270857996748a09e1bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbe5a55ff24745158568ded0b8328415":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e43b7994a0bc41de8fc8b82c137f6d63":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b9625dc7ed7343fa898826e3c6c1c418","IPY_MODEL_d6f0b14b02fa42fca8bd344ca1e9e797","IPY_MODEL_955de9e6d0e14eed812eb49c482e0e38"],"layout":"IPY_MODEL_4ac88a162fb945bdac64a15c073d8bc3"}},"b9625dc7ed7343fa898826e3c6c1c418":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bad6aa7e03954b5bbdf3b725cff57df3","placeholder":"‚Äã","style":"IPY_MODEL_2ed121c3508546fcb96ce2ce5baa4d56","value":"Map: 100%"}},"d6f0b14b02fa42fca8bd344ca1e9e797":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa652478db22468cabe5bd562aa12fc8","max":20006,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a95dbb2095d0494f89b7d71c1a59491c","value":20006}},"955de9e6d0e14eed812eb49c482e0e38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9329f021a3ea4119a2d8ab64026cc5da","placeholder":"‚Äã","style":"IPY_MODEL_f6f0c22eb4644078adde62b408e55c4b","value":" 20006/20006 [00:11&lt;00:00, 1958.44 examples/s]"}},"4ac88a162fb945bdac64a15c073d8bc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bad6aa7e03954b5bbdf3b725cff57df3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ed121c3508546fcb96ce2ce5baa4d56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa652478db22468cabe5bd562aa12fc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a95dbb2095d0494f89b7d71c1a59491c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9329f021a3ea4119a2d8ab64026cc5da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6f0c22eb4644078adde62b408e55c4b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6590f98e84240aaa2b8593e6f0e96c5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8690b4411bf747f594c0a91f80d03892","IPY_MODEL_7c4911f92e774fc6aaf9c11dc6f95060","IPY_MODEL_0ba222e001344b5db008edcc91cb2677"],"layout":"IPY_MODEL_21559630fdce4818899537346886bda2"}},"8690b4411bf747f594c0a91f80d03892":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41e52310a5f64090883a11d2f91339c9","placeholder":"‚Äã","style":"IPY_MODEL_10df9b37f27d421bb30ea37c222605ed","value":"Map: 100%"}},"7c4911f92e774fc6aaf9c11dc6f95060":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c047725101124c34a42af061a8e02da8","max":20005,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a15aa7be1974f74a9580396974fbd9d","value":20005}},"0ba222e001344b5db008edcc91cb2677":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_680b829729da418b95637a3ea40357ca","placeholder":"‚Äã","style":"IPY_MODEL_a4ecb4d71ce84635a440a943186637bb","value":" 20005/20005 [00:12&lt;00:00, 1959.10 examples/s]"}},"21559630fdce4818899537346886bda2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41e52310a5f64090883a11d2f91339c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10df9b37f27d421bb30ea37c222605ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c047725101124c34a42af061a8e02da8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a15aa7be1974f74a9580396974fbd9d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"680b829729da418b95637a3ea40357ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4ecb4d71ce84635a440a943186637bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88d6afc90c974ad69cae3477a52da29c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5bcfa86ceb1e4f53970b42375b4159de","IPY_MODEL_ab39ab45648f4d0a9912bbf8eef00f73","IPY_MODEL_458fd4ebf8d947fabfa5340e1749f991"],"layout":"IPY_MODEL_4ef87ddfd8e74e3cab23ed9930085ba4"}},"5bcfa86ceb1e4f53970b42375b4159de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7dee16eca4e4461f8ba7679ea7f31d3f","placeholder":"‚Äã","style":"IPY_MODEL_49616d3e160a4da783394681797597a8","value":"Downloading model.safetensors: 100%"}},"ab39ab45648f4d0a9912bbf8eef00f73":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_edfac5f04dd144b9b4301d6543084d4b","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5c5cbcc3efa4bacb6fbd11f47d8c2c9","value":440449768}},"458fd4ebf8d947fabfa5340e1749f991":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_543e26ba4c1a4a6e8a5e14d42dc7193c","placeholder":"‚Äã","style":"IPY_MODEL_2b429bee187d4b0c8c5389d46f019f14","value":" 440M/440M [00:01&lt;00:00, 319MB/s]"}},"4ef87ddfd8e74e3cab23ed9930085ba4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dee16eca4e4461f8ba7679ea7f31d3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49616d3e160a4da783394681797597a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"edfac5f04dd144b9b4301d6543084d4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5c5cbcc3efa4bacb6fbd11f47d8c2c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"543e26ba4c1a4a6e8a5e14d42dc7193c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b429bee187d4b0c8c5389d46f019f14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}