{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Vp3XPuaTu9jl"},"source":["# Comment g√©n√©rer du texte : utilisation de diff√©rentes m√©thodes de d√©codage pour la g√©n√©ration de langage avec Transformers\n","\n","Credits: https://huggingface.co/blog/how-to-generate"]},{"cell_type":"markdown","metadata":{"id":"KxLvv6UaPa33"},"source":["### **Introduction**\n","\n","Ces derni√®res ann√©es, la g√©n√©ration de langage ouvert a suscit√© un int√©r√™t croissant gr√¢ce √† l'essor de grands mod√®les de langage bas√©s sur des transformateurs et entra√Æn√©s sur des millions de pages web, tels que le c√©l√®bre [mod√®le GPT2] d'OpenAI (https://openai.com/blog/better-language-models/). Les r√©sultats de la g√©n√©ration conditionn√©e de langage ouvert sont impressionnants, par exemple [GPT2 sur les licornes](https://openai.com/blog/better-language-models/#samples), [XLNet](https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e), [Langage contr√¥l√© avec CTRL](https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/). Outre l'architecture am√©lior√©e du transformateur et les donn√©es d'entra√Ænement massives non supervis√©es, **de meilleures m√©thodes de d√©codage** ont √©galement jou√© un r√¥le important.\n","\n","Cet article de blog donne un bref aper√ßu des diff√©rentes strat√©gies de d√©codage et, plus important encore, montre comment *vous* pouvez les mettre en ≈ìuvre avec tr√®s peu d'effort en utilisant la biblioth√®que populaire `transformers` !\n","\n","Toutes les fonctionnalit√©s suivantes peuvent √™tre utilis√©es pour la g√©n√©ration **auto-r√©gressive** de langage ([ici](http://jalammar.github.io/illustrated-gpt2/) un rappel). En bref, la g√©n√©ration *auto-r√©gressive* de langage est bas√©e sur l'hypoth√®se que la distribution de probabilit√© d'une s√©quence de mots peut √™tre d√©compos√©e en un produit de distributions conditionnelles des mots suivants :\n","$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1 : t-1}, W_0) \\text{ ,avec } w_{1 : 0} = \\emptyset, $$\n","\n","et $W_0$ √©tant la s√©quence de mots *contexte* initiale. La longueur $T$ de la s√©quence de mots est habituellement d√©termin√©e *√† la vol√©e* et correspond au pas de temps $t=T$ o√π le jeton EOS est g√©n√©r√© √† partir de $P(w_{t} | w_{1 : t-1}, W_{0})$.\n","\n","\n","La g√©n√©ration de langage auto-r√©gressive est maintenant disponible pour `GPT2`, `XLNet`, `OpenAi-GPT`, `CTRL`, `TransfoXL`, `XLM`, `Bart`, `T5` dans PyTorch et Tensorflow >= 2.0 !\n","\n","Nous ferons un tour d'horizon des m√©thodes de d√©codage les plus courantes, principalement *Greedy search*, *Beam search*, *Top-K sampling* et *Top-p sampling*.\n"]},{"cell_type":"markdown","metadata":{"id":"Si4GyYhOQMzi"},"source":["Installons rapidement les transformateurs et chargeons le mod√®le. Nous utiliserons GPT2 dans Tensorflow 2.1 pour la d√©monstration, mais l'API est 1-√†-1 la m√™me pour PyTorch."]},{"cell_type":"code","metadata":{"id":"XbzZ_IVTtoQe"},"source":["!pip install -q transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ue2kOQhXTAMU"},"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","\n","# add the EOS token as PAD token to avoid warnings\n","model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a8Y7cgu9ohXP"},"source":["### **Recherche avide**\n","\n","La recherche avide s√©lectionne simplement le mot ayant la probabilit√© la plus √©lev√©e comme mot suivant : $w_t = argmax_{w}P(w | w_{1:t-1})$ √† chaque √©tape $t$. L'esquisse suivante illustre la recherche avide.\n","\n","![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n","\n","En partant du mot $\\text{\"The\"}$, l'algorithme\n","choisit avec avidit√© le mot suivant ayant la probabilit√© la plus √©lev√©e $\\text{\"nice\"}$ et ainsi de suite, de sorte que la s√©quence de mots finale g√©n√©r√©e est $\\text{\"The\", \"nice\", \"woman\"}$ avec une probabilit√© globale de 0,5 \\times 0,4 = 0,2$.\n","\n","Dans ce qui suit, nous allons g√©n√©rer des s√©quences de mots en utilisant GPT2 sur le contexte $(\\text{\"I\", \"enjoy\", \"walking\", \"with\", \"my\", \"cute\", \"dog\"})$. Voyons comment la recherche avide peut √™tre utilis√©e dans les \"transformateurs\" comme suit :"]},{"cell_type":"code","metadata":{"id":"OWLd_J6lXz_t","outputId":"5c435015-2b3f-46d9-f303-277c8debbb80","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699559019356,"user_tz":-60,"elapsed":4967,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# encode context the generation is conditioned on\n","model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)\n","\n","# generate 40 new tokens\n","greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n","\n","I'm not sure\n"]}]},{"cell_type":"markdown","metadata":{"id":"BBn1ePmJvhrl"},"source":["Tr√®s bien ! Nous avons g√©n√©r√© notre premier texte court avec GPT2 üòä. Les mots g√©n√©r√©s suivant le contexte sont raisonnables, mais le mod√®le commence rapidement √† se r√©p√©ter ! Il s'agit d'un probl√®me tr√®s courant dans la g√©n√©ration de langage en g√©n√©ral et qui semble l'√™tre encore plus dans la recherche avide et la recherche par faisceau - voir [Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424) et [Shao et al., 2017](https://arxiv.org/abs/1701.03185).\n","\n","L'inconv√©nient majeur de la recherche avide est cependant qu'elle manque des mots √† forte probabilit√© cach√©s derri√®re un mot √† faible probabilit√©, comme on peut le voir dans notre croquis ci-dessus :\n","\n","Le mot $\\text{\"has\"}$ avec sa probabilit√© conditionnelle √©lev√©e de 0,9$ est cach√© derri√®re le mot $\\text{\"dog\"}$, qui n'a que la deuxi√®me probabilit√© conditionnelle la plus √©lev√©e, de sorte que la recherche avide manque la s√©quence de mots $\\text{\"The\"}, \\text{\"dog\"}, \\text{\"has\"}$.\n","\n","Heureusement, la recherche par faisceau permet de r√©soudre ce probl√®me !"]},{"cell_type":"markdown","metadata":{"id":"g8DnXZ1WiuNd"},"source":["### **Recherche par faisceau**\n","\n","La recherche par faisceau r√©duit le risque de manquer des s√©quences de mots cach√©s √† forte probabilit√© en conservant le `nombre de faisceaux` d'hypoth√®ses le plus probable √† chaque pas de temps et en choisissant finalement l'hypoth√®se qui a la probabilit√© globale la plus √©lev√©e. Prenons un exemple avec `num_beams=2` :\n","\n","![Recherche de faisceaux](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n","\n","Au pas de temps $1$, en plus de l'hypoth√®se la plus probable $\\text{\"The\", \"nice\"}$, la recherche par faisceau garde √©galement la trace de la deuxi√®me hypoth√®se la plus probable $\\text{\"The\", \"dog\"}$. Au pas de temps $2$, beam search constate que la s√©quence de mots $\\text{\"The\", \"dog\", \"has\"}$ a une probabilit√© sup√©rieure de $0.36$ √† $\\text{\"The\", \"nice\", \"woman\"}$, qui a une probabilit√© de $0.2$. G√©nial, il a trouv√© la s√©quence de mots la plus probable dans notre exemple de jouet !\n","\n","La recherche par faisceau trouvera toujours une s√©quence de sortie avec une probabilit√© plus √©lev√©e que la recherche avide, mais il n'est pas garanti qu'elle trouve la sortie la plus probable.\n","\n","Voyons comment la recherche par faisceau peut √™tre utilis√©e dans `transformers`. Nous mettons `num_beams > 1` et `early_stopping=True` pour que la g√©n√©ration soit termin√©e lorsque toutes les hypoth√®ses de faisceau ont atteint le jeton EOS."]},{"cell_type":"code","metadata":{"id":"R1R5kx30Ynej","outputId":"9ecd8d2a-c07e-4d94-86a6-4c92216ea0fa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699559024569,"user_tz":-60,"elapsed":1107,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# activate beam search and early_stopping\n","beam_output = model.generate(\n","    **model_inputs,\n","    max_new_tokens=40,\n","    num_beams=5,\n","    early_stopping=True\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n","\n","I'm not sure if I'll ever be able to walk with him again. I'm not sure\n"]}]},{"cell_type":"markdown","metadata":{"id":"AZ6xs-KLi9jT"},"source":["Bien que le r√©sultat soit sans doute plus fluide, la sortie comprend toujours des r√©p√©titions des m√™mes s√©quences de mots.  \n","Un rem√®de simple consiste √† introduire des p√©nalit√©s *n-grammes* (*a.k.a* s√©quences de $n$ mots) comme introduit par [Paulus et al. (2017)](https://arxiv.org/abs/1705.04304) et [Klein et al. (2017)](https://arxiv.org/abs/1701.02810). La p√©nalit√© *n-grams* la plus courante permet de s'assurer qu'aucun *n-gramme* n'appara√Æt deux fois en fixant manuellement la probabilit√© des mots suivants susceptibles de cr√©er un *n-gramme* d√©j√† vu √† $0$.\n","\n","Essayons-le en r√©glant `no_repeat_ngram_size=2` pour qu'aucun *2-gramme* n'apparaisse deux fois :"]},{"cell_type":"code","metadata":{"id":"jy3iVJgfnkMi","outputId":"3f7dc61a-12d9-45d3-b872-e889324c8bf5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699559028531,"user_tz":-60,"elapsed":1051,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# set no_repeat_ngram_size to 2\n","beam_output = model.generate(\n","    **model_inputs,\n","    max_new_tokens=40,\n","    num_beams=5,\n","    no_repeat_ngram_size=2,\n","    early_stopping=True\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n","\n","I've been thinking about this for a while now, and I think it's time for me to\n"]}]},{"cell_type":"markdown","metadata":{"id":"nxsksOGDpmA0"},"source":["Joli, c'est beaucoup mieux ! Nous pouvons voir que la r√©p√©tition n'appara√Æt plus. N√©anmoins, les p√©nalit√©s *n-gram* doivent √™tre utilis√©es avec pr√©caution. Un article g√©n√©r√© sur la ville *New York* ne doit pas utiliser une p√©nalit√© *2-grammes*, sinon le nom de la ville n'appara√Ætrait qu'une seule fois dans l'ensemble du texte !\n","\n","Une autre caract√©ristique importante de la recherche par faisceau est que nous pouvons comparer les meilleurs faisceaux apr√®s la g√©n√©ration et choisir le faisceau g√©n√©r√© qui correspond le mieux √† notre objectif.\n","\n","Dans `transformers`, nous fixons simplement le param√®tre `num_return_sequences` au nombre de faisceaux les mieux not√©s qui doivent √™tre retourn√©s. Assurez-vous cependant que `num_return_sequences <= num_beams` !"]},{"cell_type":"code","metadata":{"id":"5ClO3VphqGp6","outputId":"225f22a9-3f02-4f9d-d062-d18097895101","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699559041030,"user_tz":-60,"elapsed":1521,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# set return_num_sequences > 1\n","beam_outputs = model.generate(\n","    **model_inputs,\n","    max_new_tokens=40,\n","    num_beams=5,\n","    no_repeat_ngram_size=2,\n","    num_return_sequences=5,\n","    early_stopping=True\n",")\n","\n","# now we have 3 output sequences\n","print(\"Output:\\n\" + 100 * '-')\n","for i, beam_output in enumerate(beam_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n","\n","I've been thinking about this for a while now, and I think it's time for me to\n","1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n","\n","I've been thinking about this for a while now, and I think it's time for me to\n","2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n","\n","I've been thinking about this for a while now, and I think it's a good idea to\n","3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n","\n","I've been thinking about this for a while now, and I think it's time to take a\n","4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n","\n","I've been thinking about this for a while now, and I think it's a good idea.\n"]}]},{"cell_type":"markdown","metadata":{"id":"HhLKyfdbsjXc"},"source":["Comme on peut le voir, les cinq hypoth√®ses de faisceaux ne sont que marginalement diff√©rentes les unes des autres - ce qui ne devrait pas √™tre trop surprenant lorsqu'on n'utilise que 5 faisceaux.\n","\n","En ce qui concerne la g√©n√©ration ouverte, quelques raisons ont r√©cemment √©t√© avanc√©es pour expliquer pourquoi la recherche par faisceau n'est peut-√™tre pas la meilleure option possible :\n","\n","- La recherche par faisceau peut tr√®s bien fonctionner dans des t√¢ches o√π la longueur de la g√©n√©ration souhait√©e est plus ou moins pr√©visible, comme dans la traduction automatique ou le r√©sum√© - voir [Murray et al. (2018)](https://arxiv.org/abs/1808.10006) et [Yang et al. (2018)](https://arxiv.org/abs/1808.09582). Mais ce n'est pas le cas pour la g√©n√©ration ouverte o√π la longueur de sortie souhait√©e peut varier consid√©rablement, par exemple la g√©n√©ration de dialogues et d'histoires.\n","\n","- Nous avons vu que la recherche par faisceau souffre fortement de la g√©n√©ration r√©p√©titive. Ceci est particuli√®rement difficile √† contr√¥ler avec des p√©nalit√©s *n-gram*- ou autres dans la g√©n√©ration d'histoires puisque trouver un bon compromis entre la \"non-r√©p√©tition\" forc√©e et la r√©p√©tition de cycles de *n-grammes* identiques demande beaucoup d'ajustements.\n","\n","- Comme indiqu√© dans [Ari Holtzman et al. (2019)] (https://arxiv.org/abs/1904.09751), un langage humain de haute qualit√© ne suit pas une distribution de mots suivants √† forte probabilit√©. En d'autres termes, en tant qu'humains, nous voulons que le texte g√©n√©r√© nous surprenne et ne soit pas ennuyeux/pr√©visible. Les auteurs le montrent joliment en tra√ßant la probabilit√© qu'un mod√®le donnerait √† un texte humain par rapport √† ce que fait la recherche par faisceau.\n","\n","![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n","\n","\n","Alors arr√™tons d'√™tre ennuyeux et introduisons un peu de hasard ü§™."]},{"cell_type":"markdown","metadata":{"id":"XbbIyK84wHq6"},"source":["### **√âchantillonnage**\n","\n","Dans sa forme la plus basique, l'√©chantillonnage consiste √† choisir al√©atoirement le mot suivant $w_t$ en fonction de sa distribution de probabilit√© conditionnelle :\n","\n","$$w_t \\sim P(w|w_{1:t-1})$$\n","\n","En reprenant l'exemple ci-dessus, le graphique suivant visualise la g√©n√©ration du langage lors de l'√©chantillonnage.\n","\n","![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n","\n","Il est √©vident que la g√©n√©ration de langage par √©chantillonnage n'est plus *d√©terministe*. Le mot\n","est √©chantillonn√© √† partir de la distribution de probabilit√© conditionn√©e $P(w | \\text{\"The\"})$, suivi par l'√©chantillonnage de $\\text{\"drives\"}$ √† partir de $P(w | \\text{\"The\"}, \\text{\"car\"})$.\n","\n","Dans `transformers`, nous fixons `do_sample=True` et d√©sactivons l'√©chantillonnage *Top-K* (nous y reviendrons) via `top_k=0`. Dans ce qui suit, nous fixerons `random_seed=0` √† des fins d'illustration. N'h√©sitez pas √† modifier `random_seed` pour jouer avec le mod√®le."]},{"cell_type":"code","metadata":{"outputId":"3833034f-0040-43c2-d376-c7a81bf45be9","id":"aRAz4D-Ks0_4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699559060299,"user_tz":-60,"elapsed":7017,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# set seed to reproduce results. Feel free to change the seed though to get different results\n","from transformers import set_seed\n","set_seed(42)\n","\n","# activate sampling and deactivate top_k by setting top_k sampling to 0\n","sample_output = model.generate(\n","    **model_inputs,\n","    max_new_tokens=40,\n","    do_sample=True,\n","    top_k=0\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","I enjoy walking with my cute dog for the rest of the day, but this had me staying in an unusual room and not going on nights out with friends (which will always be wondered for a mere minute or so at this point).\n"]}]},{"cell_type":"markdown","metadata":{"id":"mQHuo911wfT-"},"source":["Int√©ressant! Le texte semble correct, mais en y regardant de plus pr√®s, il n'est pas tr√®s coh√©rent. les *3 grammes* *le nouveau sens de la main* et le *harnais de frappe local* sont tr√®s √©tranges et ne semblent pas avoir √©t√© √©crits par un humain. C'est le gros probl√®me lors de l'√©chantillonnage de s√©quences de mots : les mod√®les g√©n√®rent souvent un charabia incoh√©rent, *cf.* [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751).\n","\n","Une astuce consiste √† rendre la distribution $P(w|w_{1:t-1})$ plus nette (augmentant la probabilit√© de mots √† forte probabilit√© et diminuant la probabilit√© de mots √† faible probabilit√©) en abaissant la ¬´ temp√©rature ¬ª de le [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max).\n","\n","Une illustration de l‚Äôapplication de la temp√©rature √† notre exemple ci-dessus pourrait ressembler √† ceci.\n","\n","![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n","\n","La distribution conditionnelle du mot suivant de l'√©tape $t=1$ devient beaucoup plus nette, ne laissant presque aucune chance au mot $\\text{\"car\"}$ d'√™tre s√©lectionn√©.\n","\n","\n","Voyons comment nous pouvons refroidir la distribution dans la biblioth√®que en d√©finissant `temperature=0.7`¬†:"]},{"cell_type":"code","metadata":{"id":"WgJredc-0j0Z","outputId":"77dbb140-d248-40f8-c244-ce377f55fbb0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699559061188,"user_tz":-60,"elapsed":893,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# set seed to reproduce results. Feel free to change the seed though to get different results\n","set_seed(42)\n","\n","# use temperature to decrease the sensitivity to low probability candidates\n","sample_output = model.generate(\n","    **model_inputs,\n","    max_new_tokens=40,\n","    do_sample=True,\n","    top_k=0,\n","    temperature=0.6,\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","I enjoy walking with my cute dog, but I don't like to chew on it. I like to eat it and not chew on it. I like to be able to walk with my dog.\"\n","\n","So how did you decide\n"]}]},{"cell_type":"markdown","metadata":{"id":"kzGuu24hZZnq"},"source":["D'ACCORD. Il y a moins de n-grammes bizarres et le r√©sultat est un peu plus coh√©rent maintenant¬†! Bien que l'application de la temp√©rature puisse rendre une distribution moins al√©atoire, dans sa limite, lors du r√©glage de la ¬´ temp√©rature ¬ª $ \\√† 0$, l'√©chantillonnage √©chelonn√© en temp√©rature devient √©gal √† un d√©codage glouton et souffrira des m√™mes probl√®mes qu'auparavant."]},{"cell_type":"markdown","metadata":{"id":"binNTroyzQBu"},"source":["### **√âchantillonnage Top-K**\n","\n","[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf) a introduit un sch√©ma d'√©chantillonnage simple mais tr√®s puissant, appel√© √©chantillonnage ***Top-K***. Dans l'√©chantillonnage *Top-K*, les *K* mots suivants les plus probables sont filtr√©s et la masse de probabilit√© est redistribu√©e uniquement parmi ces *K* mots suivants.\n","GPT2 a adopt√© ce sch√©ma d'√©chantillonnage, ce qui a √©t√© l'une des raisons de son succ√®s dans la g√©n√©ration d'histoires.\n","\n","Nous √©tendons la gamme de mots utilis√©s pour les deux √©tapes d'√©chantillonnage dans l'exemple ci-dessus de 3 mots √† 10 mots pour mieux illustrer l'√©chantillonnage *Top-K*.\n","\n","![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n","\n","Apr√®s avoir d√©fini $K = 6$, dans les deux √©tapes d'√©chantillonnage, nous limitons notre pool d'√©chantillonnage √† 6 mots. Alors que les 6 mots les plus probables, d√©finis comme $V_{\\text{top-K}}$, n'englobent que *environ.* les deux tiers de la masse de probabilit√© totale dans la premi√®re √©tape, ils incluent la quasi-totalit√© de la masse de probabilit√© dans la premi√®re √©tape. deuxi√®me √©tape. N√©anmoins, on voit qu'il r√©ussit √† √©liminer les candidats plut√¥t bizarres $\\text{\"not\", \"the\", \"small\", \"told\"}$\n","lors de la deuxi√®me √©tape d‚Äô√©chantillonnage.\n","\n","\n","Voyons comment *Top-K* peut √™tre utilis√© dans la biblioth√®que en d√©finissant `top_k=50`¬†:"]},{"cell_type":"code","metadata":{"id":"HBtDOdD0wx3l","outputId":"e2c2310f-6a0e-4262-9cad-cdc8fd32667e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699559069551,"user_tz":-60,"elapsed":1740,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# set seed to reproduce results. Feel free to change the seed though to get different results\n","set_seed(42)\n","\n","# set top_k to 50\n","sample_output = model.generate(\n","    **model_inputs,\n","    max_new_tokens=40,\n","    do_sample=True,\n","    top_k=50\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","I enjoy walking with my cute dog for the rest of the day, but this time it was hard for me to figure out what to do with it. (One reason I asked this for a few months back is that I had a\n"]}]},{"cell_type":"markdown","metadata":{"id":"Y77H5m4ZmhEX"},"source":["Pas mal du tout! Le texte est sans doute le texte le plus *√† consonance humaine* jusqu'√† pr√©sent.\n","Un probl√®me cependant avec l'√©chantillonnage *Top-K* est qu'il n'adapte pas dynamiquement le nombre de mots filtr√©s √† partir de la distribution de probabilit√© du mot suivant $P(w|w_{1:t-1})$.\n","Cela peut √™tre probl√©matique car certains mots peuvent √™tre √©chantillonn√©s √† partir d'une distribution tr√®s nette (distribution √† droite dans le graphique ci-dessus), tandis que d'autres peuvent provenir d'une distribution beaucoup plus plate (distribution √† gauche dans le graphique ci-dessus).\n","\n","A l'√©tape $t=1$, *Top-K* √©limine la possibilit√© de\n","sample $\\text{\"people\", \"big\", \"house\", \"cat\"}$, qui semblent √™tre des candidats raisonnables. D'un autre c√¥t√©, √† l'√©tape $t=2$, la m√©thode inclut les mots sans doute mal adapt√©s $\\text{\"down\", \"a\"}$ dans l'√©chantillon de mots. Ainsi, limiter le pool d'√©chantillons √† une taille fixe *K* pourrait mettre le mod√®le en danger de produire du charabia pour des distributions nettes et limiter la cr√©ativit√© du mod√®le pour une distribution plate.\n","Cette intuition a conduit [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) pour cr√©er un √©chantillonnage ***Top-p*** ou ***nucleus***."]},{"cell_type":"markdown","metadata":{"id":"ki9LAaexzV3H"},"source":["### **√âchantillonnage Top-p (noyau)**\n","\n","Au lieu d'√©chantillonner uniquement √† partir des mots *K* les plus probables, dans l'√©chantillonnage *Top-p*, on choisit parmi le plus petit ensemble possible de mots dont la probabilit√© cumul√©e d√©passe la probabilit√© *p*. La masse de probabilit√© est alors redistribu√©e parmi cet ensemble de mots. De cette fa√ßon, la taille de l'ensemble de mots (*alias* le nombre de mots dans l'ensemble) peut augmenter et diminuer dynamiquement en fonction de la distribution de probabilit√© du mot suivant. Ok, c'√©tait tr√®s verbeux, visualisons.\n","\n","![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n","\n","Apr√®s avoir d√©fini $p=0,92$, l'√©chantillonnage *Top-p* s√©lectionne le nombre *minimum* de mots √† d√©passer ensemble $p=92\\%$ de la masse de probabilit√©, d√©finie comme $V_{\\text{top-p}} $. Dans le premier exemple, cela incluait les 9 mots les plus probables, alors qu'il suffit de s√©lectionner les 3 premiers mots dans le deuxi√®me exemple pour d√©passer 92 %. Assez simple en fait ! On peut voir qu'il conserve un large √©ventail de mots l√† o√π le mot suivant est sans doute moins pr√©visible, *par exemple* $P(w | \\text{\"The\"})$, et seulement quelques mots lorsque le mot suivant semble plus pr√©visible, *par exemple* $P(w | \\text{\"The\", \"car\"})$.\n","\n","Tr√®s bien, il est temps de v√©rifier cela dans ¬´ Transformers ¬ª¬†!\n","Nous activons l'√©chantillonnage *Top-p* en d√©finissant `0 < top_p < 1`¬†:"]},{"cell_type":"code","metadata":{"id":"EvwIc7YAx77F","outputId":"1013ae35-0d7a-46c7-b600-e655d8334f26","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699559082508,"user_tz":-60,"elapsed":3,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# set seed to reproduce results. Feel free to change the seed though to get different results\n","set_seed(42)\n","\n","# set top_k to 50\n","sample_output = model.generate(\n","    **model_inputs,\n","    max_new_tokens=40,\n","    do_sample=True,\n","    top_p=0.92,\n","    top_k=0\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","I enjoy walking with my cute dog for the rest of the day, but this had me staying in an unusual room and not going on nights out with friends (which will always be my yearning for such a spacious screen on my desk\n"]}]},{"cell_type":"markdown","metadata":{"id":"tn-8gLaR4lat"},"source":["G√©nial, on dirait que cela aurait pu √™tre √©crit par un humain. Eh bien, peut-√™tre pas encore.\n","\n","Alors qu'en th√©orie, *Top-p* semble plus √©l√©gant que *Top-K*, les deux m√©thodes fonctionnent bien en pratique. *Top-p* peut √©galement √™tre utilis√© en combinaison avec *Top-K*, ce qui permet d'√©viter les mots tr√®s mal class√©s tout en permettant une s√©lection dynamique.\n","\n","Enfin, pour obtenir plusieurs sorties √©chantillonn√©es ind√©pendamment, nous pouvons *√† nouveau* d√©finir le param√®tre `num_return_sequences > 1`¬†:"]},{"cell_type":"code","metadata":{"id":"3kY8P9VG8Gi9","outputId":"b392c55c-3935-4835-bcce-5880d7ace495","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699559088773,"user_tz":-60,"elapsed":1149,"user":{"displayName":"nabil bcom","userId":"04169804032469069549"}}},"source":["# set seed to reproduce results. Feel free to change the seed though to get different results\n","set_seed(42)\n","\n","# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n","sample_outputs = model.generate(\n","    **model_inputs,\n","    max_new_tokens=40,\n","    do_sample=True,\n","    top_k=50,\n","    top_p=0.95,\n","    num_return_sequences=3,\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","for i, sample_output in enumerate(sample_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0: I enjoy walking with my cute dog for the rest of the day, but this time it was hard for me to figure out what to do with it. When I finally looked at this for a few moments, I immediately thought, \"\n","1: I enjoy walking with my cute dog. He has this weird sense of smell. I like walking with him, especially when it comes to walking indoors or if it's a bit chilly. He does walk a lot during the day as well\n","2: I enjoy walking with my cute dog.\n","\n","You've done a lot to help the homeless. Now it's time for our next step: help you find them.\n"]}]},{"cell_type":"markdown","metadata":{"id":"-vRPfMl88rk0"},"source":["Cool, vous devriez maintenant avoir tous les outils pour laisser votre mod√®le √©crire vos histoires avec des ¬´ transformateurs ¬ª¬†!"]}]}